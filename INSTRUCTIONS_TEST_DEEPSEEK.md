# ðŸ§ª Instructions pour tester DeepSeek

## RÃ©sultats du test OpenAI

âœ… **OpenAI a Ã©tÃ© testÃ© avec succÃ¨s !**

- **Latence:** 42.92 secondes
- **QualitÃ©:** Excellente (rÃ©ponse structurÃ©e, personnalisÃ©e, pertinente)
- **Longueur:** 2,908 caractÃ¨res
- **RÃ©sultats:** Voir `openai_test_result.json` et `RAPPORT_LLM_COMPARISON.md`

---

## Pour tester DeepSeek et comparer

### Ã‰tape 1 : Obtenir une clÃ© API DeepSeek

1. Aller sur https://platform.deepseek.com
2. CrÃ©er un compte (ou se connecter)
3. GÃ©nÃ©rer une clÃ© API
4. Copier la clÃ© (format: `sk-xxxxxxxxxxxxx`)

### Ã‰tape 2 : Configurer la clÃ© dans le projet

Ouvrir le fichier `conversation_service_v3/.env` et ajouter votre clÃ© DeepSeek :

```bash
# DeepSeek API Configuration
DEEPSEEK_API_KEY=sk-xxxxxxxxxxxxx  # â† Remplacer par votre clÃ©
DEEPSEEK_BASE_URL=https://api.deepseek.com
```

### Ã‰tape 3 : ExÃ©cuter le test de comparaison automatique

#### Option A : Script Python complet (recommandÃ©)

```bash
python test_provider_comparison.py
```

Ce script va :
- âœ… Tester OpenAI avec la question "compare mes dÃ©penses en mai Ã  celle de juin"
- âœ… Tester DeepSeek avec la mÃªme question
- âœ… GÃ©nÃ©rer un rapport de comparaison dÃ©taillÃ©
- âœ… Comparer latence, qualitÃ©, cohÃ©rence
- âœ… Sauvegarder les rÃ©sultats dans un fichier JSON

**RÃ©sultat attendu :**
```
ðŸ”¬ TEST COMPARATIF: OpenAI vs DeepSeek
======================================================================
Question: "compare mes dÃ©penses en mai Ã  celle de juin"
======================================================================

TEST 1/2: OpenAI
...
âœ… Response received in X.XXs

TEST 2/2: DeepSeek
...
âœ… Response received in X.XXs

ðŸ“Š RAPPORT DE COMPARAISON
======================================================================
â±ï¸  LATENCE
  OpenAI:   42.920s
  DeepSeek: X.XXXs

ðŸ† [Provider] est XX.X% plus rapide

ðŸ“ LONGUEUR DES RÃ‰PONSES
  OpenAI:   2908 caractÃ¨res
  DeepSeek: XXXX caractÃ¨res

ðŸ’¬ CONTENU DES RÃ‰PONSES
  [Affichage des deux rÃ©ponses]

ðŸ“ RÃ©sultats complets sauvegardÃ©s dans: test_comparison_results_YYYYMMDD_HHMMSS.json
```

#### Option B : Tests manuels avec curl

**Test 1 : OpenAI** (dÃ©jÃ  fait)
```bash
# Configuration
LLM_PROVIDER=openai dans .env

# RedÃ©marrer le service
docker-compose restart conversation_service_v3

# Tester
curl --request POST \
  --url http://localhost:3008/api/v3/conversation/3 \
  --header "Authorization: Bearer [VOTRE_TOKEN]" \
  --header "Content-Type: application/json" \
  --data '{
    "client_info": {"platform": "web", "version": "1.0.0"},
    "message": "compare mes dÃ©penses en mai Ã  celle de juin",
    "message_type": "text",
    "priority": "normal"
  }' \
  -w '\nTime: %{time_total}s\n' \
  > openai_result.json
```

**Test 2 : DeepSeek**
```bash
# Configuration
LLM_PROVIDER=deepseek dans .env
LLM_MODEL=deepseek-chat dans .env
LLM_RESPONSE_MODEL=deepseek-chat dans .env

# RedÃ©marrer le service
docker-compose restart conversation_service_v3

# Attendre 5 secondes
sleep 5

# Tester
curl --request POST \
  --url http://localhost:3008/api/v3/conversation/3 \
  --header "Authorization: Bearer [VOTRE_TOKEN]" \
  --header "Content-Type: application/json" \
  --data '{
    "client_info": {"platform": "web", "version": "1.0.0"},
    "message": "compare mes dÃ©penses en mai Ã  celle de juin",
    "message_type": "text",
    "priority": "normal"
  }' \
  -w '\nTime: %{time_total}s\n' \
  > deepseek_result.json
```

**Comparer les rÃ©sultats**
```bash
# Voir la latence
echo "OpenAI:" && grep "Time:" openai_result.json
echo "DeepSeek:" && grep "Time:" deepseek_result.json

# Voir la longueur des rÃ©ponses
echo "OpenAI length:" && cat openai_result.json | jq '.response.message | length'
echo "DeepSeek length:" && cat deepseek_result.json | jq '.response.message | length'

# Comparer les rÃ©ponses (visuel)
cat openai_result.json | jq '.response.message'
cat deepseek_result.json | jq '.response.message'
```

---

## MÃ©triques Ã  comparer

### 1. Latence â±ï¸
- Temps total de traitement
- OpenAI : **42.92s** â† Baseline

### 2. QualitÃ© de la rÃ©ponse ðŸ“Š
- PrÃ©cision des donnÃ©es
- Pertinence de l'analyse
- Personnalisation (utilisation du profil utilisateur)
- Structure et lisibilitÃ©

### 3. CohÃ©rence ðŸŽ¯
- Les chiffres correspondent-ils aux agrÃ©gations ?
- Y a-t-il des contradictions ?
- Le ton est-il adaptÃ© ?

### 4. RequÃªtes ES gÃ©nÃ©rÃ©es ðŸ”
- **OpenAI a gÃ©nÃ©rÃ© :** 2 requÃªtes (comparative analysis)
  - PÃ©riode 1 : Mai 2025 (128 transactions)
  - PÃ©riode 2 : Juin 2025 (134 transactions)
- **DeepSeek gÃ©nÃ¨re-t-il les mÃªmes requÃªtes ?**
  - MÃªme nombre de queries ?
  - MÃªmes filtres ?
  - MÃªmes agrÃ©gations ?

### 5. Longueur de la rÃ©ponse ðŸ“
- OpenAI : **2,908 caractÃ¨res**
- DeepSeek : **? caractÃ¨res**

---

## RÃ©sultats attendus

### ScÃ©nario optimiste âœ…
- DeepSeek gÃ©nÃ¨re les mÃªmes requÃªtes ES
- Latence similaire (Â±20%)
- QualitÃ© comparable
- **â†’ DeepSeek est viable pour production**

### ScÃ©nario rÃ©aliste âš ï¸
- RequÃªtes ES lÃ©gÃ¨rement diffÃ©rentes mais correctes
- Latence +10-30% plus lente
- QualitÃ© bonne mais moins de personnalisation
- **â†’ DeepSeek viable pour dev/test, OpenAI pour production**

### ScÃ©nario pessimiste âŒ
- RequÃªtes ES incorrectes ou incomplÃ¨tes
- Latence >50% plus lente
- QualitÃ© insuffisante
- **â†’ Rester sur OpenAI uniquement**

---

## DÃ©pannage

### Erreur : "DEEPSEEK_API_KEY not configured"
â†’ VÃ©rifier que la clÃ© est bien dans `.env` et redÃ©marrer le service

### Erreur : "Invalid API key"
â†’ VÃ©rifier que la clÃ© commence par `sk-` et est valide sur platform.deepseek.com

### Timeout
â†’ DeepSeek peut Ãªtre plus lent, augmenter le timeout dans la requÃªte curl

### Service ne dÃ©marre pas
â†’ VÃ©rifier les logs : `docker logs harena_conversation_v3`

---

## Support

Pour toute question :
1. Consulter `docs/LLM_PROVIDER_GUIDE.md`
2. Voir les logs : `docker logs harena_conversation_v3`
3. VÃ©rifier la configuration : `cat conversation_service_v3/.env | grep -E "LLM_|DEEPSEEK"`

---

**CrÃ©Ã© le :** 2025-10-26
**Auteur :** Claude Code
