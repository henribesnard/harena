# Sprint 1.4 - Tests et D√©ploiement Production - Plan D√©taill√©

**Date**: 2025-01-12
**Version cible**: v3.2.8 (post-validation production)
**Dur√©e estim√©e**: 2 sessions (Phase 1: 1 session, Phase 2: 1 session)

---

## üìã Vue d'Ensemble

Sprint 1.4 valide l'ensemble du workflow Phase 1 avec des questions utilisateur r√©elles, puis d√©ploie en production apr√®s validation.

**Approche en 2 Phases** :
- **Phase 1** : Validation intensive avec vraies questions ‚Üí Go/No-Go decision
- **Phase 2** : D√©ploiement production + monitoring (si Phase 1 OK)

---

## üéØ Objectifs

### Objectif Principal
Valider que le workflow complet (Analytics + User Profiles + Visualizations) fonctionne correctement avec des questions utilisateur r√©elles avant d√©ploiement production.

### Objectifs Secondaires
1. D√©tecter les bugs/edge cases avec vraies donn√©es
2. Mesurer les performances end-to-end
3. Valider la qualit√© des r√©ponses LLM
4. Documenter les r√©sultats de validation
5. D√©ployer en production si validation OK

---

## üì¶ Phase 1 : Validation avec Questions R√©elles

### T4.1 : Tests Unitaires Visualisations ‚úÖ (D√©j√† fait)

**Statut** : D√©j√† compl√©t√© dans Sprint 1.3
- 12 tests E2E visualisations passent
- Pas de travail suppl√©mentaire n√©cessaire

---

### T4.2 : Tests avec Vraies Questions Utilisateur

**Objectif** : Tester le workflow complet avec 10-15 questions utilisateur r√©alistes

**Questions √† Tester** (exemples) :

#### Cat√©gorie 1 : Transaction Search Simple
1. "Mes d√©penses du mois dernier"
2. "Transactions de cette semaine"
3. "Combien j'ai d√©pens√© aujourd'hui ?"

**Validation attendue** :
- ‚úÖ Intent classifi√© : `transaction_search.simple`
- ‚úÖ Search service retourne transactions
- ‚úÖ Response Generator g√©n√®re r√©ponse + insights
- ‚úÖ Visualisations : 3 KPI Cards (Total, Count, Average)
- ‚úÖ Pas d'erreur 500

#### Cat√©gorie 2 : By Category
4. "Mes d√©penses alimentaires ce mois-ci"
5. "Combien j'ai d√©pens√© en transport ?"
6. "R√©partition de mes d√©penses par cat√©gorie"

**Validation attendue** :
- ‚úÖ Intent : `transaction_search.by_category`
- ‚úÖ Filtrage par cat√©gorie fonctionnel
- ‚úÖ Visualisations : KPI Cards + Pie Chart
- ‚úÖ Pie Chart affiche top 5 cat√©gories + "Autres"

#### Cat√©gorie 3 : By Merchant
7. "Mes d√©penses chez Carrefour"
8. "Combien j'ai pay√© √† Netflix ce mois ?"
9. "Transactions SNCF du dernier mois"

**Validation attendue** :
- ‚úÖ Intent : `transaction_search.by_merchant`
- ‚úÖ Filtrage par merchant fonctionnel
- ‚úÖ Visualisations : KPI Cards + Bar Chart

#### Cat√©gorie 4 : Analytics & Insights
10. "Quelles sont mes d√©penses inhabituelles ?"
11. "Mes tendances de d√©penses"
12. "Comparaison avec le mois dernier"

**Validation attendue** :
- ‚úÖ Analytics Agent d√©tecte anomalies
- ‚úÖ Insights g√©n√©r√©s (unusual transactions, trends)
- ‚úÖ Visualisations appropri√©es (Bar/Line charts)

#### Cat√©gorie 5 : Edge Cases
13. "Mes transactions du 1er janvier 1970" (date invalide)
14. "D√©penses chez XYZ_MERCHANT_INEXISTANT"
15. Question ambigu√´ : "Mes trucs"

**Validation attendue** :
- ‚úÖ Pas de crash
- ‚úÖ Message d'erreur clair
- ‚úÖ Fallback gracieux

**Script de Test** :
```python
# scripts/test_sprint_1.4_validation.py

import asyncio
from conversation_service.api.dependencies import get_context_manager

test_questions = [
    {"user_id": 1, "message": "Mes d√©penses du mois dernier"},
    {"user_id": 1, "message": "Mes d√©penses alimentaires ce mois-ci"},
    # ... 15 questions
]

async def test_workflow(question):
    context_manager = get_context_manager()
    result = await context_manager.process_conversation(
        user_id=question["user_id"],
        message=question["message"]
    )

    # Validation
    assert result.success is True
    assert result.response_text is not None
    assert len(result.data_visualizations) > 0  # Au moins 1 viz

    return {
        "question": question["message"],
        "intent": result.intent_group,
        "success": result.success,
        "visualizations_count": len(result.data_visualizations),
        "insights_count": len(result.insights),
        "processing_time_ms": result.processing_time_ms
    }

# Run all tests
results = await asyncio.gather(*[test_workflow(q) for q in test_questions])
```

---

### T4.3 : Validation End-to-End Workflow Complet

**Objectif** : Valider le flow complet avec monitoring des performances

**Workflow √† Valider** :

```
User Question
    ‚Üì
Intent Classifier
    ‚Üì
Search Service (fetch transactions + aggregations)
    ‚Üì
User Profile Service (fetch profile + metrics)
    ‚Üì
Response Generator
    ‚îú‚îÄ‚îÄ Analytics Agent (insights)
    ‚îú‚îÄ‚îÄ VisualizationService (charts)
    ‚îî‚îÄ‚îÄ LLM (response text)
    ‚Üì
ResponseGenerationResult
```

**M√©triques √† Mesurer** :

1. **Latence** :
   - Intent classification : < 100ms
   - Search service : < 500ms
   - User profile fetch : < 100ms
   - Response generation : < 3000ms
   - **Total E2E : < 4000ms** (objectif)

2. **Taux de Succ√®s** :
   - Objectif : **95%+** success rate
   - Max 1 erreur sur 20 questions

3. **Qualit√© des Visualisations** :
   - 100% des DATA_PRESENTATION responses ont visualisations
   - KPI Cards pr√©sents pour tous les intents
   - Charts appropri√©s selon intent type

4. **Qualit√© des Insights** :
   - Analytics Agent d√©tecte anomalies (si pr√©sentes)
   - Insights pertinents g√©n√©r√©s
   - Pas de faux positifs excessifs

**Script de Monitoring** :
```python
# scripts/test_performance_monitoring.py

import time
from statistics import mean, stdev

async def measure_performance(questions):
    results = []

    for question in questions:
        start = time.time()
        result = await context_manager.process_conversation(
            user_id=1,
            message=question
        )
        elapsed_ms = (time.time() - start) * 1000

        results.append({
            "question": question,
            "latency_ms": elapsed_ms,
            "success": result.success,
            "viz_count": len(result.data_visualizations),
            "insights_count": len(result.insights)
        })

    # Statistiques
    latencies = [r["latency_ms"] for r in results]
    success_rate = sum(1 for r in results if r["success"]) / len(results) * 100

    print(f"Latency - Mean: {mean(latencies):.2f}ms, Stdev: {stdev(latencies):.2f}ms")
    print(f"Success Rate: {success_rate:.1f}%")

    return results
```

---

### T4.4 : Documentation R√©sultats Validation

**Objectif** : Documenter les r√©sultats des tests et d√©cider Go/No-Go

**Rapport de Validation** (template) :

```markdown
# Sprint 1.4 - Phase 1 - Rapport de Validation

## R√©sum√© Ex√©cutif
- Tests effectu√©s : 15 questions r√©elles
- Taux de succ√®s : XX%
- Latence moyenne : XXms
- Decision : GO / NO-GO

## R√©sultats par Cat√©gorie

### Transaction Search Simple (3 questions)
| Question | Intent | Success | Viz Count | Latency |
|----------|--------|---------|-----------|---------|
| "Mes d√©penses du mois dernier" | transaction_search.simple | ‚úÖ | 3 | 2500ms |
| ... | ... | ... | ... | ... |

### By Category (3 questions)
...

### Anomalies D√©tect√©es
- Question X : Erreur 500 (cause: ...)
- Question Y : Latence excessive (5000ms)
- Question Z : Visualisations manquantes

## M√©triques Globales
- Latency P50: XXms
- Latency P95: XXms
- Latency P99: XXms
- Success Rate: XX%
- Visualization Coverage: XX%

## Recommandations
- [ ] GO pour production (si >95% success, <4000ms P95)
- [ ] NO-GO - Corrections n√©cessaires
```

**Crit√®res Go/No-Go** :

**‚úÖ GO pour Phase 2 (Production)** si :
- Success rate ‚â• 95%
- Latency P95 ‚â§ 4000ms
- 100% des DATA_PRESENTATION ont visualisations
- Pas de bugs critiques

**‚ùå NO-GO** si :
- Success rate < 95%
- Latency P95 > 5000ms
- Bugs critiques d√©tect√©s
- Visualisations manquantes

---

## üì¶ Phase 2 : D√©ploiement Production

**Condition** : Phase 1 valid√©e avec GO decision

---

### T4.5 : Pr√©paration Deployment Scripts

**Objectif** : Pr√©parer les scripts de d√©ploiement pour serveur production

**Scripts √† Cr√©er** :

#### 1. `deploy_v3.2.7_to_production.sh`
```bash
#!/bin/bash
# Deployment script for v3.2.7

set -e

ELASTIC_IP="52.210.228.191"
INSTANCE_ID="i-0147cf4bfd894e87d"
TAG="v3.2.7"

echo "=== Deploying v3.2.7 to Production ==="

# 1. SSH into instance
echo "Step 1: Connecting to production server..."

# 2. Pull latest code
echo "Step 2: Pulling v3.2.7 from GitHub..."
ssh ec2-user@$ELASTIC_IP << EOF
  cd /home/ec2-user/harena
  git fetch --tags
  git checkout $TAG
  source env/bin/activate
  pip install -r requirements.txt
EOF

# 3. Run migrations if needed
echo "Step 3: Running database migrations..."
# (if any)

# 4. Restart services
echo "Step 4: Restarting services..."
aws ssm send-command \
  --instance-ids $INSTANCE_ID \
  --document-name "AWS-RunShellScript" \
  --parameters 'commands=[
    "sudo systemctl restart conversation-service",
    "sudo systemctl restart search-service",
    "sudo systemctl status conversation-service"
  ]' \
  --output text --query 'Command.CommandId'

echo "=== Deployment Complete ==="
echo "Check health: curl http://$ELASTIC_IP:8001/api/v1/health"
```

#### 2. `rollback_to_v3.2.6.2.sh`
```bash
#!/bin/bash
# Rollback script to v3.2.6.2

set -e

ELASTIC_IP="52.210.228.191"
TAG="v3.2.6.2"

echo "=== ROLLBACK to v3.2.6.2 ==="

ssh ec2-user@$ELASTIC_IP << EOF
  cd /home/ec2-user/harena
  git checkout $TAG
  source env/bin/activate
  sudo systemctl restart conversation-service
EOF

echo "=== Rollback Complete ==="
```

#### 3. `verify_deployment.sh`
```bash
#!/bin/bash
# Verification script post-deployment

ELASTIC_IP="52.210.228.191"

echo "=== Verifying Deployment ==="

# Health check
echo "1. Health Check:"
curl -s "http://$ELASTIC_IP:8001/api/v1/health" | jq .

# Test question
echo "2. Test Question:"
curl -s -X POST "http://$ELASTIC_IP:8001/api/v1/conversation/chat" \
  -H "Content-Type: application/json" \
  -d '{"user_id": 1, "message": "Mes d√©penses du mois"}' | jq .

# Check logs
echo "3. Recent Logs:"
ssh ec2-user@$ELASTIC_IP "sudo journalctl -u conversation-service -n 20 --no-pager"
```

---

### T4.6 : D√©ploiement sur Serveur Production

**Objectif** : D√©ployer v3.2.7 sur le serveur de production

**√âtapes de D√©ploiement** :

1. **Backup actuel** :
   ```bash
   # Backup database
   ssh ec2-user@52.210.228.191 "pg_dump harena > backup_pre_v3.2.7.sql"

   # Tag current version
   git tag v3.2.6.2-backup
   ```

2. **D√©ploiement** :
   ```bash
   ./scripts/deploy_v3.2.7_to_production.sh
   ```

3. **V√©rification** :
   ```bash
   ./scripts/verify_deployment.sh
   ```

4. **Monitoring** (premi√®res 30 minutes) :
   - Watch logs en temps r√©el
   - Monitor error rate
   - Tester 5-10 questions manuellement

**Checklist D√©ploiement** :
- [ ] Backup database effectu√©
- [ ] Code v3.2.7 d√©ploy√© sur serveur
- [ ] Services red√©marr√©s (conversation-service, search-service)
- [ ] Health check OK (200 status)
- [ ] Test question fonctionne
- [ ] Logs ne montrent pas d'erreurs
- [ ] Visualisations g√©n√©r√©es correctement

---

### T4.7 : Monitoring et Rollback Plan

**Objectif** : Monitorer la production et pr√©parer rollback si n√©cessaire

**Monitoring Continu (24h post-deployment)** :

#### M√©triques √† Surveiller :
1. **Error Rate** : < 5% (objectif)
2. **Latency P95** : < 4000ms
3. **Success Rate** : > 95%
4. **Crash Rate** : 0%

#### Alertes :
- Error rate > 10% ‚Üí Investigate imm√©diatement
- Latency P95 > 5000ms ‚Üí Performance issue
- Service down ‚Üí Rollback imm√©diat

**Rollback Plan** :

**Trigger Rollback si** :
- Error rate > 20%
- Service crash repeatedly
- Critical bug d√©tect√©
- Data corruption

**Rollback Procedure** :
```bash
# 1. Execute rollback script
./scripts/rollback_to_v3.2.6.2.sh

# 2. Verify rollback
curl http://52.210.228.191:8001/api/v1/health

# 3. Restore database if needed
ssh ec2-user@52.210.228.191 "psql harena < backup_pre_v3.2.7.sql"

# 4. Notify team
echo "ROLLBACK executed - investigating v3.2.7 issues"
```

**Post-Deployment Report** (template) :
```markdown
# v3.2.7 Production Deployment Report

## Deployment Info
- Date: YYYY-MM-DD HH:MM
- Version: v3.2.7
- Deployer: [name]
- Duration: XXm

## 24h Post-Deployment Metrics
- Total Requests: XXXX
- Success Rate: XX%
- Error Rate: XX%
- Latency P50/P95/P99: XXms / XXms / XXms
- Visualizations Generated: XXXX

## Issues Detected
- None / [list issues]

## Status
- ‚úÖ Stable in production
- ‚ùå Rolled back (reason: ...)
```

---

## üéØ Acceptance Criteria

### Phase 1 : Validation
- [ ] T4.1: Tests unitaires (d√©j√† ‚úÖ)
- [ ] T4.2: 15 questions test√©es avec succ√®s
- [ ] T4.3: Workflow E2E valid√© (latency, success rate)
- [ ] T4.4: Rapport de validation document√©
- [ ] **GO/NO-GO decision** : GO pour Phase 2

### Phase 2 : D√©ploiement
- [ ] T4.5: Scripts d√©ploiement cr√©√©s et test√©s
- [ ] T4.6: v3.2.7 d√©ploy√© sur production
- [ ] T4.7: Monitoring 24h sans erreurs critiques
- [ ] Rapport post-deployment r√©dig√©

---

## üìä Livrables

### Phase 1
1. Script de test : `scripts/test_sprint_1.4_validation.py`
2. Script monitoring : `scripts/test_performance_monitoring.py`
3. Rapport validation : `SPRINT_1.4_PHASE1_VALIDATION_REPORT.md`

### Phase 2
1. Scripts d√©ploiement : `scripts/deploy_v3.2.7_to_production.sh`
2. Script rollback : `scripts/rollback_to_v3.2.6.2.sh`
3. Script v√©rification : `scripts/verify_deployment.sh`
4. Rapport d√©ploiement : `SPRINT_1.4_PHASE2_DEPLOYMENT_REPORT.md`

---

## ‚ö†Ô∏è Risques et Mitigation

### Risques Phase 1
| Risque | Impact | Probabilit√© | Mitigation |
|--------|--------|-------------|------------|
| Tests √©chouent | High | Medium | Fix bugs avant Phase 2 |
| Latence excessive | Medium | Low | Optimize queries |
| Visualisations manquantes | Medium | Low | Debug VisualizationService |

### Risques Phase 2
| Risque | Impact | Probabilit√© | Mitigation |
|--------|--------|-------------|------------|
| Crash production | Critical | Low | Rollback imm√©diat |
| Performance d√©grad√©e | High | Medium | Monitoring + rollback |
| Database migration fail | Critical | Very Low | Backup avant d√©ploiement |

---

## üöÄ Timeline

### Phase 1 (Session 1 - ~2-3h)
- T4.2: Tests questions r√©elles (1h)
- T4.3: Validation E2E workflow (1h)
- T4.4: Documentation r√©sultats (30min)
- **GO/NO-GO Decision**

### Phase 2 (Session 2 - ~2h)
- T4.5: Pr√©paration scripts (30min)
- T4.6: D√©ploiement production (30min)
- T4.7: Monitoring initial (1h)

---

## üìù Notes

- **Phase 1 doit √™tre 100% valid√©e avant Phase 2**
- Si NO-GO, corriger les issues et re-tester Phase 1
- D√©ploiement production uniquement si tous les crit√®res GO sont remplis
- Rollback plan doit √™tre pr√™t AVANT d√©ploiement

---

**Pr√™t √† d√©marrer Sprint 1.4 - Phase 1 ?**
