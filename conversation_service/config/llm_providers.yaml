# Configuration des Providers LLM - Conversation Service v2.0
version: "2.0.0"
description: "Configuration des providers LLM avec fallback automatique"

providers:
  deepseek:
    enabled: true
    priority: 1
    model: "deepseek-chat"
    temperature: 0.1
    max_tokens: 1500
    timeout: 30
    fallback_on_error: true
    
  openai:
    enabled: true  
    priority: 2
    model: "gpt-3.5-turbo"
    temperature: 0.1
    max_tokens: 1500
    timeout: 30
    fallback_on_error: true
    
  local:
    enabled: false
    priority: 3
    model: "llama2"
    temperature: 0.1
    max_tokens: 1500
    timeout: 60
    fallback_on_error: false

# Stratégie de fallback
fallback_strategy: "sequential"  # sequential | parallel | disabled
max_retries: 2
retry_delay: 1.0  # seconds

# Configuration spécifique par tâche
task_configs:
  intent_classification:
    preferred_providers: ["deepseek", "openai"]
    max_tokens: 500
    temperature: 0.1
    
  response_generation:
    preferred_providers: ["deepseek", "openai"]
    max_tokens: 4000  # Augmenté pour permettre des réponses complètes avec plusieurs transactions
    temperature: 0.7  # Plus créatif pour des réponses naturelles

# Monitoring et coûts
monitoring:
  track_costs: true
  track_latency: true
  track_token_usage: true