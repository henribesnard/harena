"""
Test d'intÃ©gration Ã©quipe AutoGen avec infrastructure existante
Test sans dÃ©pendances externes pour validation architecture
"""

import sys
import os
import asyncio
from typing import Dict, Any

# Ajouter le chemin conversation_service au PYTHONPATH pour les tests
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

def test_imports_infrastructure():
    """Test des imports infrastructure existante"""
    
    try:
        # Test import prompts AutoGen (dÃ©jÃ  validÃ©)
        from prompts.autogen.team_orchestration import MULTI_AGENT_TEAM_ORCHESTRATION_PROMPT
        from prompts.autogen import get_intent_classification_with_team_collaboration
        print("âœ“ Prompts AutoGen importÃ©s")
        
        # Test structure Ã©quipe (sans dÃ©pendances externes)
        from multi_agent_financial_team import MultiAgentFinancialTeam
        print("âœ“ Classe Ã©quipe importÃ©e")
        
        return True
        
    except ImportError as e:
        print(f"âœ— Erreur import: {e}")
        return False

def test_team_structure():
    """Test structure Ã©quipe sans instanciation complÃ¨te"""
    
    try:
        # Import classe sans instanciation (Ã©vite dÃ©pendances externes)
        from multi_agent_financial_team import MultiAgentFinancialTeam
        
        # VÃ©rifier mÃ©thodes principales
        methods = [
            'process_user_message',
            '_extract_team_results', 
            '_validate_intent_entity_coherence',
            'health_check',
            'get_team_statistics'
        ]
        
        for method in methods:
            if hasattr(MultiAgentFinancialTeam, method):
                print(f"âœ“ MÃ©thode {method} prÃ©sente")
            else:
                print(f"âœ— MÃ©thode {method} manquante")
                return False
        
        return True
        
    except Exception as e:
        print(f"âœ— Erreur structure: {e}")
        return False

def test_prompts_integration():
    """Test intÃ©gration prompts avec Ã©quipe"""
    
    try:
        from prompts.autogen.team_orchestration import (
            MULTI_AGENT_TEAM_ORCHESTRATION_PROMPT,
            get_orchestration_prompt_for_context,
            get_workflow_completion_message
        )
        
        # Test prompt orchestration
        assert len(MULTI_AGENT_TEAM_ORCHESTRATION_PROMPT) > 100
        print("âœ“ Prompt orchestration valide")
        
        # Test prompt contexte
        context_prompt = get_orchestration_prompt_for_context({
            "retry_count": 1,
            "failed_agents": ["entity_extractor"]
        })
        assert "CONTEXTE RETRY" in context_prompt
        print("âœ“ Prompt contexte adaptatif fonctionnel")
        
        # Test message completion
        completion_msg = get_workflow_completion_message(
            {"intent": "SEARCH_BY_MERCHANT", "confidence": 0.9},
            {"entities": {"merchants": ["Leclerc"]}, "confidence": 0.8},
            2.5
        )
        assert "Workflow Ã©quipe AutoGen terminÃ©" in completion_msg
        print("âœ“ Message completion gÃ©nÃ©rÃ©")
        
        return True
        
    except Exception as e:
        print(f"âœ— Erreur prompts: {e}")
        return False

def test_coherence_validation_logic():
    """Test logique validation cohÃ©rence (sans dÃ©pendances)"""
    
    try:
        # Simuler mÃ©thode validation cohÃ©rence
        def validate_coherence(intent_result, entities_result):
            """Version simplifiÃ©e pour test"""
            
            if not intent_result or not entities_result:
                return 0.0
                
            intent_type = intent_result.get("intent", "")
            entities = entities_result.get("entities", {})
            
            coherence_checks = {
                "SEARCH_BY_MERCHANT": lambda e: len(e.get("merchants", [])) > 0,
                "SEARCH_BY_AMOUNT": lambda e: len(e.get("amounts", [])) > 0,
                "BALANCE_INQUIRY": lambda e: True
            }
            
            check_function = coherence_checks.get(intent_type, lambda e: True)
            intent_confidence = intent_result.get("confidence", 0.5)
            entity_confidence = entities_result.get("confidence", 0.5)
            
            if check_function(entities):
                return min((intent_confidence + entity_confidence) / 2 + 0.2, 1.0)
            else:
                return max((intent_confidence + entity_confidence) / 2 - 0.3, 0.0)
        
        # Test cas cohÃ©rents
        test_cases = [
            {
                "name": "SEARCH_BY_MERCHANT cohÃ©rent",
                "intent": {"intent": "SEARCH_BY_MERCHANT", "confidence": 0.9},
                "entities": {"entities": {"merchants": ["Carrefour"]}, "confidence": 0.8},
                "expected_min": 0.7
            },
            {
                "name": "BALANCE_INQUIRY neutre",
                "intent": {"intent": "BALANCE_INQUIRY", "confidence": 0.7},
                "entities": {"entities": {}, "confidence": 0.6},
                "expected_min": 0.6
            },
            {
                "name": "IncohÃ©rence SEARCH_BY_MERCHANT",
                "intent": {"intent": "SEARCH_BY_MERCHANT", "confidence": 0.8},
                "entities": {"entities": {"merchants": []}, "confidence": 0.7},
                "expected_max": 0.5
            }
        ]
        
        for case in test_cases:
            score = validate_coherence(case["intent"], case["entities"])
            
            if "expected_min" in case and score >= case["expected_min"]:
                print(f"âœ“ {case['name']}: score {score:.2f} >= {case['expected_min']}")
            elif "expected_max" in case and score <= case["expected_max"]:
                print(f"âœ“ {case['name']}: score {score:.2f} <= {case['expected_max']}")
            else:
                print(f"âœ— {case['name']}: score {score:.2f} inattendu")
                return False
        
        return True
        
    except Exception as e:
        print(f"âœ— Erreur validation cohÃ©rence: {e}")
        return False

def test_team_metrics_structure():
    """Test structure mÃ©triques Ã©quipe"""
    
    try:
        # Structure mÃ©triques attendue
        expected_metrics = {
            "conversations_processed": 0,
            "success_rate": 0.0,
            "avg_processing_time_ms": 0.0,
            "total_processing_time_ms": 0.0,
            "cache_hit_rate": 0.0,
            "coherence_avg_score": 0.0
        }
        
        # Simulation mise Ã  jour mÃ©triques
        def update_metrics(current_metrics, processing_time, success, coherence_score):
            current_metrics["conversations_processed"] += 1
            current_metrics["total_processing_time_ms"] += processing_time
            current_metrics["avg_processing_time_ms"] = (
                current_metrics["total_processing_time_ms"] / 
                current_metrics["conversations_processed"]
            )
            
            total = current_metrics["conversations_processed"]
            if success:
                current_successes = current_metrics["success_rate"] * (total - 1)
                current_metrics["success_rate"] = (current_successes + 1.0) / total
            
            coherence_sum = current_metrics["coherence_avg_score"] * (total - 1)
            current_metrics["coherence_avg_score"] = (coherence_sum + coherence_score) / total
            
            return current_metrics
        
        # Test mise Ã  jour
        metrics = expected_metrics.copy()
        
        # Simulation 3 conversations
        metrics = update_metrics(metrics, 1500, True, 0.8)
        metrics = update_metrics(metrics, 2200, True, 0.9) 
        metrics = update_metrics(metrics, 1800, False, 0.4)
        
        # VÃ©rifications
        assert metrics["conversations_processed"] == 3
        assert 0.6 < metrics["success_rate"] < 0.7  # 2/3 succÃ¨s
        assert 1700 < metrics["avg_processing_time_ms"] < 1900  # ~1833ms
        assert 0.6 < metrics["coherence_avg_score"] < 0.8  # ~0.7
        
        print("âœ“ Structure mÃ©triques Ã©quipe validÃ©e")
        print(f"  - {metrics['conversations_processed']} conversations")
        print(f"  - {metrics['success_rate']:.1%} taux succÃ¨s")
        print(f"  - {metrics['avg_processing_time_ms']:.0f}ms temps moyen")
        print(f"  - {metrics['coherence_avg_score']:.2f} cohÃ©rence moyenne")
        
        return True
        
    except Exception as e:
        print(f"âœ— Erreur mÃ©triques: {e}")
        return False

def main():
    """Test principal intÃ©gration"""
    
    print("=== TEST INTÃ‰GRATION Ã‰QUIPE AUTOGEN ===\n")
    
    tests = [
        ("Imports infrastructure", test_imports_infrastructure),
        ("Structure Ã©quipe", test_team_structure), 
        ("IntÃ©gration prompts", test_prompts_integration),
        ("Logique validation cohÃ©rence", test_coherence_validation_logic),
        ("Structure mÃ©triques", test_team_metrics_structure)
    ]
    
    results = []
    
    for test_name, test_func in tests:
        print(f"--- {test_name} ---")
        try:
            success = test_func()
            results.append((test_name, success))
            print(f"RÃ©sultat: {'âœ“ SUCCÃˆS' if success else 'âœ— Ã‰CHEC'}\n")
        except Exception as e:
            print(f"âœ— Ã‰CHEC: {e}\n")
            results.append((test_name, False))
    
    # RÃ©sumÃ© final
    print("=== RÃ‰SUMÃ‰ TESTS ===")
    successful = sum(1 for _, success in results if success)
    total = len(results)
    
    for test_name, success in results:
        status = "âœ“" if success else "âœ—"
        print(f"{status} {test_name}")
    
    print(f"\n{successful}/{total} tests rÃ©ussis ({successful/total:.1%})")
    
    if successful == total:
        print("\nðŸŽ¯ INTÃ‰GRATION Ã‰QUIPE AUTOGEN VALIDÃ‰E")
        print("   - Architecture respectÃ©e")
        print("   - Prompts intÃ©grÃ©s")
        print("   - MÃ©triques fonctionnelles")
        print("   - Logique mÃ©tier cohÃ©rente")
    else:
        print(f"\n  {total - successful} tests Ã©chouÃ©s - Corrections nÃ©cessaires")

if __name__ == "__main__":
    main()