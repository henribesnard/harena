# Service configuration
SERVICE_NAME=conversation_service_v3
SERVICE_VERSION=3.0.0
HOST=0.0.0.0
PORT=3008

# External services
SEARCH_SERVICE_URL=http://localhost:3002

# LLM Provider Configuration with Fallback Support
# Primary provider (default: deepseek for cost efficiency)
LLM_PRIMARY_PROVIDER=deepseek

# Fallback provider if primary fails (default: openai for reliability)
LLM_FALLBACK_PROVIDER=openai

# Enable automatic fallback (default: true)
LLM_FALLBACK_ENABLED=true

# Legacy support (deprecated, use PRIMARY/FALLBACK instead)
LLM_PROVIDER=deepseek

# API Keys
OPENAI_API_KEY=your_openai_api_key_here
DEEPSEEK_API_KEY=your_deepseek_api_key_here
DEEPSEEK_BASE_URL=https://api.deepseek.com

# Primary Models (DeepSeek by default)
LLM_MODEL=deepseek-chat
LLM_RESPONSE_MODEL=deepseek-chat

# Fallback Models (OpenAI by default)
LLM_FALLBACK_MODEL=gpt-4o-mini
LLM_FALLBACK_RESPONSE_MODEL=gpt-4o

# Common Settings
LLM_TEMPERATURE=0.1
LLM_TIMEOUT=60  # Timeout in seconds before fallback

# Agent configuration
MAX_CORRECTION_ATTEMPTS=2
QUERY_TIMEOUT_SECONDS=30

# Logging
LOG_LEVEL=INFO
