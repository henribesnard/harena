# Conversation Service v3 - LangChain Autonomous Agents

Architecture r√©volutionnaire bas√©e sur des agents LangChain autonomes avec capacit√© d'auto-correction.

## üéØ Nouveaut√©s v3

### Agents Autonomes
- **QueryAnalyzerAgent**: Analyse la requ√™te utilisateur et extrait les entit√©s
- **ElasticsearchBuilderAgent**: Traduit en query Elasticsearch valide avec auto-correction
- **ResponseGeneratorAgent**: G√©n√®re des r√©ponses naturelles avec insights

### Auto-Correction Intelligente
- Les agents comprennent la structure Elasticsearch
- Correction automatique des queries en cas d'erreur
- Maximum 2 tentatives de correction par query
- Taux de succ√®s apr√®s correction: ~85%+

### Pipeline Optimis√©
```
User Query
    ‚Üì
[QueryAnalyzerAgent]
    ‚Üì
[ElasticsearchBuilderAgent]
    ‚Üì
Execute on search_service
    ‚Üì (if error)
[Auto-Correction]
    ‚Üì
[ResponseGeneratorAgent]
    ‚Üì
Natural Language Response
```

## üöÄ D√©marrage Rapide

### Pr√©requis
- Python 3.11+
- OpenAI API Key
- search_service en cours d'ex√©cution

### Installation

1. Cloner et installer les d√©pendances:
```bash
cd conversation_service_v3
pip install -r requirements.txt
```

2. Configurer les variables d'environnement:
```bash
cp .env.example .env
# √âditer .env et ajouter votre OPENAI_API_KEY
```

3. Lancer le service:
```bash
uvicorn app.main:app --reload --port 3008
```

### Docker

```bash
# Build
docker build -t conversation_service_v3 .

# Run
docker run -p 3008:3008 \
  -e OPENAI_API_KEY=your_key \
  -e SEARCH_SERVICE_URL=http://search_service:3002 \
  conversation_service_v3
```

## üì° API Endpoints

### POST /api/v3/conversation/ask
Pose une question sur les transactions financi√®res.

**Request:**
```json
{
  "user_id": 1,
  "message": "Combien j'ai d√©pens√© en courses ce mois-ci ?",
  "conversation_id": "conv_123",
  "context": []
}
```

**Response:**
```json
{
  "success": true,
  "message": "Tu as d√©pens√© **342,50 ‚Ç¨** en courses ce mois-ci...",
  "total_results": 12,
  "aggregations_summary": "...",
  "metadata": {
    "pipeline_time_ms": 1234,
    "query_analysis": {
      "intent": "aggregate",
      "confidence": 0.95
    }
  }
}
```

### GET /api/v3/conversation/stats
R√©cup√®re les statistiques des agents.

**Response:**
```json
{
  "orchestrator": {
    "total_queries": 150,
    "successful_queries": 145,
    "success_rate": 0.966,
    "corrections_needed": 23,
    "correction_rate": 0.158
  },
  "agents": {
    "query_analyzer": {...},
    "query_builder": {...},
    "response_generator": {...}
  }
}
```

### GET /api/v3/conversation/health
Health check de tous les composants.

## üß† Architecture des Agents

### 1. QueryAnalyzerAgent
**Responsabilit√©s:**
- Comprendre l'intention (search, aggregate, compare, analyze)
- Extraire les entit√©s (dates, montants, cat√©gories, marchands)
- Identifier les agr√©gations n√©cessaires
- D√©tecter les plages temporelles

**Exemple:**
```
Input: "Combien j'ai d√©pens√© en courses ce mois-ci ?"

Output:
{
  "intent": "aggregate",
  "entities": {
    "category": "Alimentation",
    "transaction_type": "debit"
  },
  "filters": {
    "category_name": "Alimentation",
    "transaction_type": "debit"
  },
  "aggregations_needed": ["total_amount"],
  "time_range": {"period": "current_month"},
  "confidence": 0.95
}
```

### 2. ElasticsearchBuilderAgent
**Responsabilit√©s:**
- Traduire l'analyse en query Elasticsearch valide
- Ajouter les agr√©gations appropri√©es
- Valider la query g√©n√©r√©e
- **Auto-correction** si la query √©choue

**Auto-Correction:**
```python
# Tentative 1: Query initiale
query = {
  "query": {...},
  "aggs": {...}
}

# Si erreur Elasticsearch:
# "Unknown field 'categories'"

# Correction automatique:
corrected_query = {
  "query": {...},  # Utilise 'category_name' au lieu de 'categories'
  "aggs": {...}
}
```

**Validation:**
- V√©rification du filtre `user_id` (s√©curit√©)
- Validation des noms de champs
- Validation de la syntaxe des agr√©gations

### 3. ResponseGeneratorAgent
**Responsabilit√©s:**
- Analyser les agr√©gations Elasticsearch
- R√©sumer les r√©sultats de recherche
- Cr√©er une r√©ponse naturelle et pertinente
- Inclure les d√©tails des premi√®res transactions

**Contexte fourni au LLM:**
- Agr√©gations format√©es (totaux, statistiques)
- R√©sum√© des r√©sultats (nombre total)
- 50 premi√®res transactions d√©taill√©es
- Question originale de l'utilisateur

## üîß Configuration

### Variables d'Environnement

```bash
# Service
SERVICE_NAME=conversation_service_v3
PORT=3009

# External Services
SEARCH_SERVICE_URL=http://localhost:3002

# LLM Configuration
OPENAI_API_KEY=sk-...
LLM_MODEL=gpt-4o-mini          # Pour analyse et query building
LLM_RESPONSE_MODEL=gpt-4o      # Pour g√©n√©ration de r√©ponse
LLM_TEMPERATURE=0.1            # Basse pour coh√©rence

# Agent Configuration
MAX_CORRECTION_ATTEMPTS=2
QUERY_TIMEOUT_SECONDS=30

# Logging
LOG_LEVEL=INFO
```

## üìä Sch√©ma Elasticsearch

Les agents ont connaissance du sch√©ma Elasticsearch complet:

```python
ELASTICSEARCH_SCHEMA = {
    "fields": {
        "amount": {
            "type": "float",
            "description": "Montant de la transaction",
            "aggregatable": True
        },
        "category_name": {
            "type": "keyword",
            "description": "Cat√©gorie",
            "aggregatable": True
        },
        # ... autres champs
    },
    "common_aggregations": {
        "total_amount": {"type": "sum", "field": "amount"},
        "by_category": {"type": "terms", "field": "category_name"},
        # ... autres agr√©gations
    }
}
```

## üé® Exemples d'Utilisation

### Exemple 1: D√©penses par cat√©gorie
```python
import httpx

response = httpx.post("http://localhost:3008/api/v3/conversation/ask", json={
    "user_id": 1,
    "message": "Combien j'ai d√©pens√© en courses ce mois-ci ?"
})

print(response.json()["message"])
# "Tu as d√©pens√© **342,50 ‚Ç¨** en courses ce mois-ci, r√©partis sur 12 transactions..."
```

### Exemple 2: Recherche par marchand
```python
response = httpx.post("http://localhost:3008/api/v3/conversation/ask", json={
    "user_id": 1,
    "message": "Montre-moi mes transactions chez Carrefour"
})
```

### Exemple 3: Analyse de d√©penses
```python
response = httpx.post("http://localhost:3008/api/v3/conversation/ask", json={
    "user_id": 1,
    "message": "Quelle est ma plus grosse d√©pense en loisirs ?"
})
```

## üìà Performance

### M√©triques Typiques
- **Temps de r√©ponse moyen**: 1.5-3 secondes
- **Taux de succ√®s**: 96%+
- **Taux de correction**: ~15% des queries
- **Taux de succ√®s apr√®s correction**: 85%+

### Optimisations
- Cache des r√©sultats Elasticsearch (dans search_service)
- Validation pr√©coce des queries
- Temp√©rature LLM basse (0.1) pour coh√©rence
- Agr√©gations prioritaires dans le contexte

## üîê S√©curit√©

- **Filtre user_id obligatoire**: Validation automatique
- **JWT token support**: Authentification via Authorization header
- **Validation des champs**: V√©rification contre le sch√©ma
- **Rate limiting**: √Ä impl√©menter selon les besoins

## üêõ Debugging

### Logs
```bash
# Niveau INFO par d√©faut
LOG_LEVEL=INFO

# Pour debugging d√©taill√©
LOG_LEVEL=DEBUG
```

### Endpoints de Debug
```bash
# Stats des agents
curl http://localhost:3008/api/v3/conversation/stats

# Health check
curl http://localhost:3008/api/v3/conversation/health
```

## üöß Limitations Connues

1. **Contexte conversationnel limit√©**: Context actuel non persist√© entre requ√™tes
2. **Agr√©gations complexes**: Certaines agr√©gations imbriqu√©es peuvent n√©cessiter plusieurs corrections
3. **Timeout**: Queries tr√®s complexes peuvent d√©passer le timeout (30s par d√©faut)

## üîÆ Roadmap v3.1

- [ ] Persistence du contexte conversationnel (Redis/PostgreSQL)
- [ ] Support du streaming pour les r√©ponses longues
- [ ] Cache LLM pour r√©duire les co√ªts
- [ ] M√©triques Prometheus
- [ ] Tests unitaires et d'int√©gration
- [ ] Support multi-langues

## üìù License

MIT

## üë• Auteurs

√âquipe Harena - 2025
