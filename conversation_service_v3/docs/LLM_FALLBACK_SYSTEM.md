# üîÑ Syst√®me de Fallback LLM - Documentation

**Date :** 2025-10-26
**Auteur :** Claude Code
**Version :** 1.0

---

## üìã Table des mati√®res

1. [Vue d'ensemble](#vue-densemble)
2. [Architecture](#architecture)
3. [Configuration](#configuration)
4. [Comportement](#comportement)
5. [Tests](#tests)
6. [Logs et monitoring](#logs-et-monitoring)
7. [FAQ](#faq)

---

## üéØ Vue d'ensemble

Le syst√®me de fallback LLM permet une **haute disponibilit√©** et une **r√©silience** du service conversation_v3 en basculant automatiquement sur un LLM de secours si le LLM principal √©choue.

### Cas d'usage

- **API timeout** : Si DeepSeek est lent ou ne r√©pond pas
- **API down** : Si DeepSeek est temporairement indisponible
- **Rate limiting** : Si les quotas DeepSeek sont d√©pass√©s
- **Erreurs API** : Toute erreur 4xx/5xx du provider principal

### Avantages

‚úÖ **Haute disponibilit√©** : Le service ne tombe jamais
‚úÖ **R√©duction des co√ªts** : Utilise DeepSeek (90% moins cher) par d√©faut
‚úÖ **Performance** : DeepSeek est 10% plus rapide qu'OpenAI
‚úÖ **Fiabilit√©** : OpenAI en secours si DeepSeek √©choue
‚úÖ **Transparent** : Fallback automatique sans intervention

---

## üèóÔ∏è Architecture

### Configuration par d√©faut

```
Primary:  DeepSeek (deepseek-chat)
          ‚Üì (si √©chec)
Fallback: OpenAI (gpt-4o-mini / gpt-4o)
```

### Composants

#### 1. `LLMFactory`
Cr√©e des instances LLM pour chaque provider.

```python
from app.core.llm_factory import LLMFactory

# Primary factory
primary_factory = LLMFactory(
    provider="deepseek",
    api_key="sk-...",
    base_url="https://api.deepseek.com",
    timeout=60
)

# Fallback factory
fallback_factory = LLMFactory(
    provider="openai",
    api_key="sk-...",
    timeout=60
)
```

#### 2. `LLMWithFallback`
Wrapper qui g√®re la logique de fallback automatique.

```python
from app.core.llm_factory import LLMWithFallback

llm_with_fallback = LLMWithFallback(
    primary_llm=primary_llm,
    fallback_llm=fallback_llm,
    primary_provider="deepseek",
    fallback_provider="openai"
)

# Utilisation transparente
response = await llm_with_fallback.ainvoke(messages)
# ‚Üí Essaie DeepSeek, bascule sur OpenAI si √©chec
```

#### 3. `create_llm_from_settings()`
Fonction utilitaire qui configure automatiquement le fallback.

```python
from app.core.llm_factory import create_llm_from_settings
from app.config.settings import settings

# Avec fallback (par d√©faut)
llm = create_llm_from_settings(settings, temperature=0.1)

# Sans fallback
llm = create_llm_from_settings(settings, use_fallback=False)
```

### Flux de fonctionnement

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Requ√™te user   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  LLMWithFallback‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Tentative PRIMARY (DeepSeek)‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Succ√®s? ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ YES         ‚îÇ NO
    ‚ñº             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Return  ‚îÇ   ‚îÇ Log warning + tentative ‚îÇ
‚îÇ result  ‚îÇ   ‚îÇ FALLBACK (OpenAI)       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                  ‚îÇ Succ√®s? ‚îÇ
                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                  ‚îÇ YES         ‚îÇ NO
                  ‚ñº             ‚ñº
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ Return  ‚îÇ   ‚îÇ Raise    ‚îÇ
              ‚îÇ result  ‚îÇ   ‚îÇ Exception‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## ‚öôÔ∏è Configuration

### Variables d'environnement

#### `.env` ou `docker-compose.yml`

```bash
# Primary LLM provider (default: deepseek)
LLM_PRIMARY_PROVIDER=deepseek

# Fallback LLM provider (default: openai)
LLM_FALLBACK_PROVIDER=openai

# Enable automatic fallback (default: true)
LLM_FALLBACK_ENABLED=true

# API Keys
OPENAI_API_KEY=sk-xxxxx
DEEPSEEK_API_KEY=sk-xxxxx
DEEPSEEK_BASE_URL=https://api.deepseek.com

# Primary models (DeepSeek)
LLM_MODEL=deepseek-chat
LLM_RESPONSE_MODEL=deepseek-chat

# Fallback models (OpenAI)
LLM_FALLBACK_MODEL=gpt-4o-mini
LLM_FALLBACK_RESPONSE_MODEL=gpt-4o

# Common settings
LLM_TEMPERATURE=0.1
LLM_TIMEOUT=60  # Timeout before fallback (seconds)
```

### Configurations recommand√©es

#### 1. Production (co√ªt optimis√©)
```bash
LLM_PRIMARY_PROVIDER=deepseek
LLM_FALLBACK_PROVIDER=openai
LLM_FALLBACK_ENABLED=true
```
**R√©sultat :** 90% des requ√™tes sur DeepSeek (√©conomie), 10% sur OpenAI (fallback)

#### 2. Production (qualit√© maximale)
```bash
LLM_PRIMARY_PROVIDER=openai
LLM_FALLBACK_PROVIDER=deepseek
LLM_FALLBACK_ENABLED=true
```
**R√©sultat :** OpenAI principal, DeepSeek en secours

#### 3. D√©veloppement
```bash
LLM_PRIMARY_PROVIDER=deepseek
LLM_FALLBACK_PROVIDER=openai
LLM_FALLBACK_ENABLED=false
```
**R√©sultat :** Pas de fallback, erreur si √©chec (pour d√©boguer)

---

## üé¨ Comportement

### Sc√©narios

#### Sc√©nario 1 : Succ√®s du primary

```python
# DeepSeek r√©pond correctement
llm = create_llm_from_settings(settings)
result = await llm.ainvoke(messages)

# Log:
# INFO: Attempting ainvoke with primary LLM (deepseek)
# INFO: Response received from deepseek
```

**R√©sultat :** Pas de fallback, DeepSeek utilis√© ‚úÖ

---

#### Sc√©nario 2 : Timeout du primary

```python
# DeepSeek timeout apr√®s 60s
llm = create_llm_from_settings(settings)
result = await llm.ainvoke(messages)

# Log:
# INFO: Attempting ainvoke with primary LLM (deepseek)
# WARNING: Primary LLM (deepseek) failed: Request timeout
# INFO: Falling back to openai
# INFO: Fallback LLM (openai) succeeded
```

**R√©sultat :** Fallback automatique sur OpenAI ‚úÖ

---

#### Sc√©nario 3 : √âchec primary ET fallback

```python
# DeepSeek ET OpenAI √©chouent
llm = create_llm_from_settings(settings)
result = await llm.ainvoke(messages)

# Log:
# INFO: Attempting ainvoke with primary LLM (deepseek)
# WARNING: Primary LLM (deepseek) failed: API error
# INFO: Falling back to openai
# ERROR: Fallback LLM (openai) also failed: API error
# ERROR: Both primary (deepseek) and fallback (openai) failed
```

**R√©sultat :** Exception raised ‚ùå

---

#### Sc√©nario 4 : Fallback d√©sactiv√©

```python
# Fallback d√©sactiv√©
llm = create_llm_from_settings(settings, use_fallback=False)
result = await llm.ainvoke(messages)

# Log:
# INFO: LLM created without fallback: deepseek
# INFO: Attempting ainvoke with primary LLM (deepseek)
# ERROR: Primary LLM (deepseek) failed and no fallback configured
```

**R√©sultat :** Exception raised si √©chec ‚ùå

---

## üß™ Tests

### Test 1 : V√©rifier que le fallback est actif

```bash
# V√©rifier les variables d'environnement
docker exec harena_conversation_v3 printenv | grep LLM_

# Sortie attendue:
# LLM_PRIMARY_PROVIDER=deepseek
# LLM_FALLBACK_PROVIDER=openai
# LLM_FALLBACK_ENABLED=true
```

### Test 2 : Tester le service normalement

```bash
curl --request POST \
  --url http://localhost:3008/api/v3/conversation/3 \
  --header "Authorization: Bearer YOUR_TOKEN" \
  --header "Content-Type: application/json" \
  --data '{
    "client_info": {"platform": "web", "version": "1.0.0"},
    "message": "combien j'\''ai d√©pens√© ce mois ?",
    "message_type": "text",
    "priority": "normal"
  }'

# V√©rifier les logs
docker logs harena_conversation_v3 --tail 50 | grep "LLM"

# Sortie attendue (si succ√®s primary):
# INFO: Attempting ainvoke with primary LLM (deepseek)
```

### Test 3 : Simuler une erreur DeepSeek

Pour tester le fallback, on peut temporairement invalider la cl√© DeepSeek :

```bash
# 1. Invalider la cl√© DeepSeek
docker exec harena_conversation_v3 sh -c 'export DEEPSEEK_API_KEY=invalid'

# 2. Faire une requ√™te
curl [... m√™me requ√™te que Test 2 ...]

# 3. V√©rifier les logs
docker logs harena_conversation_v3 --tail 50 | grep -E "(WARNING|Fallback)"

# Sortie attendue:
# WARNING: Primary LLM (deepseek) failed: ...
# INFO: Falling back to openai
# INFO: Fallback LLM (openai) succeeded
```

### Test 4 : Script de test automatique

Cr√©ez un fichier `test_fallback.py` :

```python
import asyncio
from app.core.llm_factory import create_llm_from_settings, LLMWithFallback
from app.config.settings import settings

async def test_fallback():
    # Cr√©er LLM avec fallback
    llm = create_llm_from_settings(settings)

    # V√©rifier que c'est un LLMWithFallback
    assert isinstance(llm, LLMWithFallback), "Fallback should be enabled"

    print(f"‚úÖ Primary: {llm.primary_provider}")
    print(f"‚úÖ Fallback: {llm.fallback_provider}")

    # Test simple
    messages = [{"role": "user", "content": "Hello"}]
    result = await llm.ainvoke(messages)

    print(f"‚úÖ Response: {result.content[:100]}")
    print(f"‚úÖ Fallback used: {llm.fallback_used}")

if __name__ == "__main__":
    asyncio.run(test_fallback())
```

---

## üìä Logs et monitoring

### Logs importants

```python
# Succ√®s primary
"INFO: Attempting ainvoke with primary LLM (deepseek)"
"INFO: Response received from deepseek"

# Fallback activ√©
"WARNING: Primary LLM (deepseek) failed: [error]"
"INFO: Falling back to openai"
"INFO: Fallback LLM (openai) succeeded"

# √âchec total
"ERROR: Fallback LLM (openai) also failed: [error]"
"ERROR: Both primary (deepseek) and fallback (openai) failed"
```

### M√©triques √† surveiller

1. **Taux de fallback** : % de requ√™tes utilisant le fallback
2. **Latence** : Temps de r√©ponse avec/sans fallback
3. **Taux d'erreur** : % d'√©checs total (primary + fallback)
4. **Co√ªts** : R√©partition primary vs fallback

### Alertes recommand√©es

- ‚ö†Ô∏è **Taux de fallback > 20%** ‚Üí Probl√®me avec le primary
- üö® **Taux d'erreur > 5%** ‚Üí Probl√®me avec les deux providers
- üìà **Latence > 30s** ‚Üí Timeouts fr√©quents

---

## ‚ùì FAQ

### Q: Le fallback ralentit-il les requ√™tes ?

**R:** Non, seulement si le primary √©choue. Dans ce cas :
- Primary timeout (60s) + Fallback (20s) = **~80s total**
- Sans fallback : √âchec apr√®s 60s

Le fallback ajoute donc ~20s en cas d'√©chec, mais permet d'avoir une r√©ponse au lieu d'une erreur.

---

### Q: Puis-je d√©sactiver le fallback ?

**R:** Oui, mettez `LLM_FALLBACK_ENABLED=false` dans le `.env`.

```bash
LLM_FALLBACK_ENABLED=false
```

---

### Q: Comment savoir quel LLM a √©t√© utilis√© ?

**R:** V√©rifiez les logs ou ajoutez un champ dans la r√©ponse :

```python
# Dans l'agent
if hasattr(llm, 'fallback_used'):
    metadata["llm_fallback_used"] = llm.fallback_used
    metadata["llm_provider"] = llm.fallback_provider if llm.fallback_used else llm.primary_provider
```

---

### Q: Peut-on utiliser OpenAI en primary et DeepSeek en fallback ?

**R:** Oui ! Inversez simplement les providers :

```bash
LLM_PRIMARY_PROVIDER=openai
LLM_FALLBACK_PROVIDER=deepseek
LLM_MODEL=gpt-4o-mini
LLM_FALLBACK_MODEL=deepseek-chat
```

---

### Q: Que se passe-t-il si les deux √©chouent ?

**R:** Une exception est lev√©e avec les d√©tails des deux erreurs :

```python
Exception: Both primary (deepseek) and fallback (openai) failed.
Primary error: Timeout after 60s
Fallback error: API error 503
```

Le client re√ßoit une erreur 500 avec ce message.

---

### Q: Le fallback fonctionne-t-il en streaming ?

**R:** Oui ! `LLMWithFallback` supporte :
- `invoke()` : Appel synchrone
- `ainvoke()` : Appel asynchrone
- `astream()` : Streaming asynchrone

---

## üöÄ Prochaines √©tapes

1. **Monitoring** : Impl√©menter des m√©triques Prometheus
2. **Circuit breaker** : D√©sactiver temporairement le primary si taux d'√©chec √©lev√©
3. **Multi-fallback** : Cha√Æne de fallback (DeepSeek ‚Üí OpenAI ‚Üí Anthropic)
4. **Smart routing** : Router selon le type de requ√™te (simple ‚Üí DeepSeek, complexe ‚Üí OpenAI)

---

**Derni√®re mise √† jour :** 2025-10-26
**Auteur :** Claude Code
**Contact :** Voir documentation projet
