version: '3.8'

# ============================================
# HARENA - PRODUCTION AWS INFRASTRUCTURE
# ============================================
# Déploiement optimisé pour EC2 t3.small (2GB RAM)
# Budget: <20$/mois
# ============================================

networks:
  harena-network:
    driver: bridge

volumes:
  # Note: postgres_data et redis_data ne sont plus nécessaires
  # car on utilise les services PostgreSQL et Redis déjà installés sur l'EC2
  grafana_data:
  prometheus_data:
  loki_data:
  uptime_kuma_data:

services:
  # ============================================
  # NOTE IMPORTANTE: PostgreSQL et Redis
  # ============================================
  # PostgreSQL et Redis sont déjà installés sur l'EC2 (host machine)
  # Nous n'utilisons PAS de containers pour ces services
  # Les services backend se connectent directement aux instances existantes
  # via host.docker.internal (depuis les containers)
  #
  # Pour vérifier les services existants:
  #   PostgreSQL: sudo systemctl status postgresql
  #   Redis:      sudo systemctl status redis
  # ============================================

  # ============================================
  # BACKEND SERVICES
  # ============================================

  user_service:
    build:
      context: .
      dockerfile: user_service/Dockerfile
    container_name: harena_user_service
    restart: unless-stopped
    ports:
      - "3000:3000"  # Exposé pour tests externes (Bruno/Postman)
    environment:
      # Connexion aux services PostgreSQL et Redis sur l'EC2 host
      - DATABASE_URL=postgresql://harena_admin:HaReNa2024SecureDbPassword123@host.docker.internal:5432/harena
      - REDIS_URL=redis://:HaReNa2024-Redis-Auth-Token-Secure-Key-123456@host.docker.internal:6379/0
      - SECRET_KEY=Harena2032Harena2032Harena2032Harena2032Harena2032
      - BRIDGE_CLIENT_ID=${BRIDGE_CLIENT_ID}
      - BRIDGE_CLIENT_SECRET=${BRIDGE_CLIENT_SECRET}
      - API_V1_STR=/api/v1
      - ENVIRONMENT=production
      - LOG_LEVEL=INFO
    networks:
      - harena-network
    extra_hosts:
      - "host.docker.internal:host-gateway"  # Permet d'accéder à l'EC2 host
    deploy:
      resources:
        limits:
          memory: 250M
        reservations:
          memory: 150M
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "user_service"

  search_service:
    build:
      context: .
      dockerfile: search_service/Dockerfile
    container_name: harena_search_service
    restart: unless-stopped
    ports:
      - "3001:3001"  # Exposé pour tests externes
    environment:
      - DATABASE_URL=postgresql://harena_admin:HaReNa2024SecureDbPassword123@host.docker.internal:5432/harena
      - REDIS_URL=redis://:HaReNa2024-Redis-Auth-Token-Secure-Key-123456@host.docker.internal:6379/0
      - ELASTICSEARCH_URL=https://37r8v9zfzn:4o7ydjkcc8@fir-178893546.eu-west-1.bonsaisearch.net:443
      - API_V1_STR=/api/v1
      - ENVIRONMENT=production
      - LOG_LEVEL=INFO
    networks:
      - harena-network
    extra_hosts:
      - "host.docker.internal:host-gateway"
    deploy:
      resources:
        limits:
          memory: 250M
        reservations:
          memory: 150M
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "search_service"

  metric_service:
    build:
      context: .
      dockerfile: metric_service/Dockerfile
    container_name: harena_metric_service
    restart: unless-stopped
    ports:
      - "3002:3002"  # Exposé pour tests externes
    environment:
      - DATABASE_URL=postgresql://harena_admin:HaReNa2024SecureDbPassword123@host.docker.internal:5432/harena
      - API_V1_STR=/api/v1
      - ENVIRONMENT=production
      - LOG_LEVEL=INFO
    networks:
      - harena-network
    extra_hosts:
      - "host.docker.internal:host-gateway"
    deploy:
      resources:
        limits:
          memory: 200M
        reservations:
          memory: 128M
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3002/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "metric_service"

  conversation_service_v3:
    build:
      context: .
      dockerfile: conversation_service_v3/Dockerfile
    container_name: harena_conversation_v3
    restart: unless-stopped
    ports:
      - "3008:3008"  # Exposé pour tests externes
    environment:
      - SERVICE_NAME=conversation_service_v3
      - SERVICE_VERSION=3.1.0
      - HOST=0.0.0.0
      - PORT=3008
      - DATABASE_URL=postgresql://harena_admin:HaReNa2024SecureDbPassword123@host.docker.internal:5432/harena
      - REDIS_URL=redis://:HaReNa2024-Redis-Auth-Token-Secure-Key-123456@host.docker.internal:6379/0
      - SECRET_KEY=Harena2032Harena2032Harena2032Harena2032Harena2032
      - SEARCH_SERVICE_URL=http://search_service:3001
      - LLM_PRIMARY_PROVIDER=openai
      - LLM_FALLBACK_PROVIDER=deepseek
      - LLM_FALLBACK_ENABLED=true
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - DEEPSEEK_API_KEY=${DEEPSEEK_API_KEY}
      - DEEPSEEK_BASE_URL=https://api.deepseek.com
      - LLM_MODEL=gpt-4o-mini
      - LLM_RESPONSE_MODEL=gpt-4o
      - LLM_FALLBACK_MODEL=deepseek-chat
      - LLM_FALLBACK_RESPONSE_MODEL=deepseek-chat
      - LLM_TEMPERATURE=0.1
      - LLM_TIMEOUT=60
      - MAX_CORRECTION_ATTEMPTS=2
      - QUERY_TIMEOUT_SECONDS=30
      - LOG_LEVEL=INFO
      - API_V3_PREFIX=/api/v3
      - ENVIRONMENT=production
    networks:
      - harena-network
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      - search_service
      - metric_service
    deploy:
      resources:
        limits:
          memory: 350M
        reservations:
          memory: 200M
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3008/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "conversation_v3"

  budget_profiling_service:
    build:
      context: .
      dockerfile: budget_profiling_service/Dockerfile
    container_name: harena_budget_profiling
    restart: unless-stopped
    ports:
      - "3006:3006"  # Exposé pour tests externes
    environment:
      - DATABASE_URL=postgresql://harena_admin:HaReNa2024SecureDbPassword123@host.docker.internal:5432/harena
      - SECRET_KEY=Harena2032Harena2032Harena2032Harena2032Harena2032
      - API_V1_STR=/api/v1
      - ENVIRONMENT=production
      - BUDGET_PROFILING_ENABLED=true
      - BUDGET_PROFILING_LOG_LEVEL=INFO
      - BUDGET_PROFILING_PORT=3006
    networks:
      - harena-network
    extra_hosts:
      - "host.docker.internal:host-gateway"
    deploy:
      resources:
        limits:
          memory: 200M
        reservations:
          memory: 128M
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3006/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "budget_profiling"

  # ============================================
  # FRONTEND
  # ============================================

  frontend:
    build:
      context: ./harena_front
      dockerfile: Dockerfile
      args:
        # Variables d'environnement pour le build Vite
        # Le frontend appelle l'API via Nginx (même origine = pas de CORS)
        # Les URLs incluent déjà le préfixe /api/vX, services.ts détecte ça et ne le duplique pas
        VITE_USER_SERVICE_URL: /api/v1
        VITE_SEARCH_SERVICE_URL: /api/v1
        VITE_METRIC_SERVICE_URL: /api/v1
        VITE_CONVERSATION_SERVICE_URL: /api/v3
        VITE_BUDGET_PROFILING_API_URL: /api/v1/budget
    container_name: harena_frontend
    restart: unless-stopped
    ports:
      - "8080:80"  # Nginx production port
    networks:
      - harena-network
    deploy:
      resources:
        limits:
          memory: 128M
        reservations:
          memory: 64M
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "frontend"

  # ============================================
  # REVERSE PROXY - NGINX
  # ============================================

  nginx:
    image: nginx:alpine
    container_name: harena_nginx
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/conf.d:/etc/nginx/conf.d:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
      - ./nginx/logs:/var/log/nginx
    networks:
      - harena-network
    depends_on:
      - frontend
      - user_service
      - search_service
      - metric_service
      - conversation_service_v3
      - budget_profiling_service
    deploy:
      resources:
        limits:
          memory: 100M
        reservations:
          memory: 50M
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "nginx"

  # ============================================
  # MONITORING - PROMETHEUS
  # ============================================

  prometheus:
    image: prom/prometheus:latest
    container_name: harena_prometheus
    restart: unless-stopped
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=7d'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    volumes:
      - ./monitoring/prometheus:/etc/prometheus
      - prometheus_data:/prometheus
    networks:
      - harena-network
    deploy:
      resources:
        limits:
          memory: 256M
        reservations:
          memory: 128M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "prometheus"

  # Node Exporter - Métriques système
  node_exporter:
    image: prom/node-exporter:latest
    container_name: harena_node_exporter
    restart: unless-stopped
    command:
      - '--path.procfs=/host/proc'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    networks:
      - harena-network
    deploy:
      resources:
        limits:
          memory: 64M
        reservations:
          memory: 32M
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "2"
        tag: "node_exporter"

  # Blackbox Exporter - Health Checks HTTP
  blackbox_exporter:
    image: prom/blackbox-exporter:latest
    container_name: harena_blackbox_exporter
    restart: unless-stopped
    volumes:
      - ./monitoring/blackbox:/etc/blackbox_exporter
    command:
      - '--config.file=/etc/blackbox_exporter/blackbox.yml'
    networks:
      - harena-network
    deploy:
      resources:
        limits:
          memory: 32M
        reservations:
          memory: 16M
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "2"
        tag: "blackbox_exporter"

  # cAdvisor - Métriques Docker
  cadvisor:
    image: gcr.io/cadvisor/cadvisor:latest
    container_name: harena_cadvisor
    restart: unless-stopped
    privileged: true
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
    networks:
      - harena-network
    deploy:
      resources:
        limits:
          memory: 128M
        reservations:
          memory: 64M
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "2"
        tag: "cadvisor"

  # ============================================
  # MONITORING - GRAFANA
  # ============================================

  grafana:
    image: grafana/grafana:latest
    container_name: harena_grafana
    restart: unless-stopped
    ports:
      - "3033:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=HarenaAdmin2024!
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_SERVER_ROOT_URL=http://63.35.52.216:3033
      - GF_INSTALL_PLUGINS=grafana-piechart-panel
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources
    networks:
      - harena-network
    depends_on:
      - prometheus
      - loki
    deploy:
      resources:
        limits:
          memory: 150M
        reservations:
          memory: 100M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "grafana"

  # ============================================
  # LOGGING - LOKI
  # ============================================

  loki:
    image: grafana/loki:latest
    container_name: harena_loki
    restart: unless-stopped
    command: -config.file=/etc/loki/loki-config.yml
    volumes:
      - ./monitoring/loki:/etc/loki
      - loki_data:/loki
    networks:
      - harena-network
    deploy:
      resources:
        limits:
          memory: 128M
        reservations:
          memory: 64M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "loki"

  # Promtail - Collecteur de logs pour Loki
  promtail:
    image: grafana/promtail:latest
    container_name: harena_promtail
    restart: unless-stopped
    command: -config.file=/etc/promtail/promtail-config.yml
    volumes:
      - ./monitoring/promtail:/etc/promtail
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    networks:
      - harena-network
    depends_on:
      - loki
    deploy:
      resources:
        limits:
          memory: 64M
        reservations:
          memory: 32M
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "2"
        tag: "promtail"

  # ============================================
  # UPTIME MONITORING - UPTIME KUMA
  # ============================================

  uptime-kuma:
    image: louislam/uptime-kuma:1
    container_name: harena_uptime_kuma
    restart: unless-stopped
    ports:
      - "3010:3001"
    volumes:
      - uptime_kuma_data:/app/data
    networks:
      - harena-network
    deploy:
      resources:
        limits:
          memory: 128M
        reservations:
          memory: 64M
    environment:
      - UPTIME_KUMA_DISABLE_FRAME_SAMEORIGIN=true
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "uptime_kuma"
