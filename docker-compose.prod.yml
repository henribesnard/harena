version: '3.8'

# ============================================
# DOCKER COMPOSE PROD - AWS EC2
# ============================================
# Un seul réseau: harena-prod
# PostgreSQL et Redis utilisent les conteneurs existants
# Tous les services connectés au même réseau
# ============================================

services:
  # ============================================
  # BACKEND SERVICES
  # ============================================

  user_service:
    build:
      context: .
      dockerfile: user_service/Dockerfile
    container_name: harena_user_service
    ports:
      - "0.0.0.0:3000:3000"
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=${REDIS_URL}
      - SECRET_KEY=${SECRET_KEY}
      - BRIDGE_CLIENT_ID=${BRIDGE_CLIENT_ID}
      - BRIDGE_CLIENT_SECRET=${BRIDGE_CLIENT_SECRET}
      - API_V1_STR=/api/v1
      - ENVIRONMENT=production
    networks:
      - harena-prod
    depends_on:
      - postgres
      - redis
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "-c", "import requests; requests.get('http://localhost:3000/health')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    deploy:
      resources:
        limits:
          memory: 256M
        reservations:
          memory: 128M

  search_service:
    build:
      context: .
      dockerfile: search_service/Dockerfile
    container_name: harena_search_service
    ports:
      - "0.0.0.0:3001:3001"
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=${REDIS_URL}
      - ELASTICSEARCH_URL=${ELASTICSEARCH_URL}
      - API_V1_STR=/api/v1
      - ENVIRONMENT=production
    networks:
      - harena-prod
    depends_on:
      - postgres
      - redis
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3001/api/v1/search/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    deploy:
      resources:
        limits:
          memory: 256M
        reservations:
          memory: 128M

  metric_service:
    build:
      context: .
      dockerfile: metric_service/Dockerfile
    container_name: harena_metric_service
    ports:
      - "0.0.0.0:3002:3002"
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - API_V1_STR=/api/v1
      - ENVIRONMENT=production
    networks:
      - harena-prod
    depends_on:
      - postgres
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3002/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    deploy:
      resources:
        limits:
          memory: 200M
        reservations:
          memory: 100M

  conversation_service_v3:
    build:
      context: .
      dockerfile: conversation_service_v3/Dockerfile
    container_name: harena_conversation_v3
    ports:
      - "0.0.0.0:3008:3008"
    environment:
      - SERVICE_NAME=conversation_service_v3
      - SERVICE_VERSION=3.1.0
      - HOST=0.0.0.0
      - PORT=3008
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=${REDIS_URL}
      - SECRET_KEY=${SECRET_KEY}
      - SEARCH_SERVICE_URL=http://harena_search_service:3001
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - LLM_MODEL=gpt-4o-mini
      - LLM_RESPONSE_MODEL=gpt-4o
      - LLM_TEMPERATURE=0.1
      - MAX_CORRECTION_ATTEMPTS=2
      - QUERY_TIMEOUT_SECONDS=30
      - LOG_LEVEL=INFO
      - API_V3_PREFIX=/api/v3
      - ENVIRONMENT=production
    networks:
      - harena-prod
    depends_on:
      - postgres
      - redis
      - search_service
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "-c", "import requests; requests.get('http://localhost:3008/health')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    deploy:
      resources:
        limits:
          memory: 512M
        reservations:
          memory: 256M

  budget_profiling_service:
    build:
      context: .
      dockerfile: budget_profiling_service/Dockerfile
    container_name: harena_budget_profiling
    ports:
      - "0.0.0.0:3006:3006"
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - SECRET_KEY=${SECRET_KEY}
      - API_V1_STR=/api/v1
      - ENVIRONMENT=production
      - BUDGET_PROFILING_ENABLED=true
      - BUDGET_PROFILING_LOG_LEVEL=INFO
      - BUDGET_PROFILING_PORT=3006
    networks:
      - harena-prod
    depends_on:
      - postgres
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3006/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    deploy:
      resources:
        limits:
          memory: 256M
        reservations:
          memory: 128M

  # ============================================
  # MONITORING
  # ============================================

  uptime-kuma:
    image: louislam/uptime-kuma:1
    container_name: harena_uptime_kuma
    ports:
      - "0.0.0.0:3100:3001"
    volumes:
      - uptime-kuma-data:/app/data
    networks:
      - harena-prod
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "node", "extra/healthcheck.js"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    deploy:
      resources:
        limits:
          memory: 256M
        reservations:
          memory: 128M

  # ============================================
  # INFRASTRUCTURE
  # ============================================
  # NOTE: Ces services EXISTENT DÉJÀ sur AWS
  # Cette configuration les connecte simplement au réseau harena-prod
  # ============================================

  postgres:
    image: postgres:16-alpine
    container_name: harena-postgres
    external_links:
      - harena-postgres
    networks:
      - harena-prod

  redis:
    image: redis:7-alpine
    container_name: harena-redis
    external_links:
      - harena-redis
    networks:
      - harena-prod

networks:
  harena-prod:
    name: harena-prod
    driver: bridge
    external: false

volumes:
  uptime-kuma-data:
    driver: local
