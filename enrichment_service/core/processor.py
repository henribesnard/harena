"""
Processeur principal pour l'enrichissement des transactions - Elasticsearch uniquement.

Ce module coordonne la structuration et l'indexation des transactions financi√®res 
dans Elasticsearch, sans vectorisation ni embeddings.
"""
import logging
import time
import uuid
from typing import List, Dict, Any, Optional
from datetime import datetime

from prometheus_client import Counter, Histogram

from enrichment_service.models import (
    TransactionInput,
    BatchTransactionInput,
    ElasticsearchEnrichmentResult,
    BatchEnrichmentResult,
    UserSyncResult,
    StructuredTransaction,

)
from enrichment_service.core.account_enrichment_service import (
    AccountEnrichmentService,
)
from enrichment_service.core.data_quality_validator import DataQualityValidator

logger = logging.getLogger(__name__)

# M√©triques Prometheus
TRANSACTION_PROCESSING_TIME = Histogram(
    "enrichment_processing_seconds",
    "Temps de traitement d'une transaction"
)
CACHE_HITS = Counter(
    "enrichment_cache_hits_total",
    "Nombre de transactions ignor√©es car d√©j√† index√©es"
)
PROCESSING_ERRORS = Counter(
    "enrichment_errors_total",
    "Nombre d'erreurs lors du traitement"
)
BATCH_PROCESSING_TIME = Histogram(
    "enrichment_batch_processing_seconds",
    "Temps de traitement d'un lot de transactions"
)

class ElasticsearchTransactionProcessor:
    """
    Processeur de transactions pour Elasticsearch uniquement.
    G√®re la structuration et l'indexation des donn√©es financi√®res.
    """
    
    def __init__(
        self,
        elasticsearch_client,
        account_enrichment_service: Optional[AccountEnrichmentService] = None,
    ):
        """Initialise le processeur avec le client Elasticsearch et les services externes."""
        self.elasticsearch_client = elasticsearch_client
        self.validator = DataQualityValidator()
        self.account_enrichment_service = account_enrichment_service
        logger.info("ElasticsearchTransactionProcessor initialis√©")

    def _apply_enrichment(self, structured_tx: StructuredTransaction, data: Dict[str, Optional[str]]):
        """Append enrichment information to the searchable text."""
        if not data:
            return
        parts = []
        account = data.get("account_name")
        category = data.get("category_name")
        merchant = data.get("merchant_name")
        if account:
            parts.append(f"Compte: {account}")
        if category:
            parts.append(f"Cat√©gorie: {category}")
        if merchant:
            parts.append(f"Marchand: {merchant}")
        if parts:
            structured_tx.searchable_text += " | " + " | ".join(parts)
    
    


    async def process_single_transaction(
        self,
        transaction: TransactionInput,
        force_update: bool = False,
    ) -> ElasticsearchEnrichmentResult:
        """
        Traite et indexe une transaction unique dans Elasticsearch.

        Args:
            transaction: Transaction √† traiter
            force_update: Force la mise √† jour m√™me si elle existe

        Returns:
            ElasticsearchEnrichmentResult: R√©sultat du traitement
        """
        start_time = time.perf_counter()
        user_id = transaction.user_id
        transaction_id = transaction.bridge_transaction_id

        correlation_id = f"{user_id}-{transaction_id}"
        log = logging.LoggerAdapter(logger, {"correlation_id": correlation_id})
        log.info(f"üîÑ Traitement transaction {transaction_id} pour user {user_id}")
        
        logger.info(f"üîÑ Traitement transaction {transaction_id} pour user {user_id}")

        try:
            # 1. Enrichir les donn√©es du compte avant structuration
            enriched = {}
            if self.account_enrichment_service:
                try:
                    enriched = await self.account_enrichment_service.enrich_with_account_data(transaction)
                except Exception as e:  # pragma: no cover - logging only
                    logger.warning(f"Enrichment failed for tx {transaction_id}: {e}")

            # 2. Structurer la transaction
            structured_tx = StructuredTransaction.from_transaction_input(transaction)
            log.debug(f"üìã Transaction structur√©e: {structured_tx.searchable_text[:100]}...")
            
            # 2. G√©n√©rer l'ID du document
            self._apply_enrichment(structured_tx, enriched)
            logger.debug(f"üìã Transaction structur√©e: {structured_tx.searchable_text[:100]}...")

            # 2. Valider la qualit√© des donn√©es
            is_valid, quality_score, flags = self.validator.evaluate(transaction)
            structured_tx.quality_score = quality_score
            if not is_valid:
                processing_time = time.time() - start_time
                logger.warning(
                    f"üö´ Qualit√© insuffisante pour transaction {transaction_id}: {flags}"
                )
                return ElasticsearchEnrichmentResult(
                    transaction_id=transaction_id,
                    user_id=user_id,
                    searchable_text=structured_tx.searchable_text,
                    document_id="",
                    indexed=False,
                    metadata={
                        "action": "skipped",
                        "reason": "data_quality",
                        "quality_score": quality_score,
                        "flags": flags,
                    },
                    processing_time=processing_time,
                    status="skipped",
                )


            # 3. G√©n√©rer l'ID du document
            document_id = structured_tx.get_document_id()

            # 4. V√©rifier si le document existe d√©j√† (si pas de force_update)
            if not force_update:
                exists = await self.elasticsearch_client.document_exists(document_id)
                if exists:
                    CACHE_HITS.inc()
                    log.debug(f"‚è≠Ô∏è Transaction {transaction_id} existe d√©j√†, ignor√©e")
                    processing_time = time.perf_counter() - start_time
                    TRANSACTION_PROCESSING_TIME.observe(processing_time)

                    logger.debug(f"‚è≠Ô∏è Transaction {transaction_id} existe d√©j√†, ignor√©e")
                    processing_time = time.time() - start_time
                    return ElasticsearchEnrichmentResult(
                        transaction_id=transaction_id,
                        user_id=user_id,
                        searchable_text=structured_tx.searchable_text,
                        document_id=document_id,
                        indexed=False,
                        metadata={
                            "action": "skipped",
                            "reason": "document_exists",
                            "quality_score": quality_score,
                        },
                        processing_time=processing_time,
                        status="skipped",
                    )

            # 5. Indexer dans Elasticsearch
            success = await self.elasticsearch_client.index_document(
                document_id=document_id,
                document=structured_tx.to_elasticsearch_document(),
            )
            
            processing_time = time.perf_counter() - start_time
            TRANSACTION_PROCESSING_TIME.observe(processing_time)

            if success:
                log.debug(f"‚úÖ Transaction {transaction_id} index√©e avec succ√®s ({processing_time:.3f}s)")
                

            processing_time = time.time() - start_time

            if success:
                logger.debug(
                    f"‚úÖ Transaction {transaction_id} index√©e avec succ√®s ({processing_time:.3f}s)"
                )

                return ElasticsearchEnrichmentResult(
                    transaction_id=transaction_id,
                    user_id=user_id,
                    searchable_text=structured_tx.searchable_text,
                    document_id=document_id,
                    indexed=True,
                    metadata={
                        "action": "created" if not force_update else "updated",
                        "elasticsearch_index": self.elasticsearch_client.index_name,
                        "document_size": len(
                            str(structured_tx.to_elasticsearch_document())
                        ),
                        "quality_score": quality_score,
                    },
                    processing_time=processing_time,
                    status="success",
                )
            else:
                log.error(f"‚ùå √âchec indexation transaction {transaction_id}")
                PROCESSING_ERRORS.inc()
                logger.error(f"‚ùå √âchec indexation transaction {transaction_id}")

                return ElasticsearchEnrichmentResult(
                    transaction_id=transaction_id,
                    user_id=user_id,
                    searchable_text=structured_tx.searchable_text,
                    document_id=document_id,
                    indexed=False,
                    metadata={"action": "failed", "quality_score": quality_score},
                    processing_time=processing_time,
                    status="error",
                    error_message="Elasticsearch indexing failed",
                )

        except Exception as e:
            processing_time = time.perf_counter() - start_time
            TRANSACTION_PROCESSING_TIME.observe(processing_time)
            PROCESSING_ERRORS.inc()
            log.error(f"üí• Exception traitement transaction {transaction_id}: {e}")
            
            processing_time = time.time() - start_time
            logger.error(f"üí• Exception traitement transaction {transaction_id}: {e}")

            return ElasticsearchEnrichmentResult(
                transaction_id=transaction_id,
                user_id=user_id,
                searchable_text="",
                document_id="",
                indexed=False,
                metadata={"action": "error"},
                processing_time=processing_time,
                status="error",
                error_message=str(e),
            )

    async def process_transactions_batch(
        self,
        batch_input: BatchTransactionInput,
        force_update: bool = False,
    ) -> BatchEnrichmentResult:
        """
        Traite un lot de transactions en mode bulk Elasticsearch.

        Args:
            batch_input: Lot de transactions √† traiter
            force_update: Force la mise √† jour m√™me si elles existent

        Returns:
            BatchEnrichmentResult: R√©sultat du traitement en lot
        """
        start_time = time.perf_counter()
        user_id = batch_input.user_id
        transactions = batch_input.transactions

        correlation_id = f"batch-{user_id}-{uuid.uuid4()}"
        log = logging.LoggerAdapter(logger, {"correlation_id": correlation_id})
        log.info(f"üîÑ Traitement en lot: {len(transactions)} transactions pour user {user_id}")
        
        logger.info(f"üîÑ Traitement en lot: {len(transactions)} transactions pour user {user_id}")

        try:
            structured_transactions = []
            documents_to_index = []
            results = []
            errors = []
            valid_pairs = []


            for tx in transactions:
                enriched = {}
                if self.account_enrichment_service:
                    try:
                        enriched = await self.account_enrichment_service.enrich_with_account_data(tx)
                    except Exception as e:  # pragma: no cover - logging only
                        logger.warning(f"Enrichment failed for tx {tx.bridge_transaction_id}: {e}")
                structured_tx = StructuredTransaction.from_transaction_input(tx)
                is_valid, quality_score, flags = self.validator.evaluate(tx)
                structured_tx.quality_score = quality_score
                if not is_valid:
                    results.append(
                        ElasticsearchEnrichmentResult(
                            transaction_id=tx.bridge_transaction_id,
                            user_id=user_id,
                            searchable_text=structured_tx.searchable_text,
                            document_id="",
                            indexed=False,
                            metadata={
                                "batch_processing": True,
                                "reason": "data_quality",
                                "quality_score": quality_score,
                                "flags": flags,
                            },
                            processing_time=0,
                            status="skipped",
                        )
                    )
                    errors.append(f"Transaction {tx.bridge_transaction_id}: data_quality")
                    continue
                self._apply_enrichment(structured_tx, enriched)
                structured_transactions.append(structured_tx)
                valid_pairs.append((tx, structured_tx))
                document_id = structured_tx.get_document_id()
                documents_to_index.append({
                    "id": document_id,
                    "document": structured_tx.to_elasticsearch_document(),
                    "transaction_id": tx.bridge_transaction_id,
                })
            
            log.debug(f"üìã {len(structured_transactions)} transactions structur√©es")
            
            # 2. Indexation bulk dans Elasticsearch

            logger.debug(f"üìã {len(structured_transactions)} transactions structur√©es")

            bulk_result = await self.elasticsearch_client.bulk_index_documents(
                documents_to_index,
                force_update=force_update,
            )
            
            # 3. Analyser les r√©sultats du bulk
            processing_time = time.perf_counter() - start_time

            processing_time = time.time() - start_time
            successful = bulk_result.get("indexed", 0)
            failed = bulk_result.get("errors", 0) + len([r for r in results if r.status == "skipped"])
            total = len(transactions)

            bulk_responses = bulk_result.get("responses", [])

            for i, (tx, structured_tx) in enumerate(valid_pairs):
                document_id = structured_tx.get_document_id()
                if i < len(bulk_responses):
                    response = bulk_responses[i]
                    indexed = response.get("success", False)
                    status = "success" if indexed else "error"
                    error_msg = response.get("error") if not indexed else None
                else:
                    indexed = False
                    status = "error"
                    error_msg = "No response from bulk operation"

                result = ElasticsearchEnrichmentResult(
                    transaction_id=tx.bridge_transaction_id,
                    user_id=user_id,
                    searchable_text=structured_tx.searchable_text if indexed else "",
                    document_id=document_id,
                    indexed=indexed,
                    metadata={
                        "batch_processing": True,
                        "batch_size": total,
                        "force_update": force_update,
                        "quality_score": structured_tx.quality_score,
                    },
                    processing_time=processing_time / max(len(valid_pairs), 1),
                    status=status,
                    error_message=error_msg,
                )
                results.append(result)
                if not indexed and error_msg:
                    errors.append(f"Transaction {tx.bridge_transaction_id}: {error_msg}")
            
            log.info(f"üéâ Traitement en lot termin√©: {successful}/{total} succ√®s en {processing_time:.2f}s")

            cache_hits = len([r for r in results if r.status == "skipped"])
            if cache_hits:
                CACHE_HITS.inc(cache_hits)

            BATCH_PROCESSING_TIME.observe(processing_time)

            logger.info(
                f"üéâ Traitement en lot termin√©: {successful}/{total} succ√®s en {processing_time:.2f}s"
            )

            return BatchEnrichmentResult(
                user_id=user_id,
                total_transactions=total,
                successful=successful,
                failed=failed,
                processing_time=processing_time,
                results=results,
                errors=errors,
            )

        except Exception as e:
            processing_time = time.perf_counter() - start_time
            BATCH_PROCESSING_TIME.observe(processing_time)
            PROCESSING_ERRORS.inc()
            log.error(f"üí• Erreur traitement en lot: {e}")
            
            # Cr√©er des r√©sultats d'erreur pour toutes les transactions
            processing_time = time.time() - start_time
            logger.error(f"üí• Erreur traitement en lot: {e}")

            results = []
            for tx in transactions:
                result = ElasticsearchEnrichmentResult(
                    transaction_id=tx.bridge_transaction_id,
                    user_id=user_id,
                    searchable_text="",
                    document_id="",
                    indexed=False,
                    metadata={"batch_processing": True, "error": "batch_failed"},
                    processing_time=0,
                    status="error",
                    error_message=str(e),
                )
                results.append(result)

            return BatchEnrichmentResult(
                user_id=user_id,
                total_transactions=len(transactions),
                successful=0,
                failed=len(transactions),
                processing_time=processing_time,
                results=results,
                errors=[f"Batch processing failed: {str(e)}"],
            )
async def sync_user_transactions(
        self, 
        user_id: int, 
        transactions: List[TransactionInput],
        force_refresh: bool = False
    ) -> UserSyncResult:
        """
        Synchronise toutes les transactions d'un utilisateur dans Elasticsearch.
        
        Args:
            user_id: ID de l'utilisateur
            transactions: Liste compl√®te des transactions de l'utilisateur
            force_refresh: Force la suppression et recr√©ation de tous les documents
            
        Returns:
            UserSyncResult: R√©sultat de la synchronisation
        """
        start_time = time.time()
        logger.info(f"üîÑ Synchronisation user {user_id}: {len(transactions)} transactions (force_refresh={force_refresh})")
        
        try:
            # 1. Optionnel: Nettoyer les donn√©es existantes si force_refresh
            if force_refresh:
                logger.info(f"üßπ Nettoyage des donn√©es existantes pour user {user_id}")
                
                deleted_count = await self.elasticsearch_client.delete_user_transactions(user_id)
                logger.info(f"üóëÔ∏è {deleted_count} documents supprim√©s pour user {user_id}")
            
            # 2. Cr√©er un batch input et traiter
            batch_input = BatchTransactionInput(
                user_id=user_id,
                transactions=transactions
            )
            
            batch_result = await self.process_transactions_batch(
                batch_input, 
                force_update=force_refresh
            )
            
            # 3. Convertir le r√©sultat batch en r√©sultat de sync utilisateur
            processing_time = time.time() - start_time
            
            # Compter les actions sp√©cifiques
            indexed_count = sum(1 for result in batch_result.results if result.status == "success" and result.indexed)
            updated_count = sum(1 for result in batch_result.results 
                              if result.status == "success" and result.metadata.get("action") == "updated")
            error_count = len(batch_result.errors)
            
            # Collecter les d√©tails d'erreurs
            error_details = batch_result.errors.copy()
            
            return UserSyncResult(
                user_id=user_id,
                total_transactions=len(transactions),
                indexed=indexed_count,
                updated=updated_count,
                errors=error_count,
                processing_time=processing_time,
                status="success" if error_count == 0 else "partial_success" if indexed_count > 0 else "failed",
                error_details=error_details
            )
            
        except Exception as e:
            processing_time = time.time() - start_time
            logger.error(f"üí• Erreur synchronisation user {user_id}: {e}")
            
            return UserSyncResult(
                user_id=user_id,
                total_transactions=len(transactions),
                indexed=0,
                updated=0,
                errors=len(transactions),
                processing_time=processing_time,
                status="failed",
                error_details=[f"Sync failed: {str(e)}"]
            )
    
    async def delete_user_data(self, user_id: int) -> Dict[str, Any]:
        """
        Supprime toutes les donn√©es d'un utilisateur d'Elasticsearch.
        
        Args:
            user_id: ID de l'utilisateur
            
        Returns:
            Dict: R√©sultat de la suppression
        """
        logger.info(f"üóëÔ∏è Suppression des donn√©es pour user {user_id}")
        
        try:
            deleted_count = await self.elasticsearch_client.delete_user_transactions(user_id)
            
            logger.info(f"‚úÖ {deleted_count} documents supprim√©s pour user {user_id}")
            
            return {
                "user_id": user_id,
                "deleted_count": deleted_count,
                "status": "success",
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"‚ùå Erreur suppression user {user_id}: {e}")
            
            return {
                "user_id": user_id,
                "deleted_count": 0,
                "status": "error",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
    
    async def get_user_stats(self, user_id: int) -> Dict[str, Any]:
        """
        R√©cup√®re les statistiques d'un utilisateur dans Elasticsearch.
        
        Args:
            user_id: ID de l'utilisateur
            
        Returns:
            Dict: Statistiques de l'utilisateur
        """
        try:
            stats = await self.elasticsearch_client.get_user_statistics(user_id)
            
            return {
                "user_id": user_id,
                "timestamp": datetime.now().isoformat(),
                "elasticsearch_stats": stats,
                "status": "success"
            }
            
        except Exception as e:
            logger.error(f"‚ùå Erreur r√©cup√©ration stats user {user_id}: {e}")
            
            return {
                "user_id": user_id,
                "timestamp": datetime.now().isoformat(),
                "elasticsearch_stats": {},
                "status": "error",
                "error": str(e)
            }
    
    async def health_check(self) -> Dict[str, Any]:
        """
        V√©rifie l'√©tat de sant√© du processeur et d'Elasticsearch.
        
        Returns:
            Dict: Statut de sant√©
        """
        try:
            # V√©rifier la disponibilit√© d'Elasticsearch
            es_healthy = await self.elasticsearch_client.health_check()
            
            # Obtenir des m√©triques de base
            es_info = await self.elasticsearch_client.get_cluster_info() if es_healthy else {}
            
            return {
                "processor": "ElasticsearchTransactionProcessor",
                "version": "2.0.0",
                "status": "healthy" if es_healthy else "degraded",
                "timestamp": datetime.now().isoformat(),
                "elasticsearch": {
                    "available": es_healthy,
                    "cluster_info": es_info,
                    "index_name": self.elasticsearch_client.index_name if hasattr(self.elasticsearch_client, 'index_name') else "unknown"
                },
                "capabilities": {
                    "single_transaction_processing": True,
                    "batch_processing": True,
                    "user_sync": True,
                    "data_deletion": True,
                    "statistics": True
                }
            }
            
        except Exception as e:
            logger.error(f"‚ùå Erreur health check: {e}")
            
            return {
                "processor": "ElasticsearchTransactionProcessor", 
                "version": "2.0.0",
                "status": "error",
                "timestamp": datetime.now().isoformat(),
                "error": str(e),
                "elasticsearch": {"available": False},
                "capabilities": {}
            }