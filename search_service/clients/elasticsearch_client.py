"""
Client Elasticsearch optimis√© pour Bonsai - Search Service
Sp√©cialis√© dans les recherches lexicales de transactions financi√®res
"""

import logging
import ssl
import time
from typing import Dict, Any, List, Optional, Union
from datetime import datetime
import aiohttp

from .base_client import (
    BaseClient, 
    RetryConfig, 
    CircuitBreakerConfig, 
    HealthCheckConfig,
    ClientStatus
)
from config import settings, INDEXED_FIELDS

logger = logging.getLogger(__name__)


class ElasticsearchClient(BaseClient):
    """
    Client Elasticsearch/Bonsai optimis√© pour le Search Service
    
    Responsabilit√©s principales:
    - Recherches lexicales ultra-rapides (<50ms)
    - Validation d√©fensive des requ√™tes (√©vite body=None)
    - Gestion SSL native pour Bonsai
    - Interface g√©n√©rique pour lexical_engine.py
    - M√©triques sp√©cialis√©es Elasticsearch
    - Health checks d√©taill√©s (cluster, index, mapping)
    
    Optimisations sp√©cifiques:
    - Connection pooling pour Bonsai
    - Query validation avant envoi
    - Gestion des timeouts adaptatifs
    - Cache des requ√™tes fr√©quentes
    """
    
    def __init__(
        self,
        bonsai_url: str,
        index_name: str = "harena_transactions",
        timeout: float = 10.0,
        retry_config: Optional[RetryConfig] = None,
        circuit_breaker_config: Optional[CircuitBreakerConfig] = None,
        **kwargs
    ):
        # Configuration SSL sp√©cialis√©e pour Bonsai
        health_check_config = HealthCheckConfig(
            enabled=True,
            interval_seconds=30.0,
            timeout_seconds=5.0,
            endpoint="/"
        )
        
        # Headers sp√©cialis√©s Elasticsearch
        headers = {
            "Content-Type": "application/json",
            "Accept": "application/json",
            **kwargs.get("headers", {})
        }
        
        super().__init__(
            base_url=bonsai_url,
            service_name="elasticsearch",
            timeout=timeout,
            retry_config=retry_config,
            circuit_breaker_config=circuit_breaker_config,
            health_check_config=health_check_config,
            headers=headers
        )
        
        self.index_name = index_name
        self.ssl_context = ssl.create_default_context()
        
        # Cache des requ√™tes fr√©quentes (LRU simple)
        self._query_cache: Dict[str, Dict] = {}
        self._max_cache_size = 100
        
        # Statistiques sp√©cialis√©es Elasticsearch
        self.slow_query_threshold = 1.0  # 1s consid√©r√© lent pour search
        self.query_stats = {
            "search_count": 0,
            "count_count": 0,
            "aggregation_count": 0,
            "cache_hits": 0
        }
        
        logger.info(f"Elasticsearch client initialized for Bonsai index: {index_name}")
    
    async def start(self):
        """D√©marre le client avec configuration SSL optimis√©e pour Bonsai"""
        if self._session is None:
            timeout = aiohttp.ClientTimeout(total=self.timeout)
            
            # Connector SSL optimis√© pour Bonsai
            connector = aiohttp.TCPConnector(
                ssl=self.ssl_context,
                limit=20,           # Pool de connexions
                limit_per_host=10,  # Connexions par host
                keepalive_timeout=30,
                enable_cleanup_closed=True
            )
            
            self._session = aiohttp.ClientSession(
                timeout=timeout,
                connector=connector,
                headers=self.headers
            )
            
            logger.info(f"{self.service_name} client started with SSL for Bonsai")
            
            # Test initial de connexion
            try:
                await self._verify_bonsai_connection()
            except Exception as e:
                logger.warning(f"Initial Bonsai connection test failed: {e}")
    
    async def _verify_bonsai_connection(self):
        """V√©rifie la connexion initiale √† Bonsai"""
        async with self.session.get(self.base_url) as response:
            if response.status == 200:
                cluster_info = await response.json()
                cluster_name = cluster_info.get("cluster_name", "unknown")
                version = cluster_info.get("version", {}).get("number", "unknown")
                logger.info(f"‚úÖ Bonsai connected: {cluster_name} elasticsearch v{version}")
            else:
                logger.warning(f"‚ö†Ô∏è Bonsai responded with status {response.status}")
    
    async def test_connection(self) -> bool:
        """Teste la connectivit√© de base √† Bonsai Elasticsearch"""
        try:
            async def _test():
                async with self.session.get(self.base_url) as response:
                    return response.status == 200
            
            result = await self.execute_with_retry(_test, "connection_test")
            if result:
                logger.debug("Bonsai connection test successful")
            else:
                logger.warning("Bonsai connection test failed")
            return result
            
        except Exception as e:
            logger.error(f"Bonsai connection test failed: {e}")
            return False
    
    async def _perform_health_check(self) -> Dict[str, Any]:
        """Effectue une v√©rification de sant√© compl√®te de Bonsai"""
        health_info = {
            "status": "unknown",
            "cluster_name": None,
            "version": None,
            "index_exists": False,
            "index_health": None,
            "mapping_valid": False,
            "test_user_transactions": 0
        }
        
        try:
            # 1. V√©rifier le cluster Elasticsearch
            async with self.session.get(self.base_url) as response:
                if response.status == 200:
                    cluster_info = await response.json()
                    health_info.update({
                        "cluster_name": cluster_info.get("cluster_name", "unknown"),
                        "version": cluster_info.get("version", {}).get("number", "unknown")
                    })
                else:
                    health_info["status"] = "unhealthy"
                    health_info["error"] = f"Cluster HTTP {response.status}"
                    return health_info
            
            # 2. V√©rifier l'existence de l'index
            async with self.session.head(f"{self.base_url}/{self.index_name}") as response:
                health_info["index_exists"] = response.status == 200
            
            if not health_info["index_exists"]:
                health_info["status"] = "degraded"
                health_info["error"] = f"Index {self.index_name} does not exist"
                return health_info
            
            # 3. V√©rifier la sant√© de l'index
            async with self.session.get(
                f"{self.base_url}/_cluster/health/{self.index_name}"
            ) as response:
                if response.status == 200:
                    index_health = await response.json()
                    health_info["index_health"] = index_health.get("status", "unknown")
                
            # 4. V√©rifier le mapping (champs critiques)
            await self._check_mapping_validity(health_info)
            
            # 5. Compter les transactions de test (si configur√©)
            if hasattr(settings, 'test_user_id'):
                count = await self._count_test_transactions()
                health_info["test_user_transactions"] = count
            
            # D√©terminer le statut final
            if (health_info["index_exists"] and 
                health_info["mapping_valid"] and 
                health_info["index_health"] in ["green", "yellow"]):
                health_info["status"] = "healthy"
            elif health_info["index_exists"]:
                health_info["status"] = "degraded"
            else:
                health_info["status"] = "unhealthy"
            
        except Exception as e:
            health_info["status"] = "unhealthy"
            health_info["error"] = str(e)
            logger.error(f"Elasticsearch health check failed: {e}")
        
        return health_info
    
    async def _check_mapping_validity(self, health_info: Dict[str, Any]):
        """V√©rifie que le mapping contient les champs critiques"""
        try:
            async with self.session.get(
                f"{self.base_url}/{self.index_name}/_mapping"
            ) as response:
                if response.status == 200:
                    mapping_data = await response.json()
                    properties = mapping_data.get(
                        self.index_name, {}
                    ).get("mappings", {}).get("properties", {})
                    
                    # Champs critiques pour le Search Service
                    required_fields = [
                        "user_id", "searchable_text", "primary_description", 
                        "merchant_name", "amount", "date"
                    ]
                    
                    missing_fields = [
                        field for field in required_fields 
                        if field not in properties
                    ]
                    
                    health_info["mapping_valid"] = len(missing_fields) == 0
                    if missing_fields:
                        health_info["missing_fields"] = missing_fields
                        logger.warning(f"Missing critical fields in mapping: {missing_fields}")
                    
        except Exception as e:
            logger.warning(f"Could not check mapping validity: {e}")
            health_info["mapping_valid"] = False
    
    async def _count_test_transactions(self) -> int:
        """Compte les transactions de test pour v√©rifier les donn√©es"""
        try:
            test_user_id = getattr(settings, 'test_user_id', 34)
            count_query = {"query": {"term": {"user_id": test_user_id}}}
            
            async with self.session.post(
                f"{self.base_url}/{self.index_name}/_count",
                json=count_query
            ) as response:
                if response.status == 200:
                    result = await response.json()
                    return result.get("count", 0)
        except Exception as e:
            logger.debug(f"Could not count test transactions: {e}")
        
        return 0
    
    # ============================================================================
    # INTERFACE G√âN√âRIQUE POUR LEXICAL_ENGINE
    # ============================================================================
    
    async def search(
        self,
        index: str,
        body: Optional[Dict[str, Any]] = None,
        size: int = 20,
        from_: int = 0,
        **kwargs
    ) -> Dict[str, Any]:
        """
        Interface g√©n√©rique de recherche pour lexical_engine.py
        
        VALIDATION D√âFENSIVE CRITIQUE:
        Cette m√©thode est appel√©e par lexical_engine et doit absolument
        valider que body n'est pas None pour √©viter les erreurs silencieuses
        
        Args:
            index: Nom de l'index Elasticsearch
            body: Corps de la requ√™te Elasticsearch (OBLIGATOIRE)
            size: Nombre de r√©sultats √† retourner
            from_: Offset pour pagination
            **kwargs: Param√®tres additionnels Elasticsearch
            
        Returns:
            R√©ponse Elasticsearch format√©e
            
        Raises:
            ValueError: Si body est None ou invalide
            Exception: Si la recherche √©choue
        """
        # üö® VALIDATION D√âFENSIVE CRITIQUE
        if body is None:
            error_msg = (
                f"Search body cannot be None for index '{index}'. "
                f"This indicates a bug in query construction. "
                f"Check the calling code in lexical_engine.py"
            )
            logger.error(error_msg)
            logger.error(f"Search called with: index={index}, size={size}, from_={from_}")
            raise ValueError(error_msg)
        
        if not isinstance(body, dict):
            error_msg = f"Search body must be a dict, got {type(body)}: {body}"
            logger.error(error_msg)
            raise ValueError(error_msg)
        
        # Validation de l'index
        if not index:
            raise ValueError("Index name cannot be empty")
        
        # Ajouter size et from_ au corps si pas d√©j√† pr√©sents
        search_body = body.copy()
        if "size" not in search_body:
            search_body["size"] = size
        if "from" not in search_body:
            search_body["from"] = from_
        
        # V√©rifier le cache si activ√©
        cache_key = self._generate_cache_key(index, search_body)
        if cache_key in self._query_cache:
            self.query_stats["cache_hits"] += 1
            logger.debug(f"Cache hit for search query")
            return self._query_cache[cache_key]
        
        async def _search():
            async with self.session.post(
                f"{self.base_url}/{index}/_search",
                json=search_body
            ) as response:
                if response.status == 200:
                    result = await response.json()
                    
                    # Ajouter au cache si la requ√™te a r√©ussi
                    self._add_to_cache(cache_key, result)
                    
                    return result
                else:
                    error_text = await response.text()
                    raise Exception(
                        f"Elasticsearch search failed: HTTP {response.status} - {error_text}"
                    )
        
        # Ex√©cuter avec retry et circuit breaker
        result = await self.execute_with_retry(_search, "search")
        
        # Statistiques
        self.query_stats["search_count"] += 1
        
        return result
    
    async def count(
        self,
        index: str,
        body: Optional[Dict[str, Any]] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        Interface g√©n√©rique de comptage pour lexical_engine.py
        
        Args:
            index: Nom de l'index Elasticsearch
            body: Corps de la requ√™te de comptage (peut √™tre None pour count total)
            **kwargs: Param√®tres additionnels
            
        Returns:
            R√©sultat du comptage Elasticsearch
        """
        # Pour count, body peut √™tre None (count total de l'index)
        if body is not None and not isinstance(body, dict):
            error_msg = f"Count body must be a dict or None, got {type(body)}: {body}"
            logger.error(error_msg)
            raise ValueError(error_msg)
        
        # Si body est None, cr√©er une query match_all
        count_body = body or {"query": {"match_all": {}}}
        
        async def _count():
            async with self.session.post(
                f"{self.base_url}/{index}/_count",
                json=count_body
            ) as response:
                if response.status == 200:
                    return await response.json()
                else:
                    error_text = await response.text()
                    raise Exception(
                        f"Elasticsearch count failed: HTTP {response.status} - {error_text}"
                    )
        
        result = await self.execute_with_retry(_count, "count")
        
        # Statistiques
        self.query_stats["count_count"] += 1
        
        return result
    
    async def health(self) -> Dict[str, Any]:
        """
        Interface g√©n√©rique de sant√© pour les health checks
        
        Returns:
            Statut de sant√© du cluster Elasticsearch
        """
        async def _health():
            async with self.session.get(
                f"{self.base_url}/_cluster/health"
            ) as response:
                if response.status == 200:
                    return await response.json()
                else:
                    error_text = await response.text()
                    raise Exception(
                        f"Elasticsearch health failed: HTTP {response.status} - {error_text}"
                    )
        
        return await self.execute_with_retry(_health, "health")
    
    # ============================================================================
    # M√âTHODES SP√âCIALIS√âES SEARCH SERVICE
    # ============================================================================
    
    async def search_transactions(
        self,
        user_id: int,
        query: Optional[str] = None,
        filters: Optional[Dict[str, Any]] = None,
        limit: int = 20,
        offset: int = 0,
        include_highlights: bool = False,
        sort_by: Optional[List[Dict[str, str]]] = None
    ) -> Dict[str, Any]:
        """
        Recherche de transactions optimis√©e pour le domaine financier
        
        Args:
            user_id: ID utilisateur (obligatoire pour s√©curit√©)
            query: Terme de recherche textuelle (optionnel)
            filters: Filtres additionnels (cat√©gorie, montant, date)
            limit: Nombre de r√©sultats
            offset: D√©calage pagination
            include_highlights: Inclure surlignage des termes
            sort_by: Crit√®res de tri personnalis√©s
            
        Returns:
            R√©sultats de recherche format√©s
        """
        search_body = self._build_transaction_search_query(
            user_id=user_id,
            query=query,
            filters=filters,
            limit=limit,
            offset=offset,
            include_highlights=include_highlights,
            sort_by=sort_by
        )
        
        return await self.search(
            index=self.index_name,
            body=search_body,
            size=limit,
            from_=offset
        )
    
    def _build_transaction_search_query(
        self,
        user_id: int,
        query: Optional[str] = None,
        filters: Optional[Dict[str, Any]] = None,
        limit: int = 20,
        offset: int = 0,
        include_highlights: bool = False,
        sort_by: Optional[List[Dict[str, str]]] = None
    ) -> Dict[str, Any]:
        """
        Construit une requ√™te Elasticsearch optimis√©e pour les transactions
        
        Optimisations appliqu√©es:
        - user_id filter obligatoire (s√©curit√©)
        - Multi-match avec boost sur champs importants
        - Tri par pertinence puis date
        - Highlighting intelligent si demand√©
        """
        # Requ√™te de base avec filtre utilisateur obligatoire
        bool_query = {
            "bool": {
                "must": [
                    {"term": {"user_id": user_id}}
                ]
            }
        }
        
        # Ajouter recherche textuelle si fournie
        if query and query.strip():
            text_query = {
                "multi_match": {
                    "query": query.strip(),
                    "fields": [
                        "searchable_text^2.0",
                        "primary_description^1.5", 
                        "merchant_name^1.8",
                        "category_name^1.2"
                    ],
                    "type": "best_fields",
                    "operator": "or",
                    "fuzziness": "AUTO"
                }
            }
            bool_query["bool"]["must"].append(text_query)
        
        # Ajouter filtres additionnels
        if filters:
            self._apply_transaction_filters(bool_query, filters)
        
        # Corps de la requ√™te complet
        search_body = {
            "query": bool_query,
            "size": limit,
            "from": offset,
            "_source": [
                "transaction_id", "user_id", "account_id",
                "amount", "amount_abs", "transaction_type", "currency_code",
                "date", "primary_description", "merchant_name", "category_name", 
                "operation_type", "month_year", "weekday"
            ]
        }
        
        # Tri (par d√©faut: pertinence puis date d√©croissante)
        if sort_by:
            search_body["sort"] = sort_by
        else:
            search_body["sort"] = [
                {"_score": {"order": "desc"}},
                {"date": {"order": "desc", "unmapped_type": "date"}}
            ]
        
        # Highlighting si demand√©
        if include_highlights and query:
            search_body["highlight"] = {
                "fields": {
                    "searchable_text": {
                        "fragment_size": 150,
                        "number_of_fragments": 2,
                        "pre_tags": ["<mark>"],
                        "post_tags": ["</mark>"]
                    },
                    "primary_description": {
                        "fragment_size": 100,
                        "number_of_fragments": 1,
                        "pre_tags": ["<mark>"],
                        "post_tags": ["</mark>"]
                    },
                    "merchant_name": {
                        "pre_tags": ["<mark>"],
                        "post_tags": ["</mark>"]
                    }
                }
            }
        
        return search_body
    
    def _apply_transaction_filters(self, bool_query: Dict[str, Any], filters: Dict[str, Any]):
        """Applique les filtres sp√©cifiques aux transactions"""
        must_filters = bool_query["bool"]["must"]
        
        # Filtre de cat√©gorie
        if "category" in filters and filters["category"]:
            must_filters.append({
                "term": {"category_name.keyword": filters["category"]}
            })
        
        # Filtre de marchand
        if "merchant" in filters and filters["merchant"]:
            must_filters.append({
                "term": {"merchant_name.keyword": filters["merchant"]}
            })
        
        # Filtre de montant (plage)
        if "amount_min" in filters or "amount_max" in filters:
            amount_filter = {"range": {"amount_abs": {}}}
            if "amount_min" in filters:
                amount_filter["range"]["amount_abs"]["gte"] = filters["amount_min"]
            if "amount_max" in filters:
                amount_filter["range"]["amount_abs"]["lte"] = filters["amount_max"]
            must_filters.append(amount_filter)
        
        # Filtre de date (plage)
        if "date_from" in filters or "date_to" in filters:
            date_filter = {"range": {"date": {}}}
            if "date_from" in filters:
                date_filter["range"]["date"]["gte"] = filters["date_from"]
            if "date_to" in filters:
                date_filter["range"]["date"]["lte"] = filters["date_to"]
            must_filters.append(date_filter)
        
        # Filtre de type de transaction
        if "transaction_type" in filters and filters["transaction_type"] != "all":
            must_filters.append({
                "term": {"transaction_type": filters["transaction_type"]}
            })
    
    async def aggregate_transactions(
        self,
        user_id: int,
        aggregation_type: str,
        filters: Optional[Dict[str, Any]] = None,
        size: int = 10
    ) -> Dict[str, Any]:
        """
        Effectue des agr√©gations sur les transactions
        
        Args:
            user_id: ID utilisateur
            aggregation_type: Type d'agr√©gation (by_category, by_merchant, by_month)
            filters: Filtres √† appliquer avant agr√©gation
            size: Nombre de buckets √† retourner
            
        Returns:
            R√©sultats d'agr√©gation
        """
        agg_query = {
            "query": {
                "bool": {
                    "must": [{"term": {"user_id": user_id}}]
                }
            },
            "size": 0,  # Ne pas retourner les documents
            "aggs": {}
        }
        
        # Ajouter filtres si fournis
        if filters:
            self._apply_transaction_filters(agg_query, filters)
        
        # Configurer l'agr√©gation selon le type
        if aggregation_type == "by_category":
            agg_query["aggs"]["categories"] = {
                "terms": {
                    "field": "category_name.keyword",
                    "size": size
                },
                "aggs": {
                    "total_amount": {"sum": {"field": "amount_abs"}},
                    "avg_amount": {"avg": {"field": "amount_abs"}}
                }
            }
        elif aggregation_type == "by_merchant":
            agg_query["aggs"]["merchants"] = {
                "terms": {
                    "field": "merchant_name.keyword", 
                    "size": size
                },
                "aggs": {
                    "total_amount": {"sum": {"field": "amount_abs"}},
                    "transaction_count": {"value_count": {"field": "transaction_id"}}
                }
            }
        elif aggregation_type == "by_month":
            agg_query["aggs"]["months"] = {
                "terms": {
                    "field": "month_year",
                    "size": size,
                    "order": {"_key": "desc"}
                },
                "aggs": {
                    "total_amount": {"sum": {"field": "amount_abs"}},
                    "transaction_count": {"value_count": {"field": "transaction_id"}}
                }
            }
        else:
            raise ValueError(f"Unsupported aggregation type: {aggregation_type}")
        
        result = await self.search(
            index=self.index_name,
            body=agg_query
        )
        
        # Statistiques
        self.query_stats["aggregation_count"] += 1
        
        return result
    
    # ============================================================================
    # CACHE ET OPTIMISATIONS
    # ============================================================================
    
    def _generate_cache_key(self, index: str, body: Dict[str, Any]) -> str:
        """G√©n√®re une cl√© de cache pour une requ√™te"""
        import hashlib
        import json
        
        # Cr√©er une repr√©sentation stable de la requ√™te
        cache_data = {
            "index": index,
            "body": body
        }
        
        cache_str = json.dumps(cache_data, sort_keys=True)
        return f"es_query:{hashlib.md5(cache_str.encode()).hexdigest()}"
    
    def _add_to_cache(self, cache_key: str, result: Dict[str, Any]):
        """Ajoute un r√©sultat au cache avec gestion LRU simple"""
        if len(self._query_cache) >= self._max_cache_size:
            # Supprimer le plus ancien (LRU simple)
            oldest_key = next(iter(self._query_cache))
            del self._query_cache[oldest_key]
        
        self._query_cache[cache_key] = result
    
    def clear_cache(self):
        """Vide le cache des requ√™tes"""
        self._query_cache.clear()
        logger.info("Elasticsearch query cache cleared")
    
    def get_query_stats(self) -> Dict[str, Any]:
        """Retourne les statistiques sp√©cialis√©es Elasticsearch"""
        base_metrics = self.get_metrics()
        
        return {
            **base_metrics,
            "elasticsearch_stats": {
                **self.query_stats,
                "cache_size": len(self._query_cache),
                "cache_hit_rate": (
                    self.query_stats["cache_hits"] / max(self.query_stats["search_count"], 1)
                )
            }
        }
    
    def reset_query_stats(self):
        """Remet √† z√©ro les statistiques de requ√™tes"""
        self.query_stats = {
            "search_count": 0,
            "count_count": 0, 
            "aggregation_count": 0,
            "cache_hits": 0
        }
        self.reset_metrics()
        logger.info("Elasticsearch query stats reset")


# === EXPORTS ===

__all__ = [
    "ElasticsearchClient"
]