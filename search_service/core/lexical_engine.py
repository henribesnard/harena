"""
Moteur de recherche lexicale - Composant Core #4
Orchestrateur principal qui utilise tous les autres composants
"""

import logging
import asyncio
from typing import Dict, List, Any, Optional, Union
from datetime import datetime

from search_service.core.query_executor import QueryExecutor
from search_service.core.result_processor import ResultProcessor
from search_service.core.performance_optimizer import PerformanceOptimizer, OptimizationLevel
from search_service.models.service_contracts import SearchServiceQuery, SearchServiceResponse
from search_service.templates.query_templates import QueryTemplateManager
from search_service.clients.elasticsearch_client import ElasticsearchClient
from search_service.utils.validators import QueryValidator
from search_service.utils.metrics import MetricsCollector

logger = logging.getLogger(__name__)


class LexicalEngine:
    """
    Moteur de recherche lexicale principal
    
    Responsabilit√©s:
    - Orchestration de tous les composants de recherche
    - Ex√©cution des recherches lexicales optimis√©es
    - Gestion du cache et des performances
    - Interface unifi√©e pour le Search Service
    """
    
    def __init__(
        self,
        elasticsearch_client: ElasticsearchClient,
        optimization_level: OptimizationLevel = OptimizationLevel.STANDARD,
        enable_metrics: bool = True
    ):
        self.es_client = elasticsearch_client
        self.optimization_level = optimization_level
        self.enable_metrics = enable_metrics
        
        # Initialisation des composants
        self._initialize_components()
        
        # √âtat du moteur
        self.is_initialized = False
        self.startup_time = None
        
        logger.info("‚úÖ LexicalEngine cr√©√©")
    
    def _initialize_components(self):
        """
        Initialise tous les composants du moteur
        """
        try:
            # Template manager
            self.template_manager = QueryTemplateManager()
            
            # Query executor
            self.query_executor = QueryExecutor(
                elasticsearch_client=self.es_client.client,
                template_manager=self.template_manager
            )
            
            # Result processor
            self.result_processor = ResultProcessor()
            
            # Performance optimizer
            self.performance_optimizer = PerformanceOptimizer(
                optimization_level=self.optimization_level
            )
            
            # Validator
            self.validator = QueryValidator()
            
            # Metrics collector
            if self.enable_metrics:
                self.metrics_collector = MetricsCollector()
            else:
                self.metrics_collector = None
            
            logger.info("‚úÖ Tous les composants du moteur initialis√©s")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'initialisation des composants: {e}")
            raise
    
    async def initialize(self):
        """
        Initialise le moteur de mani√®re asynchrone
        """
        try:
            start_time = datetime.now()
            
            # V√©rification de la connexion Elasticsearch
            await self.es_client.health_check()
            
            # Initialisation des templates
            await self.template_manager.initialize()
            
            # Pr√©chauffage du cache si activ√©
            if self.optimization_level in [OptimizationLevel.STANDARD, OptimizationLevel.AGGRESSIVE]:
                await self._warm_cache()
            
            self.is_initialized = True
            self.startup_time = datetime.now() - start_time
            
            logger.info(f"‚úÖ LexicalEngine initialis√© en {self.startup_time.total_seconds():.3f}s")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'initialisation du moteur: {e}")
            raise
    
    async def search(self, query: SearchServiceQuery) -> SearchServiceResponse:
        """
        Ex√©cute une recherche lexicale
        
        Args:
            query: Requ√™te de recherche
            
        Returns:
            R√©ponse de recherche format√©e
        """
        try:
            if not self.is_initialized:
                await self.initialize()
            
            start_time = datetime.now()
            
            # Validation de la requ√™te
            self.validator.validate_search_query(query)
            
            # Optimisation de la requ√™te
            optimized_query = await self.performance_optimizer.optimize_query(query)
            
            # Ex√©cution avec cache
            raw_results = await self.performance_optimizer.execute_with_cache(
                optimized_query,
                self.query_executor.execute_query
            )
            
            # Traitement des r√©sultats
            processed_results = await self.result_processor.process_search_results(
                raw_results,
                query_text=query.query,
                include_aggregations=bool(query.aggregations)
            )
            
            # Calcul des scores de pertinence
            final_results = self.result_processor.calculate_relevance_scores(processed_results)
            
            # M√©triques
            execution_time = datetime.now() - start_time
            if self.metrics_collector:
                await self._record_search_metrics(query, final_results, execution_time)
            
            logger.info(f"‚úÖ Recherche ex√©cut√©e: {len(final_results.hits)} r√©sultats en {execution_time.total_seconds():.3f}s")
            
            return final_results
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la recherche: {e}")
            raise
    
    async def multi_search(self, queries: List[SearchServiceQuery]) -> List[SearchServiceResponse]:
        """
        Ex√©cute plusieurs recherches en parall√®le
        
        Args:
            queries: Liste des requ√™tes
            
        Returns:
            Liste des r√©ponses
        """
        try:
            if not self.is_initialized:
                await self.initialize()
            
            start_time = datetime.now()
            
            # Validation des requ√™tes
            for query in queries:
                self.validator.validate_search_query(query)
            
            # Optimisation des requ√™tes
            optimized_queries = []
            for query in queries:
                optimized = await self.performance_optimizer.optimize_query(query)
                optimized_queries.append(optimized)
            
            # Ex√©cution en batch optimis√©
            raw_results = await self.performance_optimizer.execute_batch_optimized(
                optimized_queries,
                self.query_executor.execute_query
            )
            
            # Traitement des r√©sultats en parall√®le
            processing_tasks = []
            for i, raw_result in enumerate(raw_results):
                task = self.result_processor.process_search_results(
                    raw_result,
                    query_text=queries[i].query,
                    include_aggregations=bool(queries[i].aggregations)
                )
                processing_tasks.append(task)
            
            processed_results = await asyncio.gather(*processing_tasks)
            
            # Calcul des scores de pertinence
            final_results = []
            for result in processed_results:
                final_result = self.result_processor.calculate_relevance_scores(result)
                final_results.append(final_result)
            
            # M√©triques
            execution_time = datetime.now() - start_time
            if self.metrics_collector:
                await self._record_multi_search_metrics(queries, final_results, execution_time)
            
            total_hits = sum(len(result.hits) for result in final_results)
            logger.info(f"‚úÖ Multi-recherche ex√©cut√©e: {len(queries)} requ√™tes, {total_hits} r√©sultats en {execution_time.total_seconds():.3f}s")
            
            return final_results
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la multi-recherche: {e}")
            raise
    
    async def search_with_aggregations(self, query: SearchServiceQuery) -> SearchServiceResponse:
        """
        Ex√©cute une recherche avec focus sur les agr√©gations
        
        Args:
            query: Requ√™te avec agr√©gations
            
        Returns:
            R√©ponse avec agr√©gations d√©taill√©es
        """
        try:
            if not self.is_initialized:
                await self.initialize()
            
            start_time = datetime.now()
            
            # Validation
            self.validator.validate_search_query(query)
            if not query.aggregations:
                raise ValueError("Aucune agr√©gation sp√©cifi√©e")
            
            # Optimisation sp√©cifique aux agr√©gations
            optimized_query = await self.performance_optimizer.optimize_query(query)
            
            # Ex√©cution des agr√©gations
            raw_results = await self.performance_optimizer.execute_with_cache(
                optimized_query,
                self.query_executor.execute_aggregation
            )
            
            # Traitement sp√©cialis√© pour les agr√©gations
            processed_results = await self.result_processor.process_search_results(
                raw_results,
                query_text=query.query,
                include_aggregations=True
            )
            
            # M√©triques
            execution_time = datetime.now() - start_time
            if self.metrics_collector:
                await self._record_aggregation_metrics(query, processed_results, execution_time)
            
            logger.info(f"‚úÖ Recherche avec agr√©gations: {len(processed_results.aggregations)} agr√©gations en {execution_time.total_seconds():.3f}s")
            
            return processed_results
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la recherche avec agr√©gations: {e}")
            raise
    
    async def suggest_queries(self, partial_query: str, limit: int = 5) -> List[str]:
        """
        Sugg√®re des requ√™tes bas√©es sur un texte partiel
        
        Args:
            partial_query: Texte partiel
            limit: Nombre de suggestions
            
        Returns:
            Liste de suggestions
        """
        try:
            if not self.is_initialized:
                await self.initialize()
            
            # Construction d'une requ√™te de suggestion
            suggestion_query = SearchServiceQuery(
                query=partial_query,
                search_type="suggest",
                size=limit,
                index="financial_documents"
            )
            
            # Utilisation du template de suggestion
            suggestions = await self.template_manager.get_query_suggestions(partial_query, limit)
            
            logger.info(f"‚úÖ {len(suggestions)} suggestions g√©n√©r√©es pour '{partial_query}'")
            
            return suggestions
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la g√©n√©ration de suggestions: {e}")
            return []
    
    async def explain_query(self, query: SearchServiceQuery) -> Dict[str, Any]:
        """
        Explique comment une requ√™te sera ex√©cut√©e
        
        Args:
            query: Requ√™te √† expliquer
            
        Returns:
            Explication d√©taill√©e
        """
        try:
            # Optimisation de la requ√™te
            optimized_query = await self.performance_optimizer.optimize_query(query)
            
            # Construction de la requ√™te Elasticsearch
            es_query = await self.query_executor._build_elasticsearch_query(optimized_query)
            
            # Explication
            explanation = {
                "original_query": {
                    "text": query.query,
                    "type": query.search_type,
                    "filters": len(query.filters) if query.filters else 0,
                    "aggregations": len(query.aggregations) if query.aggregations else 0
                },
                "optimized_query": {
                    "text": optimized_query.query,
                    "type": optimized_query.search_type,
                    "size": optimized_query.size,
                    "timeout": optimized_query.timeout
                },
                "elasticsearch_query": {
                    "index": es_query.index,
                    "body_preview": str(es_query.body)[:500] + "..." if len(str(es_query.body)) > 500 else str(es_query.body)
                },
                "template_used": self.template_manager.get_template_name_for_query(query),
                "optimization_level": self.optimization_level.value,
                "cache_key": self.performance_optimizer._generate_cache_key(optimized_query)
            }
            
            return explanation
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'explication de la requ√™te: {e}")
    
    async def health_check(self) -> Dict[str, Any]:
        """
        V√©rifie la sant√© du moteur lexical
        
        Returns:
            √âtat de sant√© d√©taill√©
        """
        try:
            health_status = {
                "status": "healthy",
                "timestamp": datetime.now().isoformat(),
                "components": {}
            }
            
            # V√©rification Elasticsearch
            try:
                es_health = await self.es_client.health_check()
                health_status["components"]["elasticsearch"] = {
                    "status": "healthy" if es_health["status"] == "ok" else "unhealthy",
                    "details": es_health
                }
            except Exception as e:
                health_status["components"]["elasticsearch"] = {
                    "status": "unhealthy",
                    "error": str(e)
                }
                health_status["status"] = "degraded"
            
            # V√©rification des composants internes
            components = {
                "query_executor": self.query_executor,
                "result_processor": self.result_processor,
                "performance_optimizer": self.performance_optimizer,
                "template_manager": self.template_manager
            }
            
            for name, component in components.items():
                try:
                    # Test basique de fonctionnement
                    if hasattr(component, 'get_stats'):
                        component.get_stats()
                    health_status["components"][name] = {"status": "healthy"}
                except Exception as e:
                    health_status["components"][name] = {
                        "status": "unhealthy",
                        "error": str(e)
                    }
                    health_status["status"] = "degraded"
            
            # Test de requ√™te simple
            try:
                test_query = SearchServiceQuery(
                    query="test",
                    size=1,
                    timeout="5s"
                )
                await self.search(test_query)
                health_status["components"]["search_functionality"] = {"status": "healthy"}
            except Exception as e:
                health_status["components"]["search_functionality"] = {
                    "status": "unhealthy",
                    "error": str(e)
                }
                health_status["status"] = "unhealthy"
            
            return health_status
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du health check: {e}")
            return {
                "status": "unhealthy",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
    
    async def optimize_performance(self, level: OptimizationLevel):
        """
        Change le niveau d'optimisation du moteur
        
        Args:
            level: Nouveau niveau d'optimisation
        """
        try:
            old_level = self.optimization_level
            self.optimization_level = level
            
            # Recr√©ation de l'optimiseur avec le nouveau niveau
            self.performance_optimizer = PerformanceOptimizer(optimization_level=level)
            
            # R√©initialisation du cache si niveau plus agressif
            if level.value == OptimizationLevel.AGGRESSIVE.value:
                await self._warm_cache()
            
            logger.info(f"‚úÖ Niveau d'optimisation chang√©: {old_level.value} ‚Üí {level.value}")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du changement d'optimisation: {e}")
            raise
    
    async def clear_all_caches(self):
        """
        Vide tous les caches du moteur
        """
        try:
            # Cache de l'optimiseur
            self.performance_optimizer.clear_cache()
            
            # Cache du processeur de r√©sultats
            self.result_processor.clear_cache()
            
            # Cache du template manager
            if hasattr(self.template_manager, 'clear_cache'):
                self.template_manager.clear_cache()
            
            logger.info("üßπ Tous les caches vid√©s")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du vidage des caches: {e}")
            raise
    
    async def rebuild_index_templates(self):
        """
        Reconstruit les templates d'index Elasticsearch
        """
        try:
            await self.template_manager.rebuild_templates()
            logger.info("üîÑ Templates d'index reconstruits")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la reconstruction des templates: {e}")
            raise
    
    async def validate_configuration(self) -> Dict[str, Any]:
        """
        Valide la configuration du moteur
        
        Returns:
            Rapport de validation
        """
        try:
            validation_report = {
                "status": "valid",
                "checks": {},
                "warnings": [],
                "errors": []
            }
            
            # V√©rification de la connexion Elasticsearch
            try:
                await self.es_client.health_check()
                validation_report["checks"]["elasticsearch_connection"] = "‚úÖ OK"
            except Exception as e:
                validation_report["checks"]["elasticsearch_connection"] = f"‚ùå {str(e)}"
                validation_report["errors"].append(f"Connexion Elasticsearch: {e}")
                validation_report["status"] = "invalid"
            
            # V√©rification des templates
            try:
                template_count = await self.template_manager.get_template_count()
                if template_count > 0:
                    validation_report["checks"]["templates"] = f"‚úÖ {template_count} templates charg√©s"
                else:
                    validation_report["checks"]["templates"] = "‚ö†Ô∏è Aucun template charg√©"
                    validation_report["warnings"].append("Aucun template de requ√™te disponible")
            except Exception as e:
                validation_report["checks"]["templates"] = f"‚ùå {str(e)}"
                validation_report["errors"].append(f"Templates: {e}")
            
            # V√©rification de la configuration du cache
            cache_stats = self.performance_optimizer.get_cache_stats()
            if cache_stats["max_size"] > 0:
                validation_report["checks"]["cache"] = f"‚úÖ Cache configur√© ({cache_stats['max_size']} entr√©es max)"
            else:
                validation_report["checks"]["cache"] = "‚ö†Ô∏è Cache d√©sactiv√©"
                validation_report["warnings"].append("Cache d√©sactiv√© - performances r√©duites")
            
            # V√©rification des m√©triques
            if self.enable_metrics and self.metrics_collector:
                validation_report["checks"]["metrics"] = "‚úÖ M√©triques activ√©es"
            else:
                validation_report["checks"]["metrics"] = "‚ö†Ô∏è M√©triques d√©sactiv√©es"
                validation_report["warnings"].append("M√©triques d√©sactiv√©es - monitoring limit√©")
            
            return validation_report
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la validation: {e}")
            return {
                "status": "error",
                "error": str(e)
            }
    
    async def export_configuration(self) -> Dict[str, Any]:
        """
        Exporte la configuration actuelle du moteur
        
        Returns:
            Configuration export√©e
        """
        try:
            config = {
                "engine_version": "1.0.0",
                "export_timestamp": datetime.now().isoformat(),
                "optimization_level": self.optimization_level.value,
                "elasticsearch_config": self.es_client.get_connection_info(),
                "performance_settings": {
                    "cache_size": self.performance_optimizer.query_cache.maxsize,
                    "query_timeout": self.performance_optimizer.query_timeout,
                    "batch_size": self.performance_optimizer.batch_size
                },
                "enabled_features": {
                    "metrics": self.enable_metrics,
                    "cache_warming": self.performance_optimizer.warm_cache_enabled
                },
                "optimization_rules": [
                    {
                        "name": rule.name,
                        "enabled": rule.enabled,
                        "priority": rule.priority
                    }
                    for rule in self.performance_optimizer.optimization_rules
                ]
            }
            
            return config
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'export de configuration: {e}")
            return {"error": str(e)}
    
    async def shutdown(self):
        """
        Arr√™t propre du moteur
        """
        try:
            logger.info("üîÑ Arr√™t du moteur lexical...")
            
            # Sauvegarde des m√©triques si n√©cessaire
            if self.metrics_collector:
                await self.metrics_collector.flush_metrics()
            
            # Fermeture des connexions
            await self.es_client.close()
            
            # Nettoyage des caches
            await self.clear_all_caches()
            
            self.is_initialized = False
            
            logger.info("‚úÖ Moteur lexical arr√™t√© proprement")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'arr√™t: {e}")
            raise
    
    def __str__(self) -> str:
        """
        Repr√©sentation string du moteur
        
        Returns:
            Description du moteur
        """
        return (
            f"LexicalEngine("
            f"initialized={self.is_initialized}, "
            f"optimization={self.optimization_level.value}, "
            f"metrics={self.enable_metrics}, "
            f"elasticsearch={self.es_client.get_connection_info()['host']}"
            f")"
        )
    
    def __repr__(self) -> str:
        """
        Repr√©sentation pour debug
        
        Returns:
            Repr√©sentation d√©taill√©e
        """
        return self.__str__()


class LexicalEngineFactory:
    """
    Factory pour cr√©er des instances de LexicalEngine
    """
    
    @staticmethod
    async def create_engine(
        elasticsearch_config: Dict[str, Any],
        optimization_level: OptimizationLevel = OptimizationLevel.STANDARD,
        enable_metrics: bool = True
    ) -> LexicalEngine:
        """
        Cr√©e et initialise un moteur lexical
        
        Args:
            elasticsearch_config: Configuration Elasticsearch
            optimization_level: Niveau d'optimisation
            enable_metrics: Activer les m√©triques
            
        Returns:
            Moteur lexical initialis√©
        """
        try:
            # Cr√©ation du client Elasticsearch
            es_client = ElasticsearchClient(elasticsearch_config)
            await es_client.connect()
            
            # Cr√©ation du moteur
            engine = LexicalEngine(
                elasticsearch_client=es_client,
                optimization_level=optimization_level,
                enable_metrics=enable_metrics
            )
            
            # Initialisation
            await engine.initialize()
            
            logger.info("‚úÖ Moteur lexical cr√©√© et initialis√© via Factory")
            
            return engine
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la cr√©ation du moteur: {e}")
            raise
    
    @staticmethod
    def create_development_engine() -> LexicalEngine:
        """
        Cr√©e un moteur pour le d√©veloppement
        
        Returns:
            Moteur configur√© pour le d√©veloppement
        """
        config = {
            "host": "localhost",
            "port": 9200,
            "timeout": 30
        }
        
        return LexicalEngineFactory.create_engine(
            elasticsearch_config=config,
            optimization_level=OptimizationLevel.BASIC,
            enable_metrics=True
        )
    
    @staticmethod
    def create_production_engine(elasticsearch_config: Dict[str, Any]) -> LexicalEngine:
        """
        Cr√©e un moteur pour la production
        
        Args:
            elasticsearch_config: Configuration Elasticsearch
            
        Returns:
            Moteur configur√© pour la production
        """
        return LexicalEngineFactory.create_engine(
            elasticsearch_config=elasticsearch_config,
            optimization_level=OptimizationLevel.AGGRESSIVE,
            enable_metrics=True
        )
    
    async def _warm_cache(self):
        """
        Pr√©chauffe le cache avec des requ√™tes communes
        """
        try:
            # Requ√™tes communes pour le pr√©chauffage
            common_queries = [
                SearchServiceQuery(query="", search_type="match_all", size=20),
                SearchServiceQuery(query="financial", size=10),
                SearchServiceQuery(query="report", size=10),
                SearchServiceQuery(query="analysis", size=10),
            ]
            
            self.performance_optimizer.warm_cache(
                common_queries,
                self.query_executor.execute_query
            )
            
            logger.info("üî• Cache pr√©chauff√© avec les requ√™tes communes")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur lors du pr√©chauffage du cache: {e}")
    
    async def _record_search_metrics(
        self, 
        query: SearchServiceQuery, 
        results: SearchServiceResponse, 
        execution_time
    ):
        """
        Enregistre les m√©triques de recherche
        
        Args:
            query: Requ√™te ex√©cut√©e
            results: R√©sultats obtenus
            execution_time: Temps d'ex√©cution
        """
        try:
            if not self.metrics_collector:
                return
            
            metrics = {
                "operation": "search",
                "query_text": query.query,
                "query_type": query.search_type,
                "result_count": len(results.hits),
                "execution_time_ms": execution_time.total_seconds() * 1000,
                "has_aggregations": len(results.aggregations) > 0,
                "cache_hit": results.metadata.processing_time < 10,  # Heuristique
                "optimization_level": self.optimization_level.value
            }
            
            await self.metrics_collector.record_metrics("lexical_search", metrics)
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'enregistrement des m√©triques: {e}")
    
    async def _record_multi_search_metrics(
        self, 
        queries: List[SearchServiceQuery], 
        results: List[SearchServiceResponse], 
        execution_time
    ):
        """
        Enregistre les m√©triques de multi-recherche
        
        Args:
            queries: Requ√™tes ex√©cut√©es
            results: R√©sultats obtenus
            execution_time: Temps d'ex√©cution total
        """
        try:
            if not self.metrics_collector:
                return
            
            total_results = sum(len(result.hits) for result in results)
            
            metrics = {
                "operation": "multi_search",
                "query_count": len(queries),
                "total_results": total_results,
                "execution_time_ms": execution_time.total_seconds() * 1000,
                "avg_time_per_query": (execution_time.total_seconds() * 1000) / len(queries),
                "optimization_level": self.optimization_level.value
            }
            
            await self.metrics_collector.record_metrics("lexical_multi_search", metrics)
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'enregistrement des m√©triques multi-recherche: {e}")
    
    async def _record_aggregation_metrics(
        self, 
        query: SearchServiceQuery, 
        results: SearchServiceResponse, 
        execution_time
    ):
        """
        Enregistre les m√©triques d'agr√©gation
        
        Args:
            query: Requ√™te avec agr√©gations
            results: R√©sultats avec agr√©gations
            execution_time: Temps d'ex√©cution
        """
        try:
            if not self.metrics_collector:
                return
            
            metrics = {
                "operation": "aggregation_search",
                "aggregation_count": len(results.aggregations),
                "execution_time_ms": execution_time.total_seconds() * 1000,
                "result_count": len(results.hits),
                "optimization_level": self.optimization_level.value
            }
            
            await self.metrics_collector.record_metrics("lexical_aggregation", metrics)
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'enregistrement des m√©triques d'agr√©gation: {e}")
    
    def get_engine_stats(self) -> Dict[str, Any]:
        """
        Retourne les statistiques compl√®tes du moteur
        
        Returns:
            Statistiques d√©taill√©es
        """
        try:
            stats = {
                "engine_info": {
                    "is_initialized": self.is_initialized,
                    "startup_time_seconds": self.startup_time.total_seconds() if self.startup_time else None,
                    "optimization_level": self.optimization_level.value,
                    "metrics_enabled": self.enable_metrics
                },
                "elasticsearch_client": self.es_client.get_client_stats(),
                "performance_optimizer": self.performance_optimizer.get_performance_stats(),
                "result_processor": self.result_processor.get_processing_stats(),
                "query_executor": self.query_executor.get_query_stats(),
                "template_manager": self.template_manager.get_template_stats(),
                "cache_stats": self.performance_optimizer.get_cache_stats()
            }
            
            if self.metrics_collector:
                stats["metrics"] = self.metrics_collector.get_metrics_summary()
            
            return stats
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la r√©cup√©ration des statistiques: {e}")
            return {"error": str(e)}