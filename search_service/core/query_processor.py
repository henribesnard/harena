"""
Processeur de requ√™tes intelligent pour la recherche - VERSION CENTRALIS√âE.

Ce module traite et enrichit les requ√™tes de recherche avant
de les envoyer aux moteurs lexical et s√©mantique.

CENTRALIS√â VIA CONFIG_SERVICE:
- Toutes les configurations viennent de config_service.config.settings
- Patterns de d√©tection, cache, synonymes configurables
- Compatible avec les autres moteurs centralis√©s
"""
import re
import logging
from typing import Dict, Any, List, Optional, Tuple, Set
from dataclasses import dataclass
from datetime import datetime, timedelta
from decimal import Decimal

# ‚úÖ CONFIGURATION CENTRALIS√âE - SEULE SOURCE DE V√âRIT√â
from config_service.config import settings

from search_service.models.search_types import FINANCIAL_SYNONYMS

logger = logging.getLogger(__name__)


@dataclass
class QueryAnalysis:
    """R√©sultat de l'analyse d'une requ√™te."""
    original_query: str
    cleaned_query: str
    expanded_query: str
    detected_entities: Dict[str, Any]
    query_type: str
    confidence: float
    suggested_filters: Dict[str, Any]
    processing_notes: List[str]
    
    # ============================================================================
    # PROPRI√âT√âS POUR COMPATIBILIT√â AVEC LES MOTEURS
    # ============================================================================
    
    @property
    def key_terms(self) -> List[str]:
        """
        Retourne les termes cl√©s extraits de la requ√™te.
        
        Cette propri√©t√© est utilis√©e par les moteurs lexical et s√©mantique
        pour analyser les mots-cl√©s importants.
        
        Returns:
            Liste des mots-cl√©s extraits
        """
        return self.detected_entities.get('keywords', [])
    
    @property
    def has_exact_phrases(self) -> bool:
        """
        V√©rifie si la requ√™te contient des phrases exactes.
        
        Cette propri√©t√© est utilis√©e par les moteurs pour d√©terminer
        si des correspondances de phrase exacte doivent √™tre privil√©gi√©es.
        
        Returns:
            True si la requ√™te contient des phrases exactes
        """
        # D√©tecter les guillemets dans la requ√™te originale
        if '"' in self.original_query:
            return True
        
        # Consid√©rer les requ√™tes courtes comme des phrases exactes potentielles
        if len(self.cleaned_query.split()) <= 3 and len(self.cleaned_query.split()) > 0:
            return True
        
        # D√©tecter des patterns de phrases exactes
        exact_phrase_patterns = [
            r'\b\w+\s+\w+\b',  # Deux mots cons√©cutifs
            r'[A-Z][a-z]+\s+[A-Z][a-z]+',  # Noms propres
        ]
        
        for pattern in exact_phrase_patterns:
            if re.search(pattern, self.original_query):
                return True
        
        return False
    
    @property
    def has_financial_entities(self) -> bool:
        """
        V√©rifie si la requ√™te contient des entit√©s financi√®res.
        
        Cette propri√©t√© est utilis√©e par les moteurs pour adapter
        leur strat√©gie de recherche aux requ√™tes financi√®res.
        
        Returns:
            True si la requ√™te contient des entit√©s financi√®res
        """
        entities = self.detected_entities
        
        # V√©rifier la pr√©sence d'entit√©s financi√®res extraites
        has_amounts = bool(entities.get('amounts', []))
        has_categories = bool(entities.get('categories', []))
        
        if has_amounts or has_categories:
            return True
        
        # V√©rifier la pr√©sence de mots-cl√©s financiers dans le texte
        financial_keywords = [
            'euro', '‚Ç¨', 'montant', 'prix', 'co√ªt', 'transaction', 'virement',
            'paiement', 'carte', 'bancaire', 'banque', 'compte', 'solde',
            'd√©bit', 'cr√©dit', 'facture', 'achat', 'vente', 'remboursement',
            'salaire', 'paie', 'pension', 'allocation', 'frais', 'commission'
        ]
        
        query_lower = self.cleaned_query.lower()
        return any(keyword in query_lower for keyword in financial_keywords)
    
    @property
    def is_question(self) -> bool:
        """
        V√©rifie si la requ√™te est une question.
        
        Cette propri√©t√© est utilis√©e par les moteurs pour adapter
        leur traitement aux requ√™tes interrogatives.
        
        Returns:
            True si la requ√™te est une question
        """
        # V√©rifier la pr√©sence d'un point d'interrogation
        if self.original_query.endswith('?'):
            return True
        
        # V√©rifier la pr√©sence de mots interrogatifs
        question_words = [
            "quoi", "comment", "pourquoi", "o√π", "quand", "qui", 
            "quel", "quelle", "quels", "quelles", "combien",
            "qu'est-ce", "est-ce", "y a-t-il", "peut-on", "peut-il"
        ]
        
        query_lower = self.original_query.lower()
        
        # V√©rifier si la requ√™te commence par un mot interrogatif
        for word in question_words:
            if query_lower.startswith(word) or f" {word} " in query_lower:
                return True
        
        # V√©rifier des patterns interrogatifs
        interrogative_patterns = [
            r'\bcomment\s+',
            r'\bpourquoi\s+',
            r'\bquand\s+',
            r'\bo√π\s+',
            r'\bque\s+',
            r'\bqui\s+',
            r'\bcombien\s+'
        ]
        
        for pattern in interrogative_patterns:
            if re.search(pattern, query_lower):
                return True
        
        return False
    
    @property
    def exact_phrases(self) -> List[str]:
        """
        Retourne les phrases exactes d√©tect√©es dans la requ√™te.
        
        Returns:
            Liste des phrases exactes
        """
        phrases = []
        
        # Extraire les phrases entre guillemets
        quoted_phrases = re.findall(r'"([^"]*)"', self.original_query)
        phrases.extend(quoted_phrases)
        
        # Si pas de guillemets mais requ√™te courte, consid√©rer comme phrase exacte
        if not phrases and self.has_exact_phrases:
            if len(self.cleaned_query.split()) <= 3:
                phrases.append(self.cleaned_query)
        
        return phrases
    
    @property
    def financial_entities_summary(self) -> Dict[str, Any]:
        """
        Retourne un r√©sum√© des entit√©s financi√®res d√©tect√©es.
        
        Returns:
            R√©sum√© structur√© des entit√©s financi√®res
        """
        entities = self.detected_entities
        
        return {
            "amounts_count": len(entities.get('amounts', [])),
            "dates_count": len(entities.get('dates', [])),
            "categories_count": len(entities.get('categories', [])),
            "has_amounts": bool(entities.get('amounts', [])),
            "has_dates": bool(entities.get('dates', [])),
            "has_categories": bool(entities.get('categories', [])),
            "amount_range": self._get_amount_range(),
            "date_range": self._get_date_range(),
            "detected_categories": entities.get('categories', [])
        }
    
    def _get_amount_range(self) -> Optional[Dict[str, float]]:
        """Calcule la fourchette de montants d√©tect√©s."""
        amounts = self.detected_entities.get('amounts', [])
        if not amounts:
            return None
        
        values = [a['value'] for a in amounts]
        return {
            "min": min(values),
            "max": max(values),
            "count": len(values)
        }
    
    def _get_date_range(self) -> Optional[Dict[str, str]]:
        """Calcule la fourchette de dates d√©tect√©es."""
        dates = self.detected_entities.get('dates', [])
        if not dates:
            return None
        
        date_objects = []
        for d in dates:
            if isinstance(d.get('date'), str):
                try:
                    date_objects.append(datetime.fromisoformat(d['date']))
                except ValueError:
                    continue
            elif isinstance(d.get('date'), datetime):
                date_objects.append(d['date'])
        
        if not date_objects:
            return None
        
        return {
            "start": min(date_objects).isoformat(),
            "end": max(date_objects).isoformat(),
            "count": len(date_objects)
        }


@dataclass
class EntityExtraction:
    """Entit√©s extraites d'une requ√™te."""
    amounts: List[Dict[str, Any]]
    dates: List[Dict[str, Any]]
    merchants: List[str]
    categories: List[str]
    keywords: List[str]
    negations: List[str]


class QueryProcessor:
    """
    Processeur intelligent de requ√™tes de recherche.
    
    Fonctionnalit√©s:
    - Nettoyage et normalisation du texte
    - Extraction d'entit√©s financi√®res
    - Expansion avec synonymes
    - D√©tection de type de requ√™te
    - Suggestion de filtres automatiques
    - Correction orthographique basique
    
    CONFIGURATION CENTRALIS√âE VIA CONFIG_SERVICE.
    """
    
    def __init__(self):
        # Patterns de regex pour l'extraction d'entit√©s (configurables via settings)
        self.amount_patterns = [
            r'(\d+(?:[,.]?\d{3})*(?:[,.]\d{2})?)\s*‚Ç¨?',  # 1000.50‚Ç¨, 1,000.50
            r'‚Ç¨\s*(\d+(?:[,.]?\d{3})*(?:[,.]\d{2})?)',   # ‚Ç¨1000.50
            r'(\d+(?:[,.]?\d{3})*(?:[,.]\d{2})?)\s*euros?', # 1000 euros
        ]
        
        self.date_patterns = [
            r'(\d{1,2})[/\-](\d{1,2})[/\-](\d{4})',      # DD/MM/YYYY
            r'(\d{4})[/\-](\d{1,2})[/\-](\d{1,2})',      # YYYY/MM/DD
            r'(\d{1,2})\s+(janvier|f√©vrier|mars|avril|mai|juin|juillet|ao√ªt|septembre|octobre|novembre|d√©cembre)\s+(\d{4})',
        ]
        
        # Mots-cl√©s par cat√©gorie (pourraient √™tre configur√©s via settings)
        self.category_keywords = {
            "restaurant": ["restaurant", "resto", "brasserie", "caf√©", "fast", "food", "mcdo", "burger", "pizza"],
            "supermarch√©": ["supermarch√©", "hypermarch√©", "courses", "carrefour", "leclerc", "auchan", "intermarch√©"],
            "essence": ["essence", "carburant", "station", "shell", "total", "bp", "esso"],
            "pharmacie": ["pharmacie", "parapharmacie", "m√©dicament", "docteur", "sant√©"],
            "transport": ["transport", "m√©tro", "bus", "train", "taxi", "uber", "sncf", "ratp"],
            "banque": ["banque", "virement", "retrait", "distributeur", "agios", "frais"],
            "abonnement": ["abonnement", "subscription", "mensuel", "annuel", "netflix", "spotify"],
            "shopping": ["shopping", "v√™tement", "chaussure", "magasin", "boutique", "amazon"]
        }
        
        # Mots de n√©gation (configurables)
        self.negation_words = ["pas", "non", "sans", "sauf", "except√©", "hormis", "ne", "n'"]
        
        # Mots vides sp√©cifiques au domaine financier (configurables)
        self.stop_words = {
            "le", "la", "les", "un", "une", "des", "du", "de", "d'", "et", "ou", "√†", "au", "aux",
            "dans", "sur", "pour", "avec", "par", "chez", "vers", "entre", "depuis", "jusqu",
            "ce", "cette", "ces", "mon", "ma", "mes", "ton", "ta", "tes", "son", "sa", "ses"
        }
        
        # Cache des analyses r√©centes avec taille configur√©e
        self._analysis_cache: Dict[str, QueryAnalysis] = {}
        self._max_cache_size = getattr(settings, 'QUERY_PROCESSOR_CACHE_SIZE', 100)
        
        logger.info("Query processor initialized with centralized config")
    
    def process_query(self, query: str, user_context: Optional[Dict[str, Any]] = None) -> QueryAnalysis:
        """
        Traite une requ√™te de recherche compl√®te.
        
        Args:
            query: Requ√™te brute
            user_context: Contexte utilisateur (historique, pr√©f√©rences)
            
        Returns:
            Analyse compl√®te de la requ√™te
        """
        # V√©rifier le cache avec taille limit√©e
        cache_key = f"{query}:{hash(str(user_context))}"
        if cache_key in self._analysis_cache:
            return self._analysis_cache[cache_key]
        
        processing_notes = []
        
        # 1. Nettoyage initial
        cleaned_query = self._clean_query(query)
        processing_notes.append(f"Query cleaned: '{query}' -> '{cleaned_query}'")
        
        # 2. Extraction d'entit√©s
        entities = self._extract_entities(cleaned_query)
        processing_notes.append(f"Entities extracted: {len(entities.amounts)} amounts, {len(entities.dates)} dates")
        
        # 3. D√©tection du type de requ√™te
        query_type, confidence = self._detect_query_type(cleaned_query, entities)
        processing_notes.append(f"Query type: {query_type} (confidence: {confidence:.2f})")
        
        # 4. Expansion avec synonymes (activable via config)
        expanded_query = self._expand_query(cleaned_query, query_type)
        processing_notes.append(f"Query expanded with synonyms")
        
        # 5. Suggestion de filtres automatiques
        suggested_filters = self._suggest_filters(entities, query_type, user_context)
        if suggested_filters:
            processing_notes.append(f"Auto-filters suggested: {list(suggested_filters.keys())}")
        
        # 6. Cr√©er l'analyse finale
        analysis = QueryAnalysis(
            original_query=query,
            cleaned_query=cleaned_query,
            expanded_query=expanded_query,
            detected_entities=self._serialize_entities(entities),
            query_type=query_type,
            confidence=confidence,
            suggested_filters=suggested_filters,
            processing_notes=processing_notes
        )
        
        # Mettre en cache avec limite de taille
        if len(self._analysis_cache) >= self._max_cache_size:
            # Supprimer le plus ancien (simple FIFO)
            oldest_key = next(iter(self._analysis_cache))
            del self._analysis_cache[oldest_key]
        
        self._analysis_cache[cache_key] = analysis
        
        return analysis
    
    def _clean_query(self, query: str) -> str:
        """Nettoie et normalise la requ√™te."""
        if not query:
            return ""
        
        # Supprimer les espaces multiples et normaliser
        cleaned = re.sub(r'\s+', ' ', query.strip().lower())
        
        # Supprimer les caract√®res sp√©ciaux inutiles
        cleaned = re.sub(r'[^\w\s‚Ç¨.,/-]', ' ', cleaned)
        
        # Normaliser les s√©parateurs de montants
        cleaned = re.sub(r'(\d),(\d{3})', r'\1\2', cleaned)  # 1,000 -> 1000
        cleaned = re.sub(r'(\d)\.(\d{3})', r'\1\2', cleaned)  # 1.000 -> 1000
        
        # Supprimer les mots vides si la requ√™te est longue
        words = cleaned.split()
        if len(words) > 3:
            words = [w for w in words if w not in self.stop_words]
            cleaned = ' '.join(words)
        
        return cleaned.strip()
    
    def _extract_entities(self, query: str) -> EntityExtraction:
        """Extrait les entit√©s de la requ√™te."""
        entities = EntityExtraction(
            amounts=[], dates=[], merchants=[], categories=[], keywords=[], negations=[]
        )
        
        # Extraction des montants
        for pattern in self.amount_patterns:
            matches = re.finditer(pattern, query, re.IGNORECASE)
            for match in matches:
                amount_str = match.group(1).replace(',', '').replace('.', '')
                try:
                    # G√©rer les centimes
                    if '.' in match.group(1) or ',' in match.group(1):
                        amount_value = float(match.group(1).replace(',', ''))
                    else:
                        amount_value = float(amount_str)
                    
                    entities.amounts.append({
                        "value": amount_value,
                        "raw": match.group(0),
                        "position": match.span()
                    })
                except ValueError:
                    continue
        
        # Extraction des dates
        for pattern in self.date_patterns:
            matches = re.finditer(pattern, query, re.IGNORECASE)
            for match in matches:
                try:
                    date_info = self._parse_date_match(match)
                    if date_info:
                        entities.dates.append(date_info)
                except ValueError:
                    continue
        
        # Extraction des cat√©gories
        for category, keywords in self.category_keywords.items():
            for keyword in keywords:
                if keyword in query:
                    if category not in entities.categories:
                        entities.categories.append(category)
        
        # D√©tection des n√©gations
        for neg_word in self.negation_words:
            if neg_word in query:
                entities.negations.append(neg_word)
        
        # Extraction des mots-cl√©s importants
        words = query.split()
        for word in words:
            if len(word) > 2 and word not in self.stop_words:
                entities.keywords.append(word)
        
        return entities
    
    def _parse_date_match(self, match) -> Optional[Dict[str, Any]]:
        """Parse un match de date."""
        groups = match.groups()
        
        # Format DD/MM/YYYY ou DD-MM-YYYY
        if len(groups) == 3 and groups[0].isdigit() and groups[1].isdigit() and groups[2].isdigit():
            day, month, year = int(groups[0]), int(groups[1]), int(groups[2])
            
            # V√©rifier si c'est DD/MM/YYYY ou YYYY/MM/DD
            if year > 1900 and year < 2100:
                if len(groups[2]) == 4:  # YYYY √† la fin
                    date_obj = datetime(year, month, day)
                else:  # YYYY au d√©but
                    date_obj = datetime(int(groups[0]), int(groups[1]), int(groups[2]))
            else:
                return None
            
            return {
                "date": date_obj,
                "raw": match.group(0),
                "position": match.span(),
                "type": "specific"
            }
        
        # Format avec nom de mois
        elif len(groups) == 3:
            month_names = {
                "janvier": 1, "f√©vrier": 2, "mars": 3, "avril": 4, "mai": 5, "juin": 6,
                "juillet": 7, "ao√ªt": 8, "septembre": 9, "octobre": 10, "novembre": 11, "d√©cembre": 12
            }
            
            day = int(groups[0])
            month_name = groups[1].lower()
            year = int(groups[2])
            
            if month_name in month_names:
                month = month_names[month_name]
                date_obj = datetime(year, month, day)
                
                return {
                    "date": date_obj,
                    "raw": match.group(0),
                    "position": match.span(),
                    "type": "named_month"
                }
        
        return None
    
    def _detect_query_type(self, query: str, entities: EntityExtraction) -> Tuple[str, float]:
        """D√©tecte le type de requ√™te et sa confiance."""
        confidence = 0.0
        
        # Analyse bas√©e sur les entit√©s pr√©sentes
        if entities.amounts:
            if len(entities.amounts) == 1:
                return "amount_search", 0.8
            elif len(entities.amounts) == 2:
                return "amount_range", 0.9
            else:
                return "complex_amount", 0.7
        
        if entities.dates:
            if len(entities.dates) == 1:
                return "date_search", 0.8
            else:
                return "date_range", 0.9
        
        # Analyse bas√©e sur les mots-cl√©s
        search_keywords = ["cherche", "trouve", "recherche", "voir", "affiche"]
        if any(keyword in query for keyword in search_keywords):
            confidence += 0.3
        
        # Type bas√© sur les cat√©gories d√©tect√©es
        if entities.categories:
            return "category_search", 0.7 + confidence
        
        # Analyse bas√©e sur la structure de la requ√™te
        if any(word in query for word in ["tous", "toutes", "liste", "historique"]):
            return "list_all", 0.6 + confidence
        
        if any(word in query for word in ["similar", "similaire", "comme", "pareil"]):
            return "similarity_search", 0.8
        
        if any(word in query for word in ["r√©cent", "dernier", "nouveau", "aujourd"]):
            return "recent_search", 0.7
        
        # Requ√™te libre par d√©faut
        return "free_text", 0.5 + confidence
    
    def _expand_query(self, query: str, query_type: str) -> str:
        """Expand la requ√™te avec des synonymes (si activ√© dans la config)."""
        # V√©rifier si l'expansion est activ√©e (configurable)
        if not getattr(settings, 'ENABLE_QUERY_EXPANSION', True):
            return query
        
        expanded_terms = set([query])
        query_words = query.split()
        
        # Expansion bas√©e sur les synonymes financiers
        for word in query_words:
            if word in FINANCIAL_SYNONYMS:
                expanded_terms.update(FINANCIAL_SYNONYMS[word])
        
        # Expansion sp√©cifique par type de requ√™te
        if query_type == "category_search":
            # Ajouter des termes de cat√©gorie similaires
            for category, keywords in self.category_keywords.items():
                if any(keyword in query for keyword in keywords):
                    expanded_terms.update(keywords[:3])  # Limiter √† 3 synonymes
        
        # Expansion pour am√©liorer les r√©sultats selon le validateur
        if "restaurant" in query:
            expanded_terms.update(["resto", "brasserie", "restauration", "dining"])
        
        if "carte" in query:
            expanded_terms.update(["cb", "paiement", "card", "bancaire"])
        
        if "virement" in query:
            expanded_terms.update(["transfer", "transfert", "salaire", "paie"])
        
        return " ".join(expanded_terms)
    
    def _suggest_filters(
        self,
        entities: EntityExtraction,
        query_type: str,
        user_context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """Sugg√®re des filtres automatiques bas√©s sur l'analyse."""
        filters = {}
        
        # Filtres de montant
        if entities.amounts:
            if len(entities.amounts) == 1:
                amount = entities.amounts[0]["value"]
                # Sugg√©rer une fourchette de ¬±20% (configurable)
                margin_percent = getattr(settings, 'AMOUNT_FILTER_MARGIN_PERCENT', 20) / 100
                margin = amount * margin_percent
                filters["amount_min"] = max(0, amount - margin)
                filters["amount_max"] = amount + margin
            elif len(entities.amounts) == 2:
                amounts = sorted([a["value"] for a in entities.amounts])
                filters["amount_min"] = amounts[0]
                filters["amount_max"] = amounts[1]
        
        # Filtres de date
        if entities.dates:
            if len(entities.dates) == 1:
                date = entities.dates[0]["date"]
                # Sugg√©rer le jour ou le mois selon le contexte
                if query_type == "recent_search":
                    days_range = getattr(settings, 'RECENT_SEARCH_DAYS_RANGE', 7)
                    filters["date_from"] = (date - timedelta(days=days_range)).strftime("%Y-%m-%d")
                    filters["date_to"] = date.strftime("%Y-%m-%d")
                else:
                    filters["date_from"] = date.strftime("%Y-%m-%d")
                    filters["date_to"] = date.strftime("%Y-%m-%d")
            elif len(entities.dates) == 2:
                dates = sorted([d["date"] for d in entities.dates])
                filters["date_from"] = dates[0].strftime("%Y-%m-%d")
                filters["date_to"] = dates[1].strftime("%Y-%m-%d")
        
        # Filtres de cat√©gorie
        if entities.categories:
            # Note: n√©cessiterait un mapping cat√©gorie -> category_id
            filters["suggested_categories"] = entities.categories
        
        # Filtres bas√©s sur le type de requ√™te
        if query_type == "recent_search":
            if "date_from" not in filters:
                recent_days = getattr(settings, 'DEFAULT_RECENT_DAYS', 30)
                recent_date = (datetime.now() - timedelta(days=recent_days)).strftime("%Y-%m-%d")
                filters["date_from"] = recent_date
        
        # Filtres bas√©s sur le contexte utilisateur
        if user_context:
            # Exemple: compte pr√©f√©r√© de l'utilisateur
            if "preferred_account_id" in user_context:
                filters["account_ids"] = [user_context["preferred_account_id"]]
        
        return filters
    
    def _serialize_entities(self, entities: EntityExtraction) -> Dict[str, Any]:
        """S√©rialise les entit√©s pour l'analyse."""
        return {
            "amounts": [
                {
                    "value": a["value"],
                    "raw": a["raw"],
                    "formatted": f"{a['value']:.2f}‚Ç¨"
                }
                for a in entities.amounts
            ],
            "dates": [
                {
                    "date": d["date"].isoformat(),
                    "raw": d["raw"],
                    "type": d["type"],
                    "formatted": d["date"].strftime("%d/%m/%Y")
                }
                for d in entities.dates
            ],
            "categories": entities.categories,
            "keywords": entities.keywords,
            "negations": entities.negations,
            "merchants": entities.merchants
        }
    
    def optimize_for_lexical_search(self, analysis: QueryAnalysis) -> str:
        """Optimise la requ√™te pour la recherche lexicale (Elasticsearch)."""
        # Bas√© sur les r√©sultats du validateur, prioriser:
        # 1. Les correspondances exactes de phrase
        # 2. Les champs merchant_name
        # 3. Les requ√™tes multi-champs
        
        optimized_parts = []
        
        # Phrase exacte si courte
        if len(analysis.cleaned_query.split()) <= 3:
            optimized_parts.append(f'"{analysis.cleaned_query}"')
        
        # Requ√™te originale nettoy√©e
        optimized_parts.append(analysis.cleaned_query)
        
        # Mots-cl√©s importants
        important_keywords = [kw for kw in analysis.key_terms if len(kw) > 3]
        if important_keywords:
            optimized_parts.extend(important_keywords[:3])  # Top 3 mots-cl√©s
        
        # Synonymes financiers pour am√©liorer la couverture
        if analysis.query_type == "category_search":
            optimized_parts.append(analysis.expanded_query)
        
        return " ".join(set(optimized_parts))
    
    def optimize_for_semantic_search(self, analysis: QueryAnalysis) -> str:
        """Optimise la requ√™te pour la recherche s√©mantique (Qdrant)."""
        # Pour la recherche s√©mantique, privil√©gier:
        # 1. Le contexte et l'intention
        # 2. Les termes enrichis
        # 3. La requ√™te √©tendue avec synonymes
        
        semantic_parts = []
        
        # Requ√™te originale pour le contexte
        semantic_parts.append(analysis.original_query)
        
        # Ajouter du contexte bas√© sur les entit√©s d√©tect√©es
        if analysis.detected_entities["amounts"]:
            amounts_context = " ".join([a["formatted"] for a in analysis.detected_entities["amounts"]])
            semantic_parts.append(f"montant {amounts_context}")
        
        if analysis.detected_entities["categories"]:
            categories_context = " ".join(analysis.detected_entities["categories"])
            semantic_parts.append(f"cat√©gorie {categories_context}")
        
        if analysis.detected_entities["dates"]:
            dates_context = " ".join([d["formatted"] for d in analysis.detected_entities["dates"]])
            semantic_parts.append(f"date {dates_context}")
        
        # Requ√™te √©tendue pour la richesse s√©mantique
        semantic_parts.append(analysis.expanded_query)
        
        return " ".join(semantic_parts)
    
    def suggest_corrections(self, query: str) -> List[str]:
        """Sugg√®re des corrections pour les requ√™tes avec fautes."""
        suggestions = []
        
        # Corrections courantes pour le domaine financier (configurables)
        corrections = {
            "restorant": "restaurant",
            "restau": "restaurant", 
            "supermach√©": "supermarch√©",
            "pharmaci": "pharmacie",
            "esence": "essence",
            "virment": "virement",
            "bancair": "bancaire",
            "abonemment": "abonnement",
            "transport": "transport"
        }
        
        words = query.lower().split()
        corrected_words = []
        has_corrections = False
        
        for word in words:
            if word in corrections:
                corrected_words.append(corrections[word])
                has_corrections = True
            else:
                # V√©rification de distance d'√©dition simple
                best_match = self._find_closest_match(word)
                if best_match and best_match != word:
                    corrected_words.append(best_match)
                    has_corrections = True
                else:
                    corrected_words.append(word)
        
        if has_corrections:
            suggestions.append(" ".join(corrected_words))
        
        return suggestions
    
    def _find_closest_match(self, word: str) -> Optional[str]:
        """Trouve le mot le plus proche dans le vocabulaire financier."""
        if len(word) < 3:
            return None
        
        # Vocabulaire financier commun
        financial_vocab = set()
        for synonyms in FINANCIAL_SYNONYMS.values():
            financial_vocab.update(synonyms)
        
        for category_keywords in self.category_keywords.values():
            financial_vocab.update(category_keywords)
        
        # Distance d'√©dition simple (substitution uniquement)
        best_match = None
        min_distance = float('inf')
        
        for vocab_word in financial_vocab:
            if abs(len(word) - len(vocab_word)) <= 2:  # Longueur similaire
                distance = self._simple_edit_distance(word, vocab_word)
                if distance < min_distance and distance <= 2:
                    min_distance = distance
                    best_match = vocab_word
        
        return best_match
    
    def _simple_edit_distance(self, s1: str, s2: str) -> int:
        """Calcule une distance d'√©dition simplifi√©e."""
        if len(s1) != len(s2):
            return abs(len(s1) - len(s2)) + 1
        
        differences = sum(c1 != c2 for c1, c2 in zip(s1, s2))
        return differences
    
    def extract_search_intent(self, analysis: QueryAnalysis) -> Dict[str, Any]:
        """Extrait l'intention de recherche d√©taill√©e."""
        intent = {
            "primary_intent": analysis.query_type,
            "confidence": analysis.confidence,
            "search_scope": "user_transactions",  # Par d√©faut
            "temporal_focus": "all_time",
            "amount_focus": "any",
            "category_focus": "any"
        }
        
        # Analyse temporelle
        if analysis.detected_entities["dates"]:
            if any(d["type"] == "recent" for d in analysis.detected_entities["dates"]):
                intent["temporal_focus"] = "recent"
            elif len(analysis.detected_entities["dates"]) == 1:
                intent["temporal_focus"] = "specific_date"
            else:
                intent["temporal_focus"] = "date_range"
        
        # Analyse des montants
        if analysis.detected_entities["amounts"]:
            if len(analysis.detected_entities["amounts"]) == 1:
                intent["amount_focus"] = "specific_amount"
            else:
                intent["amount_focus"] = "amount_range"
        
        # Analyse des cat√©gories
        if analysis.detected_entities["categories"]:
            if len(analysis.detected_entities["categories"]) == 1:
                intent["category_focus"] = "specific_category"
            else:
                intent["category_focus"] = "multiple_categories"
        
        # D√©tection d'intentions sp√©ciales
        query_lower = analysis.original_query.lower()
        
        if any(word in query_lower for word in ["similaire", "comme", "pareil"]):
            intent["special_intent"] = "find_similar"
        
        if any(word in query_lower for word in ["r√©current", "r√©gulier", "mensuel"]):
            intent["special_intent"] = "find_recurring"
        
        if any(word in query_lower for word in ["suspect", "inhabituel", "bizarre"]):
            intent["special_intent"] = "find_anomalies"
        
        if any(word in query_lower for word in ["total", "somme", "montant"]):
            intent["special_intent"] = "calculate_sum"
        
        return intent
    
    def generate_alternative_queries(self, analysis: QueryAnalysis) -> List[str]:
        """G√©n√®re des requ√™tes alternatives pour am√©liorer les r√©sultats."""
        alternatives = []
        
        # Requ√™te simplifi√©e (mots-cl√©s uniquement)
        keywords = [kw for kw in analysis.key_terms if len(kw) > 2]
        if len(keywords) > 1:
            alternatives.append(" ".join(keywords[:3]))
        
        # Requ√™te par cat√©gorie si d√©tect√©e
        if analysis.detected_entities["categories"]:
            for category in analysis.detected_entities["categories"]:
                alternatives.append(category)
        
        # Requ√™te avec synonymes
        if analysis.expanded_query != analysis.cleaned_query:
            alternatives.append(analysis.expanded_query)
        
        # Requ√™te sans entit√©s num√©riques (pour cas o√π montants/dates perturbent)
        text_only = analysis.cleaned_query
        for amount in analysis.detected_entities["amounts"]:
            text_only = text_only.replace(amount["raw"], "")
        for date in analysis.detected_entities["dates"]:
            text_only = text_only.replace(date["raw"], "")
        text_only = re.sub(r'\s+', ' ', text_only).strip()
        if text_only and text_only != analysis.cleaned_query:
            alternatives.append(text_only)
        
        # Supprimer les doublons et vides
        alternatives = list(set([alt for alt in alternatives if alt.strip()]))
        
        return alternatives[:5]  # Limiter √† 5 alternatives
    
    def analyze_search_difficulty(self, analysis: QueryAnalysis) -> Dict[str, Any]:
        """Analyse la difficult√© estim√©e de la recherche."""
        difficulty = {
            "overall_difficulty": "medium",
            "lexical_difficulty": "medium", 
            "semantic_difficulty": "medium",
            "factors": [],
            "recommendations": []
        }
        
        # Facteurs qui augmentent la difficult√©
        if len(analysis.detected_entities["negations"]) > 0:
            difficulty["factors"].append("Contains negations")
            difficulty["overall_difficulty"] = "hard"
        
        if len(analysis.cleaned_query.split()) > 6:
            difficulty["factors"].append("Long query")
            difficulty["semantic_difficulty"] = "hard"
        
        if len(analysis.cleaned_query.split()) == 1:
            difficulty["factors"].append("Single word query")
            difficulty["lexical_difficulty"] = "easy"
        
        if not analysis.key_terms:
            difficulty["factors"].append("No clear keywords")
            difficulty["overall_difficulty"] = "hard"
        
        # Facteurs qui facilitent la recherche
        if analysis.detected_entities["categories"]:
            difficulty["factors"].append("Clear category detected")
            if difficulty["overall_difficulty"] == "medium":
                difficulty["overall_difficulty"] = "easy"
        
        if analysis.detected_entities["amounts"] or analysis.detected_entities["dates"]:
            difficulty["factors"].append("Specific entities detected")
            difficulty["lexical_difficulty"] = "easy"
        
        # Recommandations bas√©es sur la difficult√©
        if difficulty["overall_difficulty"] == "hard":
            difficulty["recommendations"].extend([
                "Use expanded query with synonyms",
                "Apply semantic search with lower threshold",
                "Consider query simplification"
            ])
        
        if difficulty["lexical_difficulty"] == "hard":
            difficulty["recommendations"].append("Prioritize semantic search")
        
        if difficulty["semantic_difficulty"] == "hard":
            difficulty["recommendations"].append("Prioritize lexical search")
        
        return difficulty
    
    def clear_cache(self):
        """Vide le cache des analyses."""
        self._analysis_cache.clear()
        logger.info("Query processor cache cleared")
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """Retourne les statistiques du cache."""
        return {
            "cache_size": len(self._analysis_cache),
            "max_cache_size": self._max_cache_size,
            "cache_keys": list(self._analysis_cache.keys())[:10],  # √âchantillon
            "config_source": "centralized (config_service)"
        }
    
    def get_metrics(self) -> Dict[str, Any]:
        """Retourne les m√©triques du processeur de requ√™tes."""
        return {
            "processor_type": "query_processor",
            "cache_stats": self.get_cache_stats(),
            "entity_patterns": {
                "amount_patterns_count": len(self.amount_patterns),
                "date_patterns_count": len(self.date_patterns),
                "category_keywords_count": len(self.category_keywords),
                "negation_words_count": len(self.negation_words),
                "stop_words_count": len(self.stop_words)
            },
            "config_source": "centralized (config_service)",
            "centralized_settings": {
                "query_expansion_enabled": getattr(settings, 'ENABLE_QUERY_EXPANSION', True),
                "cache_size": self._max_cache_size,
                "amount_margin_percent": getattr(settings, 'AMOUNT_FILTER_MARGIN_PERCENT', 20),
                "recent_search_days": getattr(settings, 'RECENT_SEARCH_DAYS_RANGE', 7),
                "default_recent_days": getattr(settings, 'DEFAULT_RECENT_DAYS', 30)
            }
        }
    
    def update_config(self) -> None:
        """Met √† jour la configuration du processeur depuis config centralis√©e."""
        # Recharger la taille de cache
        new_cache_size = getattr(settings, 'QUERY_PROCESSOR_CACHE_SIZE', 100)
        if new_cache_size != self._max_cache_size:
            self._max_cache_size = new_cache_size
            # R√©duire le cache si n√©cessaire
            while len(self._analysis_cache) > self._max_cache_size:
                oldest_key = next(iter(self._analysis_cache))
                del self._analysis_cache[oldest_key]
        
        logger.info("Query processor configuration updated from centralized config")


class QueryValidator:
    """Validateur de requ√™tes pour pr√©venir les erreurs."""
    
    @staticmethod
    def validate_query(query: str) -> Dict[str, Any]:
        """
        Valide une requ√™te de recherche.
        
        Args:
            query: Requ√™te √† valider
            
        Returns:
            R√©sultat de validation avec erreurs √©ventuelles
        """
        validation = {
            "is_valid": True,
            "errors": [],
            "warnings": [],
            "suggestions": []
        }
        
        # V√©rifications de base
        if not query or not query.strip():
            validation["is_valid"] = False
            validation["errors"].append("Query cannot be empty")
            return validation
        
        # Utiliser des limites configurables
        max_query_length = getattr(settings, 'MAX_QUERY_LENGTH', 500)
        max_words = getattr(settings, 'MAX_QUERY_WORDS', 10)
        
        if len(query) > max_query_length:
            validation["is_valid"] = False
            validation["errors"].append(f"Query too long (max {max_query_length} characters)")
        
        # Caract√®res dangereux
        dangerous_chars = ['<', '>', '&', '"', "'", ';', '(', ')', '{', '}']
        if any(char in query for char in dangerous_chars):
            validation["warnings"].append("Query contains special characters that may affect search")
        
        # Trop de mots
        if len(query.split()) > max_words:
            validation["warnings"].append("Long queries may return less precise results")
            validation["suggestions"].append("Consider using fewer, more specific terms")
        
        # Mots tr√®s courts
        short_words = [w for w in query.split() if len(w) == 1]
        if len(short_words) > 2:
            validation["warnings"].append("Too many single-character terms")
            validation["suggestions"].append("Use more descriptive terms")
        
        # Requ√™te uniquement num√©rique
        if query.strip().replace(' ', '').replace('.', '').replace(',', '').isdigit():
            validation["warnings"].append("Numeric-only queries may return unexpected results")
            validation["suggestions"].append("Add descriptive terms (e.g., 'montant 100' instead of '100')")
        
        return validation


# ==========================================
# üõ†Ô∏è FONCTIONS UTILITAIRES CENTRALIS√âES
# ==========================================

def normalize_amount(amount_str: str) -> Optional[float]:
    """Normalise une cha√Æne de montant en float."""
    try:
        # Supprimer les espaces et symboles
        normalized = re.sub(r'[‚Ç¨$\s]', '', amount_str)
        
        # G√©rer les s√©parateurs de milliers et d√©cimales
        if ',' in normalized and '.' in normalized:
            # Format 1,000.50
            normalized = normalized.replace(',', '')
        elif ',' in normalized:
            # D√©tecter si c'est s√©parateur de milliers ou d√©cimales
            parts = normalized.split(',')
            if len(parts) == 2 and len(parts[1]) == 2:
                # Format fran√ßais: 1000,50
                normalized = normalized.replace(',', '.')
            else:
                # S√©parateur de milliers: 1,000
                normalized = normalized.replace(',', '')
        
        return float(normalized)
    except (ValueError, AttributeError):
        return None


def extract_keywords_by_importance(text: str, max_keywords: int = 5) -> List[str]:
    """Extrait les mots-cl√©s les plus importants d'un texte."""
    # Mots vides √©tendus
    stop_words = {
        'le', 'la', 'les', 'un', 'une', 'des', 'du', 'de', 'et', 'ou', '√†', 'au', 'aux',
        'dans', 'sur', 'pour', 'avec', 'par', 'chez', 'vers', 'entre', 'depuis', 'jusqu',
        'ce', 'cette', 'ces', 'mon', 'ma', 'mes', 'ton', 'ta', 'tes', 'son', 'sa', 'ses',
        'qui', 'que', 'quoi', 'dont', 'o√π', 'si', 'comme', 'quand', 'comment', 'pourquoi'
    }
    
    # Nettoyer et diviser
    words = re.findall(r'\b\w+\b', text.lower())
    
    # Filtrer et scorer
    keyword_scores = {}
    for word in words:
        if len(word) > 2 and word not in stop_words:
            # Score bas√© sur la longueur et la fr√©quence
            keyword_scores[word] = keyword_scores.get(word, 0) + len(word)
    
    # Trier par score et retourner les meilleurs
    sorted_keywords = sorted(keyword_scores.items(), key=lambda x: x[1], reverse=True)
    return [kw for kw, _ in sorted_keywords[:max_keywords]]


# ==========================================
# üéØ FONCTIONS DE CONFIGURATION CENTRALIS√âE
# ==========================================

def get_query_processor_config() -> Dict[str, Any]:
    """Retourne la configuration actuelle du processeur de requ√™tes."""
    return {
        "query_expansion_enabled": getattr(settings, 'ENABLE_QUERY_EXPANSION', True),
        "cache_size": getattr(settings, 'QUERY_PROCESSOR_CACHE_SIZE', 100),
        "max_query_length": getattr(settings, 'MAX_QUERY_LENGTH', 500),
        "max_query_words": getattr(settings, 'MAX_QUERY_WORDS', 10),
        "amount_filter_margin_percent": getattr(settings, 'AMOUNT_FILTER_MARGIN_PERCENT', 20),
        "recent_search_days_range": getattr(settings, 'RECENT_SEARCH_DAYS_RANGE', 7),
        "default_recent_days": getattr(settings, 'DEFAULT_RECENT_DAYS', 30),
        "config_source": "centralized (config_service)"
    }


def create_query_processor_with_config() -> QueryProcessor:
    """Factory function pour cr√©er un processeur avec config centralis√©e."""
    processor = QueryProcessor()
    logger.info("Created query processor with centralized config")
    return processor


# ==========================================
# üéØ EXPORTS PRINCIPAUX
# ==========================================

__all__ = [
    # Classes principales
    "QueryAnalysis",
    "EntityExtraction", 
    "QueryProcessor",
    "QueryValidator",
    
    # Fonctions utilitaires
    "normalize_amount",
    "extract_keywords_by_importance",
    "get_query_processor_config",
    "create_query_processor_with_config"
]