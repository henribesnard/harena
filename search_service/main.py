"""
Module principal du service de recherche avec diagnostic complet au d√©marrage.

Ce module initialise et configure le service de recherche hybride de Harena,
avec v√©rifications d√©taill√©es des services externes (Bonsai Elasticsearch + Qdrant).
"""
import logging
import time
import sys
import asyncio
from contextlib import asynccontextmanager
from datetime import datetime, timedelta
from typing import Dict, Any, Optional
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware

from search_service.api.routes import router
from search_service.core.embeddings import embedding_service
from search_service.core.reranker import reranker_service
from search_service.storage.elastic_client import ElasticClient
from search_service.storage.qdrant_client import QdrantClient
from search_service.utils.cache import SearchCache
from search_service.utils.metrics import MetricsCollector
from config_service.config import settings

# Configuration du logging avec format d√©taill√©
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - [%(name)s] - %(levelname)s - %(module)s:%(funcName)s:%(lineno)d - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger("search_service.main")

# R√©duire le bruit des biblioth√®ques externes
logging.getLogger("urllib3").setLevel(logging.WARNING)
logging.getLogger("elasticsearch").setLevel(logging.WARNING)
logging.getLogger("qdrant_client").setLevel(logging.WARNING)
logging.getLogger("httpx").setLevel(logging.WARNING)

# ==================== VARIABLES GLOBALES ====================

# Instances globales des services
elastic_client = None
qdrant_client = None
search_cache = None
metrics_collector = None
startup_time = None
startup_diagnostics = {}

# ==================== FONCTIONS DE DIAGNOSTIC ====================

def log_startup_banner():
    """Affiche la banni√®re de d√©marrage."""
    banner = """
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                        üîç HARENA SEARCH SERVICE üîç                          ‚ïë
‚ïë                     Service de Recherche Hybride v1.0.0                     ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    """
    print(banner)
    logger.info("üöÄ === D√âMARRAGE DU SERVICE DE RECHERCHE HYBRIDE ===")

def check_environment_configuration() -> Dict[str, Any]:
    """V√©rifie et diagnostique la configuration des variables d'environnement."""
    logger.info("üîß === V√âRIFICATION DE LA CONFIGURATION ===")
    
    config_status = {
        "critical_services": {},
        "optional_services": {},
        "summary": {}
    }
    
    # Services critiques pour le fonctionnement
    critical_configs = {
        "BONSAI_URL": {
            "description": "Elasticsearch via Bonsai (recherche lexicale)",
            "value": settings.BONSAI_URL,
            "required": True
        },
        "QDRANT_URL": {
            "description": "Qdrant (recherche s√©mantique)",
            "value": settings.QDRANT_URL,
            "required": True
        }
    }
    
    # Services optionnels mais recommand√©s
    optional_configs = {
        "OPENAI_API_KEY": {
            "description": "OpenAI (g√©n√©ration d'embeddings)",
            "value": settings.OPENAI_API_KEY,
            "impact": "Pas d'embeddings automatiques"
        },
        "COHERE_KEY": {
            "description": "Cohere (reranking des r√©sultats)",
            "value": settings.COHERE_KEY,
            "impact": "Pas de reranking intelligent"
        },
        "QDRANT_API_KEY": {
            "description": "Qdrant API Key (authentification)",
            "value": settings.QDRANT_API_KEY,
            "impact": "Connexion sans authentification"
        }
    }
    
    # V√©rification des services critiques
    critical_ok_count = 0
    for key, info in critical_configs.items():
        is_configured = bool(info["value"])
        config_status["critical_services"][key] = {
            "configured": is_configured,
            "description": info["description"],
            "required": info["required"]
        }
        
        if is_configured:
            critical_ok_count += 1
            # Masquer les URLs sensibles pour l'affichage
            if "URL" in key:
                safe_value = info["value"].split('@')[-1] if '@' in info["value"] else info["value"]
                logger.info(f"‚úÖ {key}: {safe_value}")
            else:
                logger.info(f"‚úÖ {key}: Configur√©")
        else:
            logger.error(f"‚ùå {key}: NON CONFIGUR√â - {info['description']}")
    
    # V√©rification des services optionnels
    optional_ok_count = 0
    for key, info in optional_configs.items():
        is_configured = bool(info["value"])
        config_status["optional_services"][key] = {
            "configured": is_configured,
            "description": info["description"],
            "impact": info["impact"]
        }
        
        if is_configured:
            optional_ok_count += 1
            if "KEY" in key:
                masked_value = f"{info['value'][:8]}...{info['value'][-4:]}" if len(info["value"]) > 12 else "***"
                logger.info(f"‚úÖ {key}: {masked_value}")
            else:
                logger.info(f"‚úÖ {key}: Configur√©")
        else:
            logger.warning(f"‚ö†Ô∏è {key}: Non configur√© - {info['impact']}")
    
    # R√©sum√© de la configuration
    config_status["summary"] = {
        "critical_configured": critical_ok_count,
        "critical_total": len(critical_configs),
        "optional_configured": optional_ok_count,
        "optional_total": len(optional_configs),
        "critical_percentage": (critical_ok_count / len(critical_configs)) * 100,
        "optional_percentage": (optional_ok_count / len(optional_configs)) * 100
    }
    
    if critical_ok_count == len(critical_configs):
        logger.info("üéâ CONFIGURATION: Tous les services critiques sont configur√©s")
    elif critical_ok_count > 0:
        logger.warning("‚ö†Ô∏è CONFIGURATION: Services critiques PARTIELLEMENT configur√©s")
    else:
        logger.error("üö® CONFIGURATION: AUCUN service critique configur√©")
    
    return config_status

async def initialize_elasticsearch() -> tuple[bool, Optional[ElasticClient], Dict[str, Any]]:
    """Initialise et teste la connexion Elasticsearch (Bonsai)."""
    logger.info("üîç === INITIALISATION ELASTICSEARCH (BONSAI) ===")
    diagnostic = {
        "service": "elasticsearch_bonsai",
        "configured": False,
        "connected": False,
        "healthy": False,
        "error": None,
        "connection_time": None,
        "cluster_info": {},
        "indices_info": {}
    }
    
    if not settings.BONSAI_URL:
        logger.error("‚ùå BONSAI_URL non configur√©e")
        diagnostic["error"] = "BONSAI_URL not configured"
        return False, None, diagnostic
    
    diagnostic["configured"] = True
    
    # Masquer les credentials pour l'affichage
    safe_url = settings.BONSAI_URL.split('@')[-1] if '@' in settings.BONSAI_URL else settings.BONSAI_URL
    logger.info(f"üîó Connexion √† Bonsai Elasticsearch: {safe_url}")
    
    try:
        start_time = time.time()
        client = ElasticClient()
        
        logger.info("‚è±Ô∏è Test de connexion Elasticsearch...")
        await client.initialize()
        
        connection_time = time.time() - start_time
        diagnostic["connection_time"] = round(connection_time, 3)
        diagnostic["connected"] = True
        
        logger.info(f"‚úÖ Connexion √©tablie en {connection_time:.3f}s")
        
        # Test de sant√©
        logger.info("ü©∫ V√©rification de la sant√© du cluster...")
        is_healthy = await client.is_healthy()
        diagnostic["healthy"] = is_healthy
        
        if is_healthy:
            logger.info("üü¢ Cluster Elasticsearch en bonne sant√©")
            
            # R√©cup√©rer les informations du cluster
            try:
                cluster_info = await client.get_cluster_info()
                diagnostic["cluster_info"] = cluster_info
                logger.info(f"üìä Cluster: {cluster_info.get('cluster_name', 'Unknown')}")
                logger.info(f"üìä Version: {cluster_info.get('version', {}).get('number', 'Unknown')}")
                logger.info(f"üìä N≈ìuds: {cluster_info.get('nodes', 'Unknown')}")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Impossible de r√©cup√©rer les infos cluster: {e}")
            
            # V√©rifier les indices
            try:
                indices_info = await client.get_indices_info()
                diagnostic["indices_info"] = indices_info
                if indices_info:
                    logger.info(f"üìÅ Indices disponibles: {len(indices_info)}")
                    for index_name, info in indices_info.items():
                        logger.info(f"   - {index_name}: {info.get('docs', {}).get('count', 0)} documents")
                else:
                    logger.info("üìÅ Aucun indice trouv√©")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Impossible de lister les indices: {e}")
            
            return True, client, diagnostic
        else:
            logger.error("üî¥ Cluster Elasticsearch non op√©rationnel")
            diagnostic["error"] = "Cluster unhealthy"
            return False, client, diagnostic
            
    except Exception as e:
        connection_time = time.time() - start_time
        diagnostic["connection_time"] = round(connection_time, 3)
        diagnostic["error"] = str(e)
        
        logger.error(f"üí• Erreur Elasticsearch apr√®s {connection_time:.3f}s:")
        logger.error(f"   Type: {type(e).__name__}")
        logger.error(f"   Message: {str(e)}")
        
        # Diagnostic sp√©cifique des erreurs
        error_str = str(e).lower()
        if "connection" in error_str or "timeout" in error_str:
            logger.error("üîå DIAGNOSTIC: Probl√®me de connectivit√© r√©seau")
            logger.error("   - V√©rifiez l'URL Bonsai")
            logger.error("   - V√©rifiez la connectivit√© r√©seau")
        elif "401" in str(e) or "403" in str(e) or "auth" in error_str:
            logger.error("üîë DIAGNOSTIC: Probl√®me d'authentification")
            logger.error("   - V√©rifiez les credentials dans BONSAI_URL")
        elif "ssl" in error_str or "certificate" in error_str:
            logger.error("üîí DIAGNOSTIC: Probl√®me SSL/TLS")
            logger.error("   - V√©rifiez les certificats SSL")
        
        return False, None, diagnostic

async def initialize_qdrant() -> tuple[bool, Optional[QdrantClient], Dict[str, Any]]:
    """Initialise et teste la connexion Qdrant."""
    logger.info("üéØ === INITIALISATION QDRANT ===")
    diagnostic = {
        "service": "qdrant",
        "configured": False,
        "connected": False,
        "healthy": False,
        "error": None,
        "connection_time": None,
        "collections_info": {},
        "version_info": {}
    }
    
    if not settings.QDRANT_URL:
        logger.error("‚ùå QDRANT_URL non configur√©e")
        diagnostic["error"] = "QDRANT_URL not configured"
        return False, None, diagnostic
    
    diagnostic["configured"] = True
    logger.info(f"üîó Connexion √† Qdrant: {settings.QDRANT_URL}")
    
    if settings.QDRANT_API_KEY:
        logger.info("üîë Authentification par API Key activ√©e")
    else:
        logger.info("üîì Connexion sans authentification")
    
    try:
        start_time = time.time()
        client = QdrantClient()
        
        logger.info("‚è±Ô∏è Test de connexion Qdrant...")
        await client.initialize()
        
        connection_time = time.time() - start_time
        diagnostic["connection_time"] = round(connection_time, 3)
        diagnostic["connected"] = True
        
        logger.info(f"‚úÖ Connexion √©tablie en {connection_time:.3f}s")
        
        # Test de sant√©
        logger.info("ü©∫ V√©rification de la sant√© de Qdrant...")
        is_healthy = await client.is_healthy()
        diagnostic["healthy"] = is_healthy
        
        if is_healthy:
            logger.info("üü¢ Service Qdrant en bonne sant√©")
            
            # R√©cup√©rer les informations des collections
            try:
                collections_info = await client.get_collections_info()
                diagnostic["collections_info"] = collections_info
                if collections_info:
                    logger.info(f"üìä Collections disponibles: {len(collections_info)}")
                    for collection_name, info in collections_info.items():
                        points_count = info.get('points_count', 0)
                        vectors_count = info.get('vectors_count', 0)
                        logger.info(f"   - {collection_name}: {points_count} points, {vectors_count} vecteurs")
                else:
                    logger.warning("üìä Aucune collection trouv√©e")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Impossible de r√©cup√©rer les infos collections: {e}")
            
            # V√©rifier la collection des transactions
            try:
                collection_exists = await client.collection_exists("financial_transactions")
                if collection_exists:
                    logger.info("‚úÖ Collection 'financial_transactions' trouv√©e")
                else:
                    logger.warning("‚ö†Ô∏è Collection 'financial_transactions' non trouv√©e")
                    logger.warning("   - Lancez l'enrichment_service pour cr√©er la collection")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Impossible de v√©rifier la collection: {e}")
            
            return True, client, diagnostic
        else:
            logger.error("üî¥ Service Qdrant non op√©rationnel")
            diagnostic["error"] = "Service unhealthy"
            return False, client, diagnostic
            
    except Exception as e:
        connection_time = time.time() - start_time
        diagnostic["connection_time"] = round(connection_time, 3)
        diagnostic["error"] = str(e)
        
        logger.error(f"üí• Erreur Qdrant apr√®s {connection_time:.3f}s:")
        logger.error(f"   Type: {type(e).__name__}")
        logger.error(f"   Message: {str(e)}")
        
        # Diagnostic sp√©cifique des erreurs
        error_str = str(e).lower()
        if "connection" in error_str or "timeout" in error_str:
            logger.error("üîå DIAGNOSTIC: Probl√®me de connectivit√© r√©seau")
            logger.error("   - V√©rifiez l'URL Qdrant")
            logger.error("   - V√©rifiez la connectivit√© r√©seau")
        elif "401" in str(e) or "403" in str(e) or "auth" in error_str:
            logger.error("üîë DIAGNOSTIC: Probl√®me d'authentification")
            logger.error("   - V√©rifiez QDRANT_API_KEY")
        elif "404" in str(e):
            logger.error("üìÇ DIAGNOSTIC: Collection non trouv√©e")
            logger.error("   - La collection 'financial_transactions' doit √™tre cr√©√©e")
            logger.error("   - Lancez d'abord l'enrichment_service")
        
        return False, None, diagnostic

async def initialize_ai_services() -> Dict[str, Dict[str, Any]]:
    """Initialise les services IA (embeddings et reranking)."""
    logger.info("ü§ñ === INITIALISATION DES SERVICES IA ===")
    ai_diagnostics = {}
    
    # Service d'embeddings (OpenAI)
    embedding_diagnostic = {
        "service": "openai_embeddings",
        "configured": bool(settings.OPENAI_API_KEY),
        "initialized": False,
        "error": None,
        "init_time": None
    }
    
    if settings.OPENAI_API_KEY:
        try:
            logger.info("ü§ñ Initialisation du service d'embeddings OpenAI...")
            start_time = time.time()
            await embedding_service.initialize()
            init_time = time.time() - start_time
            
            embedding_diagnostic["initialized"] = True
            embedding_diagnostic["init_time"] = round(init_time, 3)
            logger.info(f"‚úÖ Service d'embeddings initialis√© en {init_time:.3f}s")
        except Exception as e:
            embedding_diagnostic["error"] = str(e)
            logger.error(f"‚ùå √âchec initialisation embeddings: {type(e).__name__}: {str(e)}")
            logger.error("üìç V√©rifiez OPENAI_API_KEY et la connectivit√© internet")
    else:
        logger.warning("‚ö†Ô∏è OPENAI_API_KEY non configur√©e - embeddings indisponibles")
    
    ai_diagnostics["embeddings"] = embedding_diagnostic
    
    # Service de reranking (Cohere)
    reranking_diagnostic = {
        "service": "cohere_reranking",
        "configured": bool(settings.COHERE_KEY),
        "initialized": False,
        "error": None,
        "init_time": None
    }
    
    if settings.COHERE_KEY:
        try:
            logger.info("üéØ Initialisation du service de reranking Cohere...")
            start_time = time.time()
            await reranker_service.initialize()
            init_time = time.time() - start_time
            
            reranking_diagnostic["initialized"] = True
            reranking_diagnostic["init_time"] = round(init_time, 3)
            logger.info(f"‚úÖ Service de reranking initialis√© en {init_time:.3f}s")
        except Exception as e:
            reranking_diagnostic["error"] = str(e)
            logger.error(f"‚ùå √âchec initialisation reranking: {type(e).__name__}: {str(e)}")
            logger.error("üìç V√©rifiez COHERE_KEY et la connectivit√© internet")
    else:
        logger.warning("‚ö†Ô∏è COHERE_KEY non configur√©e - reranking indisponible")
    
    ai_diagnostics["reranking"] = reranking_diagnostic
    
    return ai_diagnostics

def initialize_cache_and_metrics() -> Dict[str, Dict[str, Any]]:
    """Initialise le cache et les m√©triques."""
    logger.info("üóÉÔ∏è === INITIALISATION CACHE ET M√âTRIQUES ===")
    utils_diagnostics = {}
    
    # Cache de recherche
    cache_diagnostic = {
        "service": "search_cache",
        "initialized": False,
        "error": None
    }
    
    global search_cache
    try:
        search_cache = SearchCache()
        cache_diagnostic["initialized"] = True
        logger.info("‚úÖ Cache de recherche initialis√©")
    except Exception as e:
        cache_diagnostic["error"] = str(e)
        logger.error(f"‚ùå Erreur initialisation cache: {e}")
        search_cache = None
    
    utils_diagnostics["cache"] = cache_diagnostic
    
    # Collecteur de m√©triques
    metrics_diagnostic = {
        "service": "metrics_collector",
        "initialized": False,
        "error": None
    }
    
    global metrics_collector
    try:
        metrics_collector = MetricsCollector()
        metrics_diagnostic["initialized"] = True
        logger.info("‚úÖ Collecteur de m√©triques initialis√©")
    except Exception as e:
        metrics_diagnostic["error"] = str(e)
        logger.error(f"‚ùå Erreur initialisation m√©triques: {e}")
        metrics_collector = None
    
    utils_diagnostics["metrics"] = metrics_diagnostic
    
    return utils_diagnostics

def inject_dependencies():
    """Injecte les d√©pendances dans le module routes."""
    logger.info("üîó === INJECTION DES D√âPENDANCES ===")
    
    import search_service.api.routes as routes
    routes.elastic_client = elastic_client
    routes.qdrant_client = qdrant_client
    routes.search_cache = search_cache
    routes.metrics_collector = metrics_collector
    
    logger.info("‚úÖ D√©pendances inject√©es dans les routes")
    logger.info(f"   üîç Elasticsearch client: {'‚úÖ Inject√©' if elastic_client else '‚ùå None'}")
    logger.info(f"   üéØ Qdrant client: {'‚úÖ Inject√©' if qdrant_client else '‚ùå None'}")
    logger.info(f"   üóÉÔ∏è Search cache: {'‚úÖ Inject√©' if search_cache else '‚ùå None'}")
    logger.info(f"   üìä Metrics collector: {'‚úÖ Inject√©' if metrics_collector else '‚ùå None'}")

def generate_startup_summary() -> Dict[str, Any]:
    """G√©n√®re un r√©sum√© complet du d√©marrage."""
    global startup_diagnostics
    
    startup_duration = time.time() - startup_time
    
    # Compter les services op√©rationnels
    operational_services = []
    failed_services = []
    
    # Services critiques
    if elastic_client:
        operational_services.append("Elasticsearch (Bonsai)")
    else:
        failed_services.append("Elasticsearch (Bonsai)")
    
    if qdrant_client:
        operational_services.append("Qdrant")
    else:
        failed_services.append("Qdrant")
    
    # Services IA
    ai_diag = startup_diagnostics.get("ai_services", {})
    if ai_diag.get("embeddings", {}).get("initialized"):
        operational_services.append("OpenAI Embeddings")
    else:
        failed_services.append("OpenAI Embeddings")
    
    if ai_diag.get("reranking", {}).get("initialized"):
        operational_services.append("Cohere Reranking")
    else:
        failed_services.append("Cohere Reranking")
    
    # Services utilitaires
    if search_cache:
        operational_services.append("Cache")
    if metrics_collector:
        operational_services.append("M√©triques")
    
    # D√©terminer l'√©tat global du service
    critical_services_ok = elastic_client and qdrant_client
    
    if critical_services_ok:
        if len(operational_services) >= 4:  # Elasticsearch + Qdrant + au moins 2 autres
            status = "FULLY_OPERATIONAL"
            status_icon = "üéâ"
            status_msg = "COMPL√àTEMENT OP√âRATIONNEL"
        else:
            status = "OPERATIONAL"
            status_icon = "‚úÖ"
            status_msg = "OP√âRATIONNEL"
    elif elastic_client or qdrant_client:
        status = "DEGRADED"
        status_icon = "‚ö†Ô∏è"
        status_msg = "OP√âRATIONNEL D√âGRAD√â"
    else:
        status = "FAILED"
        status_icon = "üö®"
        status_msg = "NON OP√âRATIONNEL"
    
    summary = {
        "status": status,
        "status_icon": status_icon,
        "status_message": status_msg,
        "startup_duration": round(startup_duration, 2),
        "operational_services": operational_services,
        "failed_services": failed_services,
        "operational_count": len(operational_services),
        "total_services": len(operational_services) + len(failed_services),
        "critical_services_operational": critical_services_ok,
        "search_capabilities": {
            "lexical_search": bool(elastic_client),
            "semantic_search": bool(qdrant_client),
            "hybrid_search": bool(elastic_client and qdrant_client),
            "ai_reranking": ai_diag.get("reranking", {}).get("initialized", False)
        }
    }
    
    # Affichage du r√©sum√©
    logger.info("=" * 100)
    logger.info(f"{status_icon} SERVICE DE RECHERCHE HYBRIDE - √âTAT: {status_msg}")
    logger.info(f"‚è±Ô∏è Temps de d√©marrage: {startup_duration:.2f}s")
    logger.info(f"üéØ Services op√©rationnels ({len(operational_services)}/{len(operational_services) + len(failed_services)}): {', '.join(operational_services) if operational_services else 'Aucun'}")
    
    if failed_services:
        logger.warning(f"‚ùå Services √©chou√©s: {', '.join(failed_services)}")
    
    # Messages informatifs selon l'√©tat
    if status == "FULLY_OPERATIONAL":
        logger.info("üöÄ Toutes les fonctionnalit√©s de recherche avanc√©e sont disponibles")
        logger.info("   ‚úì Recherche lexicale (Elasticsearch/Bonsai)")
        logger.info("   ‚úì Recherche s√©mantique (Qdrant)")
        logger.info("   ‚úì Recherche hybride avec fusion des scores")
        logger.info("   ‚úì Reranking intelligent des r√©sultats")
    elif status == "OPERATIONAL":
        logger.info("‚úÖ Fonctionnalit√©s de recherche de base disponibles")
        logger.info("   ‚úì Recherche lexicale ET s√©mantique")
        logger.warning("   ‚ö†Ô∏è Certaines fonctionnalit√©s avanc√©es peuvent √™tre limit√©es")
    elif status == "DEGRADED":
        logger.warning("‚ö†Ô∏è Service de recherche en mode d√©grad√©")
        if elastic_client and not qdrant_client:
            logger.warning("   ‚úì Recherche lexicale disponible")
            logger.warning("   ‚ùå Recherche s√©mantique indisponible")
        elif qdrant_client and not elastic_client:
            logger.warning("   ‚ùå Recherche lexicale indisponible")
            logger.warning("   ‚úì Recherche s√©mantique disponible")
    else:
        logger.error("üö® Service de recherche indisponible")
        logger.error("   ‚ùå V√©rifiez la configuration BONSAI_URL et QDRANT_URL")
        logger.error("   ‚ùå V√©rifiez la connectivit√© r√©seau")
    
    logger.info("üìä Endpoints disponibles:")
    logger.info("   GET  /health - V√©rification de sant√© d√©taill√©e")
    logger.info("   POST /api/v1/search - Recherche de transactions")
    logger.info("   GET  /api/v1/search/suggest - Suggestions de recherche")
    logger.info("=" * 100)
    
    return summary

# ==================== CYCLE DE VIE DE L'APPLICATION ====================

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Gestionnaire du cycle de vie de l'application avec diagnostics complets."""
    global startup_time, startup_diagnostics, elastic_client, qdrant_client
    
    try:
        # === PHASE DE D√âMARRAGE ===
        log_startup_banner()
        startup_time = time.time()
        
        # 1. V√©rification de la configuration
        config_status = check_environment_configuration()
        startup_diagnostics["configuration"] = config_status
        
        # 2. Initialisation d'Elasticsearch (Bonsai)
        elastic_success, elastic_client, elastic_diag = await initialize_elasticsearch()
        startup_diagnostics["elasticsearch"] = elastic_diag
        
        # 3. Initialisation de Qdrant
        qdrant_success, qdrant_client, qdrant_diag = await initialize_qdrant()
        startup_diagnostics["qdrant"] = qdrant_diag
        
        # 4. Initialisation des services IA
        ai_diagnostics = await initialize_ai_services()
        startup_diagnostics["ai_services"] = ai_diagnostics
        
        # 5. Initialisation du cache et des m√©triques
        utils_diagnostics = initialize_cache_and_metrics()
        startup_diagnostics["utilities"] = utils_diagnostics
        
        # 6. Injection des d√©pendances APR√àS initialisation
        inject_dependencies()
        
        # 7. G√©n√©ration du r√©sum√© de d√©marrage
        startup_summary = generate_startup_summary()
        startup_diagnostics["summary"] = startup_summary
        
        # Application pr√™te
        yield
        
    except Exception as e:
        logger.critical(f"üí• ERREUR CRITIQUE au d√©marrage: {type(e).__name__}: {str(e)}")
        import traceback
        logger.critical(f"Stack trace:\n{traceback.format_exc()}")
        raise
    
    finally:
        # === PHASE D'ARR√äT ===
        logger.info("‚èπÔ∏è Arr√™t du service de recherche...")
        
        # Fermeture des services IA
        try:
            if hasattr(embedding_service, 'close'):
                await embedding_service.close()
                logger.info("‚úÖ Service d'embeddings ferm√©")
        except Exception as e:
            logger.error(f"‚ùå Erreur fermeture embeddings: {e}")
        
        try:
            if hasattr(reranker_service, 'close'):
                await reranker_service.close()
                logger.info("‚úÖ Service de reranking ferm√©")
        except Exception as e:
            logger.error(f"‚ùå Erreur fermeture reranking: {e}")
        
        logger.info("üîö Service de recherche arr√™t√© proprement")

# ==================== APPLICATION FASTAPI ====================

def create_app() -> FastAPI:
    """Cr√©e et configure l'application FastAPI."""
    app = FastAPI(
        title="Harena Search Service",
        description="Service de recherche hybride combinant recherche lexicale (Bonsai) et s√©mantique (Qdrant)",
        version="1.0.0",
        docs_url="/docs",
        redoc_url="/redoc",
        lifespan=lifespan
    )
    
    # Configuration CORS
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],  # √Ä restreindre en production
        allow_credentials=True,
        allow_methods=["GET", "POST", "PUT", "DELETE"],
        allow_headers=["*"],
    )
    
    # Endpoints de sant√© √©tendus
    @app.get("/health")
    async def health_check():
        """Endpoint de sant√© d√©taill√© avec diagnostics complets."""
        try:
            # Test en temps r√©el des services
            current_elastic_health = False
            current_qdrant_health = False
            
            if elastic_client:
                try:
                    current_elastic_health = await elastic_client.is_healthy()
                except:
                    current_elastic_health = False
            
            if qdrant_client:
                try:
                    current_qdrant_health = await qdrant_client.is_healthy()
                except:
                    current_qdrant_health = False
            
            # Calcul de l'√©tat global
            services_status = {
                "elasticsearch": {
                    "available": elastic_client is not None,
                    "healthy": current_elastic_health,
                    "provider": "Bonsai"
                },
                "qdrant": {
                    "available": qdrant_client is not None,
                    "healthy": current_qdrant_health
                },
                "embeddings": {
                    "available": hasattr(embedding_service, 'client'),
                    "healthy": hasattr(embedding_service, 'client')
                },
                "reranking": {
                    "available": hasattr(reranker_service, 'client'),
                    "healthy": hasattr(reranker_service, 'client')
                },
                "cache": {
                    "available": search_cache is not None,
                    "healthy": search_cache is not None
                },
                "metrics": {
                    "available": metrics_collector is not None,
                    "healthy": metrics_collector is not None
                }
            }
            
            # Calculer les capacit√©s de recherche
            search_capabilities = {
                "lexical_search": services_status["elasticsearch"]["healthy"],
                "semantic_search": services_status["qdrant"]["healthy"],
                "hybrid_search": (services_status["elasticsearch"]["healthy"] and 
                                services_status["qdrant"]["healthy"]),
                "ai_reranking": services_status["reranking"]["healthy"]
            }
            
            # D√©terminer l'√©tat global
            critical_healthy = (services_status["elasticsearch"]["healthy"] and 
                              services_status["qdrant"]["healthy"])
            
            if critical_healthy and search_capabilities["ai_reranking"]:
                overall_status = "fully_operational"
            elif critical_healthy:
                overall_status = "operational"
            elif services_status["elasticsearch"]["healthy"] or services_status["qdrant"]["healthy"]:
                overall_status = "degraded"
            else:
                overall_status = "unhealthy"
            
            # Calcul de l'uptime
            uptime_seconds = time.time() - startup_time if startup_time else 0
            
            return {
                "service": "search_service",
                "version": "1.0.0",
                "status": overall_status,
                "timestamp": datetime.now().isoformat(),
                "uptime": {
                    "seconds": round(uptime_seconds, 2),
                    "human": str(timedelta(seconds=int(uptime_seconds)))
                },
                "services": services_status,
                "search_capabilities": search_capabilities,
                "startup_diagnostics": startup_diagnostics.get("summary", {}),
                "performance": {
                    "startup_time": startup_diagnostics.get("summary", {}).get("startup_duration"),
                    "operational_services": startup_diagnostics.get("summary", {}).get("operational_count", 0),
                    "total_services": startup_diagnostics.get("summary", {}).get("total_services", 0)
                },
                "clients_injected": {
                    "elasticsearch": elastic_client is not None,
                    "qdrant": qdrant_client is not None,
                    "cache": search_cache is not None,
                    "metrics": metrics_collector is not None
                }
            }
            
        except Exception as e:
            logger.error(f"Erreur lors du health check: {e}")
            return {
                "service": "search_service",
                "version": "1.0.0",
                "status": "error",
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
                "clients_injected": {
                    "elasticsearch": elastic_client is not None,
                    "qdrant": qdrant_client is not None,
                    "cache": search_cache is not None,
                    "metrics": metrics_collector is not None
                }
            }
    
    @app.get("/")
    async def root():
        """Point d'entr√©e racine du service."""
        uptime_seconds = time.time() - startup_time if startup_time else 0
        summary = startup_diagnostics.get("summary", {})
        
        return {
            "service": "Harena Search Service",
            "description": "Service de recherche hybride (lexicale + s√©mantique)",
            "version": "1.0.0",
            "status": summary.get("status", "unknown"),
            "uptime_seconds": round(uptime_seconds, 2),
            "search_engines": {
                "elasticsearch": "Bonsai (recherche lexicale)",
                "qdrant": "Stockage vectoriel (recherche s√©mantique)"
            },
            "capabilities": summary.get("search_capabilities", {}),
            "endpoints": {
                "health": "GET /health",
                "search": "POST /api/v1/search",
                "suggestions": "GET /api/v1/search/suggest"
            },
            "clients_status": {
                "elasticsearch_injected": elastic_client is not None,
                "qdrant_injected": qdrant_client is not None,
                "cache_injected": search_cache is not None,
                "metrics_injected": metrics_collector is not None
            },
            "timestamp": datetime.now().isoformat()
        }
    
    @app.get("/startup-diagnostics")
    async def get_startup_diagnostics():
        """Retourne les diagnostics de d√©marrage complets."""
        return {
            "timestamp": datetime.now().isoformat(),
            "startup_time": startup_time,
            "diagnostics": startup_diagnostics,
            "current_clients_status": {
                "elasticsearch": {
                    "injected": elastic_client is not None,
                    "type": type(elastic_client).__name__ if elastic_client else None
                },
                "qdrant": {
                    "injected": qdrant_client is not None,
                    "type": type(qdrant_client).__name__ if qdrant_client else None
                },
                "cache": {
                    "injected": search_cache is not None,
                    "type": type(search_cache).__name__ if search_cache else None
                },
                "metrics": {
                    "injected": metrics_collector is not None,
                    "type": type(metrics_collector).__name__ if metrics_collector else None
                }
            }
        }
    
    @app.get("/clients-debug")
    async def debug_clients():
        """Debug d√©taill√© des clients pour diagnostic."""
        try:
            # Import du module routes pour v√©rifier l'injection
            import search_service.api.routes as routes_module
            
            return {
                "timestamp": datetime.now().isoformat(),
                "global_clients": {
                    "elastic_client": {
                        "exists": elastic_client is not None,
                        "type": type(elastic_client).__name__ if elastic_client else None,
                        "id": id(elastic_client) if elastic_client else None
                    },
                    "qdrant_client": {
                        "exists": qdrant_client is not None,
                        "type": type(qdrant_client).__name__ if qdrant_client else None,
                        "id": id(qdrant_client) if qdrant_client else None
                    },
                    "search_cache": {
                        "exists": search_cache is not None,
                        "type": type(search_cache).__name__ if search_cache else None,
                        "id": id(search_cache) if search_cache else None
                    },
                    "metrics_collector": {
                        "exists": metrics_collector is not None,
                        "type": type(metrics_collector).__name__ if metrics_collector else None,
                        "id": id(metrics_collector) if metrics_collector else None
                    }
                },
                "routes_module_clients": {
                    "elastic_client": {
                        "exists": hasattr(routes_module, 'elastic_client') and routes_module.elastic_client is not None,
                        "type": type(getattr(routes_module, 'elastic_client', None)).__name__ if hasattr(routes_module, 'elastic_client') and routes_module.elastic_client else None,
                        "id": id(getattr(routes_module, 'elastic_client', None)) if hasattr(routes_module, 'elastic_client') and routes_module.elastic_client else None,
                        "same_as_global": (hasattr(routes_module, 'elastic_client') and 
                                         routes_module.elastic_client is elastic_client)
                    },
                    "qdrant_client": {
                        "exists": hasattr(routes_module, 'qdrant_client') and routes_module.qdrant_client is not None,
                        "type": type(getattr(routes_module, 'qdrant_client', None)).__name__ if hasattr(routes_module, 'qdrant_client') and routes_module.qdrant_client else None,
                        "id": id(getattr(routes_module, 'qdrant_client', None)) if hasattr(routes_module, 'qdrant_client') and routes_module.qdrant_client else None,
                        "same_as_global": (hasattr(routes_module, 'qdrant_client') and 
                                         routes_module.qdrant_client is qdrant_client)
                    }
                },
                "injection_success": {
                    "elastic_client": (hasattr(routes_module, 'elastic_client') and 
                                     routes_module.elastic_client is elastic_client and
                                     elastic_client is not None),
                    "qdrant_client": (hasattr(routes_module, 'qdrant_client') and 
                                    routes_module.qdrant_client is qdrant_client and
                                    qdrant_client is not None)
                }
            }
        except Exception as e:
            return {
                "error": f"Debug failed: {str(e)}",
                "timestamp": datetime.now().isoformat()
            }
    
    # Inclure les routes de recherche
    app.include_router(router, prefix="/api/v1/search", tags=["search"])
    
    return app

# ==================== CR√âATION DE L'APPLICATION ====================

app = create_app()

# ==================== POINT D'ENTR√âE ====================

if __name__ == "__main__":
    import uvicorn
    
    logger.info("üîß D√©marrage en mode d√©veloppement local")
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )
"""
Module principal du service de recherche avec diagnostic complet au d√©marrage.

Ce module initialise et configure le service de recherche hybride de Harena,
avec v√©rifications d√©taill√©es des services externes (Bonsai Elasticsearch + Qdrant).
"""
import logging
import time
import sys
import asyncio
from contextlib import asynccontextmanager
from datetime import datetime, timedelta
from typing import Dict, Any, Optional
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware

from search_service.api.routes import router
from search_service.core.embeddings import embedding_service
from search_service.core.reranker import reranker_service
from search_service.storage.elastic_client import ElasticClient
from search_service.storage.qdrant_client import QdrantClient
from search_service.utils.cache import SearchCache
from search_service.utils.metrics import MetricsCollector
from config_service.config import settings

# Configuration du logging avec format d√©taill√©
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - [%(name)s] - %(levelname)s - %(module)s:%(funcName)s:%(lineno)d - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger("search_service.main")

# R√©duire le bruit des biblioth√®ques externes
logging.getLogger("urllib3").setLevel(logging.WARNING)
logging.getLogger("elasticsearch").setLevel(logging.WARNING)
logging.getLogger("qdrant_client").setLevel(logging.WARNING)
logging.getLogger("httpx").setLevel(logging.WARNING)

# ==================== VARIABLES GLOBALES ====================

# Instances globales des services
elastic_client = None
qdrant_client = None
search_cache = None
metrics_collector = None
startup_time = None
startup_diagnostics = {}

# ==================== FONCTIONS DE DIAGNOSTIC ====================

def log_startup_banner():
    """Affiche la banni√®re de d√©marrage."""
    banner = """
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                        üîç HARENA SEARCH SERVICE üîç                          ‚ïë
‚ïë                     Service de Recherche Hybride v1.0.0                     ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    """
    print(banner)
    logger.info("üöÄ === D√âMARRAGE DU SERVICE DE RECHERCHE HYBRIDE ===")

def check_environment_configuration() -> Dict[str, Any]:
    """V√©rifie et diagnostique la configuration des variables d'environnement."""
    logger.info("üîß === V√âRIFICATION DE LA CONFIGURATION ===")
    
    config_status = {
        "critical_services": {},
        "optional_services": {},
        "summary": {}
    }
    
    # Services critiques pour le fonctionnement
    critical_configs = {
        "BONSAI_URL": {
            "description": "Elasticsearch via Bonsai (recherche lexicale)",
            "value": settings.BONSAI_URL,
            "required": True
        },
        "QDRANT_URL": {
            "description": "Qdrant (recherche s√©mantique)",
            "value": settings.QDRANT_URL,
            "required": True
        }
    }
    
    # Services optionnels mais recommand√©s
    optional_configs = {
        "OPENAI_API_KEY": {
            "description": "OpenAI (g√©n√©ration d'embeddings)",
            "value": settings.OPENAI_API_KEY,
            "impact": "Pas d'embeddings automatiques"
        },
        "COHERE_KEY": {
            "description": "Cohere (reranking des r√©sultats)",
            "value": settings.COHERE_KEY,
            "impact": "Pas de reranking intelligent"
        },
        "QDRANT_API_KEY": {
            "description": "Qdrant API Key (authentification)",
            "value": settings.QDRANT_API_KEY,
            "impact": "Connexion sans authentification"
        }
    }
    
    # V√©rification des services critiques
    critical_ok_count = 0
    for key, info in critical_configs.items():
        is_configured = bool(info["value"])
        config_status["critical_services"][key] = {
            "configured": is_configured,
            "description": info["description"],
            "required": info["required"]
        }
        
        if is_configured:
            critical_ok_count += 1
            # Masquer les URLs sensibles pour l'affichage
            if "URL" in key:
                safe_value = info["value"].split('@')[-1] if '@' in info["value"] else info["value"]
                logger.info(f"‚úÖ {key}: {safe_value}")
            else:
                logger.info(f"‚úÖ {key}: Configur√©")
        else:
            logger.error(f"‚ùå {key}: NON CONFIGUR√â - {info['description']}")
    
    # V√©rification des services optionnels
    optional_ok_count = 0
    for key, info in optional_configs.items():
        is_configured = bool(info["value"])
        config_status["optional_services"][key] = {
            "configured": is_configured,
            "description": info["description"],
            "impact": info["impact"]
        }
        
        if is_configured:
            optional_ok_count += 1
            if "KEY" in key:
                masked_value = f"{info['value'][:8]}...{info['value'][-4:]}" if len(info["value"]) > 12 else "***"
                logger.info(f"‚úÖ {key}: {masked_value}")
            else:
                logger.info(f"‚úÖ {key}: Configur√©")
        else:
            logger.warning(f"‚ö†Ô∏è {key}: Non configur√© - {info['impact']}")
    
    # R√©sum√© de la configuration
    config_status["summary"] = {
        "critical_configured": critical_ok_count,
        "critical_total": len(critical_configs),
        "optional_configured": optional_ok_count,
        "optional_total": len(optional_configs),
        "critical_percentage": (critical_ok_count / len(critical_configs)) * 100,
        "optional_percentage": (optional_ok_count / len(optional_configs)) * 100
    }
    
    if critical_ok_count == len(critical_configs):
        logger.info("üéâ CONFIGURATION: Tous les services critiques sont configur√©s")
    elif critical_ok_count > 0:
        logger.warning("‚ö†Ô∏è CONFIGURATION: Services critiques PARTIELLEMENT configur√©s")
    else:
        logger.error("üö® CONFIGURATION: AUCUN service critique configur√©")
    
    return config_status

async def initialize_elasticsearch() -> tuple[bool, Optional[ElasticClient], Dict[str, Any]]:
    """Initialise et teste la connexion Elasticsearch (Bonsai)."""
    logger.info("üîç === INITIALISATION ELASTICSEARCH (BONSAI) ===")
    diagnostic = {
        "service": "elasticsearch_bonsai",
        "configured": False,
        "connected": False,
        "healthy": False,
        "error": None,
        "connection_time": None,
        "cluster_info": {},
        "indices_info": {}
    }
    
    if not settings.BONSAI_URL:
        logger.error("‚ùå BONSAI_URL non configur√©e")
        diagnostic["error"] = "BONSAI_URL not configured"
        return False, None, diagnostic
    
    diagnostic["configured"] = True
    
    # Masquer les credentials pour l'affichage
    safe_url = settings.BONSAI_URL.split('@')[-1] if '@' in settings.BONSAI_URL else settings.BONSAI_URL
    logger.info(f"üîó Connexion √† Bonsai Elasticsearch: {safe_url}")
    
    try:
        start_time = time.time()
        client = ElasticClient()
        
        logger.info("‚è±Ô∏è Test de connexion Elasticsearch...")
        await client.initialize()
        
        connection_time = time.time() - start_time
        diagnostic["connection_time"] = round(connection_time, 3)
        diagnostic["connected"] = True
        
        logger.info(f"‚úÖ Connexion √©tablie en {connection_time:.3f}s")
        
        # Test de sant√©
        logger.info("ü©∫ V√©rification de la sant√© du cluster...")
        is_healthy = await client.is_healthy()
        diagnostic["healthy"] = is_healthy
        
        if is_healthy:
            logger.info("üü¢ Cluster Elasticsearch en bonne sant√©")
            
            # R√©cup√©rer les informations du cluster
            try:
                cluster_info = await client.get_cluster_info()
                diagnostic["cluster_info"] = cluster_info
                logger.info(f"üìä Cluster: {cluster_info.get('cluster_name', 'Unknown')}")
                logger.info(f"üìä Version: {cluster_info.get('version', {}).get('number', 'Unknown')}")
                logger.info(f"üìä N≈ìuds: {cluster_info.get('nodes', 'Unknown')}")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Impossible de r√©cup√©rer les infos cluster: {e}")
            
            # V√©rifier les indices
            try:
                indices_info = await client.get_indices_info()
                diagnostic["indices_info"] = indices_info
                if indices_info:
                    logger.info(f"üìÅ Indices disponibles: {len(indices_info)}")
                    for index_name, info in indices_info.items():
                        logger.info(f"   - {index_name}: {info.get('docs', {}).get('count', 0)} documents")
                else:
                    logger.info("üìÅ Aucun indice trouv√©")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Impossible de lister les indices: {e}")
            
            return True, client, diagnostic
        else:
            logger.error("üî¥ Cluster Elasticsearch non op√©rationnel")
            diagnostic["error"] = "Cluster unhealthy"
            return False, client, diagnostic
            
    except Exception as e:
        connection_time = time.time() - start_time
        diagnostic["connection_time"] = round(connection_time, 3)
        diagnostic["error"] = str(e)
        
        logger.error(f"üí• Erreur Elasticsearch apr√®s {connection_time:.3f}s:")
        logger.error(f"   Type: {type(e).__name__}")
        logger.error(f"   Message: {str(e)}")
        
        # Diagnostic sp√©cifique des erreurs
        error_str = str(e).lower()
        if "connection" in error_str or "timeout" in error_str:
            logger.error("üîå DIAGNOSTIC: Probl√®me de connectivit√© r√©seau")
            logger.error("   - V√©rifiez l'URL Bonsai")
            logger.error("   - V√©rifiez la connectivit√© r√©seau")
        elif "401" in str(e) or "403" in str(e) or "auth" in error_str:
            logger.error("üîë DIAGNOSTIC: Probl√®me d'authentification")
            logger.error("   - V√©rifiez les credentials dans BONSAI_URL")
        elif "ssl" in error_str or "certificate" in error_str:
            logger.error("üîí DIAGNOSTIC: Probl√®me SSL/TLS")
            logger.error("   - V√©rifiez les certificats SSL")
        
        return False, None, diagnostic

async def initialize_qdrant() -> tuple[bool, Optional[QdrantClient], Dict[str, Any]]:
    """Initialise et teste la connexion Qdrant."""
    logger.info("üéØ === INITIALISATION QDRANT ===")
    diagnostic = {
        "service": "qdrant",
        "configured": False,
        "connected": False,
        "healthy": False,
        "error": None,
        "connection_time": None,
        "collections_info": {},
        "version_info": {}
    }
    
    if not settings.QDRANT_URL:
        logger.error("‚ùå QDRANT_URL non configur√©e")
        diagnostic["error"] = "QDRANT_URL not configured"
        return False, None, diagnostic
    
    diagnostic["configured"] = True
    logger.info(f"üîó Connexion √† Qdrant: {settings.QDRANT_URL}")
    
    if settings.QDRANT_API_KEY:
        logger.info("üîë Authentification par API Key activ√©e")
    else:
        logger.info("üîì Connexion sans authentification")
    
    try:
        start_time = time.time()
        client = QdrantClient()
        
        logger.info("‚è±Ô∏è Test de connexion Qdrant...")
        await client.initialize()
        
        connection_time = time.time() - start_time
        diagnostic["connection_time"] = round(connection_time, 3)
        diagnostic["connected"] = True
        
        logger.info(f"‚úÖ Connexion √©tablie en {connection_time:.3f}s")
        
        # Test de sant√©
        logger.info("ü©∫ V√©rification de la sant√© de Qdrant...")
        is_healthy = await client.is_healthy()
        diagnostic["healthy"] = is_healthy
        
        if is_healthy:
            logger.info("üü¢ Service Qdrant en bonne sant√©")
            
            # R√©cup√©rer les informations des collections
            try:
                collections_info = await client.get_collections_info()
                diagnostic["collections_info"] = collections_info
                if collections_info:
                    logger.info(f"üìä Collections disponibles: {len(collections_info)}")
                    for collection_name, info in collections_info.items():
                        points_count = info.get('points_count', 0)
                        vectors_count = info.get('vectors_count', 0)
                        logger.info(f"   - {collection_name}: {points_count} points, {vectors_count} vecteurs")
                else:
                    logger.warning("üìä Aucune collection trouv√©e")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Impossible de r√©cup√©rer les infos collections: {e}")
            
            # V√©rifier la collection des transactions
            try:
                collection_exists = await client.collection_exists("financial_transactions")
                if collection_exists:
                    logger.info("‚úÖ Collection 'financial_transactions' trouv√©e")
                else:
                    logger.warning("‚ö†Ô∏è Collection 'financial_transactions' non trouv√©e")
                    logger.warning("   - Lancez l'enrichment_service pour cr√©er la collection")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Impossible de v√©rifier la collection: {e}")
            
            return True, client, diagnostic
        else:
            logger.error("üî¥ Service Qdrant non op√©rationnel")
            diagnostic["error"] = "Service unhealthy"
            return False, client, diagnostic
            
    except Exception as e:
        connection_time = time.time() - start_time
        diagnostic["connection_time"] = round(connection_time, 3)
        diagnostic["error"] = str(e)
        
        logger.error(f"üí• Erreur Qdrant apr√®s {connection_time:.3f}s:")
        logger.error(f"   Type: {type(e).__name__}")
        logger.error(f"   Message: {str(e)}")
        
        # Diagnostic sp√©cifique des erreurs
        error_str = str(e).lower()
        if "connection" in error_str or "timeout" in error_str:
            logger.error("üîå DIAGNOSTIC: Probl√®me de connectivit√© r√©seau")
            logger.error("   - V√©rifiez l'URL Qdrant")
            logger.error("   - V√©rifiez la connectivit√© r√©seau")
        elif "401" in str(e) or "403" in str(e) or "auth" in error_str:
            logger.error("üîë DIAGNOSTIC: Probl√®me d'authentification")
            logger.error("   - V√©rifiez QDRANT_API_KEY")
        elif "404" in str(e):
            logger.error("üìÇ DIAGNOSTIC: Collection non trouv√©e")
            logger.error("   - La collection 'financial_transactions' doit √™tre cr√©√©e")
            logger.error("   - Lancez d'abord l'enrichment_service")
        
        return False, None, diagnostic

async def initialize_ai_services() -> Dict[str, Dict[str, Any]]:
    """Initialise les services IA (embeddings et reranking)."""
    logger.info("ü§ñ === INITIALISATION DES SERVICES IA ===")
    ai_diagnostics = {}
    
    # Service d'embeddings (OpenAI)
    embedding_diagnostic = {
        "service": "openai_embeddings",
        "configured": bool(settings.OPENAI_API_KEY),
        "initialized": False,
        "error": None,
        "init_time": None
    }
    
    if settings.OPENAI_API_KEY:
        try:
            logger.info("ü§ñ Initialisation du service d'embeddings OpenAI...")
            start_time = time.time()
            await embedding_service.initialize()
            init_time = time.time() - start_time
            
            embedding_diagnostic["initialized"] = True
            embedding_diagnostic["init_time"] = round(init_time, 3)
            logger.info(f"‚úÖ Service d'embeddings initialis√© en {init_time:.3f}s")
        except Exception as e:
            embedding_diagnostic["error"] = str(e)
            logger.error(f"‚ùå √âchec initialisation embeddings: {type(e).__name__}: {str(e)}")
            logger.error("üìç V√©rifiez OPENAI_API_KEY et la connectivit√© internet")
    else:
        logger.warning("‚ö†Ô∏è OPENAI_API_KEY non configur√©e - embeddings indisponibles")
    
    ai_diagnostics["embeddings"] = embedding_diagnostic
    
    # Service de reranking (Cohere)
    reranking_diagnostic = {
        "service": "cohere_reranking",
        "configured": bool(settings.COHERE_KEY),
        "initialized": False,
        "error": None,
        "init_time": None
    }
    
    if settings.COHERE_KEY:
        try:
            logger.info("üéØ Initialisation du service de reranking Cohere...")
            start_time = time.time()
            await reranker_service.initialize()
            init_time = time.time() - start_time
            
            reranking_diagnostic["initialized"] = True
            reranking_diagnostic["init_time"] = round(init_time, 3)
            logger.info(f"‚úÖ Service de reranking initialis√© en {init_time:.3f}s")
        except Exception as e:
            reranking_diagnostic["error"] = str(e)
            logger.error(f"‚ùå √âchec initialisation reranking: {type(e).__name__}: {str(e)}")
            logger.error("üìç V√©rifiez COHERE_KEY et la connectivit√© internet")
    else:
        logger.warning("‚ö†Ô∏è COHERE_KEY non configur√©e - reranking indisponible")
    
    ai_diagnostics["reranking"] = reranking_diagnostic
    
    return ai_diagnostics

def initialize_cache_and_metrics() -> Dict[str, Dict[str, Any]]:
    """Initialise le cache et les m√©triques."""
    logger.info("üóÉÔ∏è === INITIALISATION CACHE ET M√âTRIQUES ===")
    utils_diagnostics = {}
    
    # Cache de recherche
    cache_diagnostic = {
        "service": "search_cache",
        "initialized": False,
        "error": None
    }
    
    global search_cache
    try:
        search_cache = SearchCache()
        cache_diagnostic["initialized"] = True
        logger.info("‚úÖ Cache de recherche initialis√©")
    except Exception as e:
        cache_diagnostic["error"] = str(e)
        logger.error(f"‚ùå Erreur initialisation cache: {e}")
        search_cache = None
    
    utils_diagnostics["cache"] = cache_diagnostic
    
    # Collecteur de m√©triques
    metrics_diagnostic = {
        "service": "metrics_collector",
        "initialized": False,
        "error": None
    }
    
    global metrics_collector
    try:
        metrics_collector = MetricsCollector()
        metrics_diagnostic["initialized"] = True
        logger.info("‚úÖ Collecteur de m√©triques initialis√©")
    except Exception as e:
        metrics_diagnostic["error"] = str(e)
        logger.error(f"‚ùå Erreur initialisation m√©triques: {e}")
        metrics_collector = None
    
    utils_diagnostics["metrics"] = metrics_diagnostic
    
    return utils_diagnostics

def inject_dependencies():
    """Injecte les d√©pendances dans le module routes."""
    logger.info("üîó === INJECTION DES D√âPENDANCES ===")
    
    import search_service.api.routes as routes
    routes.elastic_client = elastic_client
    routes.qdrant_client = qdrant_client
    routes.search_cache = search_cache
    routes.metrics_collector = metrics_collector
    
    logger.info("‚úÖ D√©pendances inject√©es dans les routes")

def generate_startup_summary() -> Dict[str, Any]:
    """G√©n√®re un r√©sum√© complet du d√©marrage."""
    global startup_diagnostics
    
    startup_duration = time.time() - startup_time
    
    # Compter les services op√©rationnels
    operational_services = []
    failed_services = []
    
    # Services critiques
    if elastic_client:
        operational_services.append("Elasticsearch (Bonsai)")
    else:
        failed_services.append("Elasticsearch (Bonsai)")
    
    if qdrant_client:
        operational_services.append("Qdrant")
    else:
        failed_services.append("Qdrant")
    
    # Services IA
    ai_diag = startup_diagnostics.get("ai_services", {})
    if ai_diag.get("embeddings", {}).get("initialized"):
        operational_services.append("OpenAI Embeddings")
    else:
        failed_services.append("OpenAI Embeddings")
    
    if ai_diag.get("reranking", {}).get("initialized"):
        operational_services.append("Cohere Reranking")
    else:
        failed_services.append("Cohere Reranking")
    
    # Services utilitaires
    if search_cache:
        operational_services.append("Cache")
    if metrics_collector:
        operational_services.append("M√©triques")
    
    # D√©terminer l'√©tat global du service
    critical_services_ok = elastic_client and qdrant_client
    
    if critical_services_ok:
        if len(operational_services) >= 4:  # Elasticsearch + Qdrant + au moins 2 autres
            status = "FULLY_OPERATIONAL"
            status_icon = "üéâ"
            status_msg = "COMPL√àTEMENT OP√âRATIONNEL"
        else:
            status = "OPERATIONAL"
            status_icon = "‚úÖ"
            status_msg = "OP√âRATIONNEL"
    elif elastic_client or qdrant_client:
        status = "DEGRADED"
        status_icon = "‚ö†Ô∏è"
        status_msg = "OP√âRATIONNEL D√âGRAD√â"
    else:
        status = "FAILED"
        status_icon = "üö®"
        status_msg = "NON OP√âRATIONNEL"
    
    summary = {
        "status": status,
        "status_icon": status_icon,
        "status_message": status_msg,
        "startup_duration": round(startup_duration, 2),
        "operational_services": operational_services,
        "failed_services": failed_services,
        "operational_count": len(operational_services),
        "total_services": len(operational_services) + len(failed_services),
        "critical_services_operational": critical_services_ok,
        "search_capabilities": {
            "lexical_search": bool(elastic_client),
            "semantic_search": bool(qdrant_client),
            "hybrid_search": bool(elastic_client and qdrant_client),
            "ai_reranking": ai_diag.get("reranking", {}).get("initialized", False)
        }
    }
    
    # Affichage du r√©sum√©
    logger.info("=" * 100)
    logger.info(f"{status_icon} SERVICE DE RECHERCHE HYBRIDE - √âTAT: {status_msg}")
    logger.info(f"‚è±Ô∏è Temps de d√©marrage: {startup_duration:.2f}s")
    logger.info(f"üéØ Services op√©rationnels ({len(operational_services)}/{len(operational_services) + len(failed_services)}): {', '.join(operational_services) if operational_services else 'Aucun'}")
    
    if failed_services:
        logger.warning(f"‚ùå Services √©chou√©s: {', '.join(failed_services)}")
    
    # Messages informatifs selon l'√©tat
    if status == "FULLY_OPERATIONAL":
        logger.info("üöÄ Toutes les fonctionnalit√©s de recherche avanc√©e sont disponibles")
        logger.info("   ‚úì Recherche lexicale (Elasticsearch/Bonsai)")
        logger.info("   ‚úì Recherche s√©mantique (Qdrant)")
        logger.info("   ‚úì Recherche hybride avec fusion des scores")
        logger.info("   ‚úì Reranking intelligent des r√©sultats")
    elif status == "OPERATIONAL":
        logger.info("‚úÖ Fonctionnalit√©s de recherche de base disponibles")
        logger.info("   ‚úì Recherche lexicale ET s√©mantique")
        logger.warning("   ‚ö†Ô∏è Certaines fonctionnalit√©s avanc√©es peuvent √™tre limit√©es")
    elif status == "DEGRADED":
        logger.warning("‚ö†Ô∏è Service de recherche en mode d√©grad√©")
        if elastic_client and not qdrant_client:
            logger.warning("   ‚úì Recherche lexicale disponible")
            logger.warning("   ‚ùå Recherche s√©mantique indisponible")
        elif qdrant_client and not elastic_client:
            logger.warning("   ‚ùå Recherche lexicale indisponible")
            logger.warning("   ‚úì Recherche s√©mantique disponible")
    else:
        logger.error("üö® Service de recherche indisponible")
        logger.error("   ‚ùå V√©rifiez la configuration BONSAI_URL et QDRANT_URL")
        logger.error("   ‚ùå V√©rifiez la connectivit√© r√©seau")
    
    logger.info("üìä Endpoints disponibles:")
    logger.info("   GET  /health - V√©rification de sant√© d√©taill√©e")
    logger.info("   POST /api/v1/search - Recherche de transactions")
    logger.info("   GET  /api/v1/search/suggest - Suggestions de recherche")
    logger.info("=" * 100)
    
    return summary

# ==================== CYCLE DE VIE DE L'APPLICATION ====================

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Gestionnaire du cycle de vie de l'application avec diagnostics complets."""
    global startup_time, startup_diagnostics, elastic_client, qdrant_client
    
    try:
        # === PHASE DE D√âMARRAGE ===
        log_startup_banner()
        startup_time = time.time()
        
        # 1. V√©rification de la configuration
        config_status = check_environment_configuration()
        startup_diagnostics["configuration"] = config_status
        
        # 2. Initialisation d'Elasticsearch (Bonsai)
        elastic_success, elastic_client, elastic_diag = await initialize_elasticsearch()
        startup_diagnostics["elasticsearch"] = elastic_diag
        
        # 3. Initialisation de Qdrant
        qdrant_success, qdrant_client, qdrant_diag = await initialize_qdrant()
        startup_diagnostics["qdrant"] = qdrant_diag
        
        # 4. Initialisation des services IA
        ai_diagnostics = await initialize_ai_services()
        startup_diagnostics["ai_services"] = ai_diagnostics
        
        # 5. Initialisation du cache et des m√©triques
        utils_diagnostics = initialize_cache_and_metrics()
        startup_diagnostics["utilities"] = utils_diagnostics
        
        # 6. Injection des d√©pendances
        inject_dependencies()
        
        # 7. G√©n√©ration du r√©sum√© de d√©marrage
        startup_summary = generate_startup_summary()
        startup_diagnostics["summary"] = startup_summary
        
        # Application pr√™te
        yield
        
    except Exception as e:
        logger.critical(f"üí• ERREUR CRITIQUE au d√©marrage: {type(e).__name__}: {str(e)}")
        import traceback
        logger.critical(f"Stack trace:\n{traceback.format_exc()}")
        raise
    
    finally:
        # === PHASE D'ARR√äT ===
        logger.info("‚èπÔ∏è Arr√™t du service de recherche...")
        
        # Fermeture des services IA
        try:
            if hasattr(embedding_service, 'close'):
                await embedding_service.close()
                logger.info("‚úÖ Service d'embeddings ferm√©")
        except Exception as e:
            logger.error(f"‚ùå Erreur fermeture embeddings: {e}")
        
        try:
            if hasattr(reranker_service, 'close'):
                await reranker_service.close()
                logger.info("‚úÖ Service de reranking ferm√©")
        except Exception as e:
            logger.error(f"‚ùå Erreur fermeture reranking: {e}")
        
        logger.info("üîö Service de recherche arr√™t√© proprement")

# ==================== APPLICATION FASTAPI ====================

def create_app() -> FastAPI:
    """Cr√©e et configure l'application FastAPI."""
    app = FastAPI(
        title="Harena Search Service",
        description="Service de recherche hybride combinant recherche lexicale (Bonsai) et s√©mantique (Qdrant)",
        version="1.0.0",
        docs_url="/docs",
        redoc_url="/redoc",
        lifespan=lifespan
    )
    
    # Configuration CORS
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],  # √Ä restreindre en production
        allow_credentials=True,
        allow_methods=["GET", "POST", "PUT", "DELETE"],
        allow_headers=["*"],
    )
    
    # Endpoints de sant√© √©tendus
    @app.get("/health")
    async def health_check():
        """Endpoint de sant√© d√©taill√© avec diagnostics complets."""
        try:
            # Test en temps r√©el des services
            current_elastic_health = elastic_client.is_healthy() if elastic_client else False
            current_qdrant_health = qdrant_client.is_healthy() if qdrant_client else False
            
            # Calcul de l'√©tat global
            services_status = {
                "elasticsearch": {
                    "available": elastic_client is not None,
                    "healthy": await current_elastic_health if elastic_client else False,
                    "provider": "Bonsai"
                },
                "qdrant": {
                    "available": qdrant_client is not None,
                    "healthy": await current_qdrant_health if qdrant_client else False
                },
                "embeddings": {
                    "available": hasattr(embedding_service, 'client'),
                    "healthy": hasattr(embedding_service, 'client')
                },
                "reranking": {
                    "available": hasattr(reranker_service, 'client'),
                    "healthy": hasattr(reranker_service, 'client')
                },
                "cache": {
                    "available": search_cache is not None,
                    "healthy": search_cache is not None
                },
                "metrics": {
                    "available": metrics_collector is not None,
                    "healthy": metrics_collector is not None
                }
            }
            
            # Calculer les capacit√©s de recherche
            search_capabilities = {
                "lexical_search": services_status["elasticsearch"]["healthy"],
                "semantic_search": services_status["qdrant"]["healthy"],
                "hybrid_search": (services_status["elasticsearch"]["healthy"] and 
                                services_status["qdrant"]["healthy"]),
                "ai_reranking": services_status["reranking"]["healthy"]
            }
            
            # D√©terminer l'√©tat global
            critical_healthy = (services_status["elasticsearch"]["healthy"] and 
                              services_status["qdrant"]["healthy"])
            
            if critical_healthy and search_capabilities["ai_reranking"]:
                overall_status = "fully_operational"
            elif critical_healthy:
                overall_status = "operational"
            elif services_status["elasticsearch"]["healthy"] or services_status["qdrant"]["healthy"]:
                overall_status = "degraded"
            else:
                overall_status = "unhealthy"
            
            # Calcul de l'uptime
            uptime_seconds = time.time() - startup_time if startup_time else 0
            
            return {
                "service": "search_service",
                "version": "1.0.0",
                "status": overall_status,
                "timestamp": datetime.now().isoformat(),
                "uptime": {
                    "seconds": round(uptime_seconds, 2),
                    "human": str(timedelta(seconds=int(uptime_seconds)))
                },
                "services": services_status,
                "search_capabilities": search_capabilities,
                "startup_diagnostics": startup_diagnostics.get("summary", {}),
                "performance": {
                    "startup_time": startup_diagnostics.get("summary", {}).get("startup_duration"),
                    "operational_services": startup_diagnostics.get("summary", {}).get("operational_count", 0),
                    "total_services": startup_diagnostics.get("summary", {}).get("total_services", 0)
                }
            }
            
        except Exception as e:
            logger.error(f"Erreur lors du health check: {e}")
            return {
                "service": "search_service",
                "version": "1.0.0",
                "status": "error",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
    
    @app.get("/")
    async def root():
        """Point d'entr√©e racine du service."""
        uptime_seconds = time.time() - startup_time if startup_time else 0
        summary = startup_diagnostics.get("summary", {})
        
        return {
            "service": "Harena Search Service",
            "description": "Service de recherche hybride (lexicale + s√©mantique)",
            "version": "1.0.0",
            "status": summary.get("status", "unknown"),
            "uptime_seconds": round(uptime_seconds, 2),
            "search_engines": {
                "elasticsearch": "Bonsai (recherche lexicale)",
                "qdrant": "Stockage vectoriel (recherche s√©mantique)"
            },
            "capabilities": summary.get("search_capabilities", {}),
            "endpoints": {
                "health": "GET /health",
                "search": "POST /api/v1/search",
                "suggestions": "GET /api/v1/search/suggest"
            },
            "timestamp": datetime.now().isoformat()
        }
    
    @app.get("/startup-diagnostics")
    async def get_startup_diagnostics():
        """Retourne les diagnostics de d√©marrage complets."""
        return {
            "timestamp": datetime.now().isoformat(),
            "startup_time": startup_time,
            "diagnostics": startup_diagnostics
        }
    
    # Inclure les routes de recherche
    app.include_router(router, prefix="/api/v1/search", tags=["search"])
    
    return app

# ==================== CR√âATION DE L'APPLICATION ====================

app = create_app()

# ==================== POINT D'ENTR√âE ====================

if __name__ == "__main__":
    import uvicorn
    
    logger.info("üîß D√©marrage en mode d√©veloppement local")
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )