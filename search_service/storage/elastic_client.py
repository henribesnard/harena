"""
Client Elasticsearch avec logging am√©lior√© pour le monitoring et le debugging.
"""
import logging
import time
from typing import List, Dict, Any, Optional
from elasticsearch import AsyncElasticsearch
from elasticsearch.exceptions import ConnectionError, NotFoundError, RequestError, TransportError

from config_service.config import settings

# Configuration du logger sp√©cifique √† Elasticsearch
logger = logging.getLogger("search_service.elasticsearch")
# Logger s√©par√© pour les m√©triques
metrics_logger = logging.getLogger("search_service.metrics.elasticsearch")


class ElasticClient:
    """Client pour interagir avec Elasticsearch avec logging am√©lior√©."""
    
    def __init__(self):
        self.client = None
        self.index_name = "harena_transactions"
        self._initialized = False
        self._connection_attempts = 0
        self._last_health_check = None
        
    async def initialize(self):
        """Initialise la connexion Elasticsearch avec logging d√©taill√©."""
        logger.info("üîÑ Initialisation du client Elasticsearch...")
        start_time = time.time()
        
        # V√©rifier la configuration
        if not settings.BONSAI_URL:
            logger.error("‚ùå BONSAI_URL non configur√©e")
            return False
        
        # Masquer les credentials dans les logs
        safe_url = self._mask_credentials(settings.BONSAI_URL)
        logger.info(f"üîó Connexion √† Bonsai Elasticsearch: {safe_url}")
        
        try:
            self._connection_attempts += 1
            logger.info(f"üîÑ Tentative de connexion #{self._connection_attempts}")
            
            # Cr√©er le client avec configuration adapt√©e √† Bonsai
            self.client = AsyncElasticsearch(
                [settings.BONSAI_URL],
                verify_certs=True,
                ssl_show_warn=False,
                max_retries=3,
                retry_on_timeout=True,
                request_timeout=30.0,  # Utiliser request_timeout au lieu de timeout
                headers={
                    "Accept": "application/json",
                    "Content-Type": "application/json"
                }
            )
            
            # Test de connexion avec timeout
            logger.info("‚è±Ô∏è Test de connexion...")
            connection_start = time.time()
            
            # Test basic de connexion
            info = await self.client.info()
            connection_time = time.time() - connection_start
            
            # Logs d√©taill√©s de la connexion
            logger.info(f"‚úÖ Connexion r√©ussie en {connection_time:.2f}s")
            logger.info(f"üìä Elasticsearch version: {info['version']['number']}")
            logger.info(f"üè∑Ô∏è Cluster name: {info['cluster_name']}")
            logger.info(f"üÜî Cluster UUID: {info['cluster_uuid']}")
            
            # M√©triques de connexion
            metrics_logger.info(f"elasticsearch.connection.success,time={connection_time:.3f},attempt={self._connection_attempts}")
            
            # V√©rifier la sant√© du cluster
            await self._check_cluster_health()
            
            # Cr√©er l'index si n√©cessaire
            await self._setup_index()
            
            self._initialized = True
            total_time = time.time() - start_time
            logger.info(f"üéâ Client Elasticsearch initialis√© avec succ√®s en {total_time:.2f}s")
            
            return True
            
        except ConnectionError as e:
            logger.error(f"üîå Erreur de connexion Elasticsearch: {e}")
            metrics_logger.error(f"elasticsearch.connection.failed,type=connection,attempt={self._connection_attempts}")
            self._handle_connection_error(e)
            return False
            
        except TransportError as e:
            logger.error(f"üö´ Erreur de transport Elasticsearch: {e}")
            logger.error(f"üìç Status code: {e.status_code if hasattr(e, 'status_code') else 'N/A'}")
            metrics_logger.error(f"elasticsearch.connection.failed,type=transport,status={getattr(e, 'status_code', 'unknown')}")
            return False
            
        except Exception as e:
            logger.error(f"üí• Erreur inattendue lors de l'initialisation Elasticsearch: {type(e).__name__}: {e}")
            logger.error(f"üìç D√©tails: {str(e)}", exc_info=True)
            metrics_logger.error(f"elasticsearch.connection.failed,type=unexpected,error={type(e).__name__}")
            return False
            
        finally:
            if not self._initialized:
                self.client = None
                logger.warning("‚ö†Ô∏è Client Elasticsearch non initialis√© - recherche lexicale indisponible")
    
    async def _check_cluster_health(self):
        """V√©rifie la sant√© du cluster Elasticsearch."""
        try:
            health = await self.client.cluster.health()
            status = health.get("status", "unknown")
            
            if status == "green":
                logger.info("üíö Cluster Elasticsearch: Sant√© EXCELLENTE")
            elif status == "yellow":
                logger.warning("üíõ Cluster Elasticsearch: Sant√© ACCEPTABLE (r√©plicas manquants)")
            elif status == "red":
                logger.error("üíî Cluster Elasticsearch: Sant√© CRITIQUE")
            else:
                logger.warning(f"‚ö†Ô∏è Cluster Elasticsearch: Statut inconnu ({status})")
            
            logger.info(f"üìä N≈ìuds: {health.get('number_of_nodes', 0)}")
            logger.info(f"üóÇÔ∏è Shards actifs: {health.get('active_shards', 0)}")
            
        except Exception as e:
            logger.error(f"‚ùå Impossible de v√©rifier la sant√© du cluster: {e}")
    
    async def _setup_index(self):
        """Configure l'index pour les transactions."""
        try:
            # V√©rifier si l'index existe
            exists = await self.client.indices.exists(index=self.index_name)
            
            if not exists:
                logger.info(f"üìö Cr√©ation de l'index '{self.index_name}'...")
                
                # Mapping pour les transactions financi√®res
                mapping = {
                    "mappings": {
                        "properties": {
                            "user_id": {"type": "integer"},
                            "transaction_id": {"type": "keyword"},
                            "amount": {"type": "float"},
                            "description": {
                                "type": "text",
                                "analyzer": "standard",
                                "fields": {
                                    "keyword": {"type": "keyword"}
                                }
                            },
                            "merchant": {
                                "type": "text",
                                "analyzer": "standard",
                                "fields": {
                                    "keyword": {"type": "keyword"}
                                }
                            },
                            "category": {"type": "keyword"},
                            "date": {"type": "date"},
                            "created_at": {"type": "date"},
                            "updated_at": {"type": "date"}
                        }
                    },
                    "settings": {
                        "number_of_shards": 1,
                        "number_of_replicas": 0
                    }
                }
                
                await self.client.indices.create(index=self.index_name, body=mapping)
                logger.info(f"‚úÖ Index '{self.index_name}' cr√©√© avec succ√®s")
            else:
                logger.info(f"üìö Index '{self.index_name}' existe d√©j√†")
                
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la configuration de l'index: {e}")
    
    def _mask_credentials(self, url: str) -> str:
        """Masque les credentials dans l'URL pour l'affichage."""
        if "@" in url:
            # Format: https://user:pass@host:port/path
            parts = url.split("@")
            if len(parts) == 2:
                protocol_and_creds = parts[0]
                host_and_path = parts[1]
                
                if "://" in protocol_and_creds:
                    protocol = protocol_and_creds.split("://")[0]
                    return f"{protocol}://***:***@{host_and_path}"
        
        return url
    
    def _handle_connection_error(self, error):
        """G√®re les erreurs de connexion avec logging d√©taill√©."""
        logger.error("üîå Diagnostic de l'erreur de connexion:")
        logger.error(f"   - Type: {type(error).__name__}")
        logger.error(f"   - Message: {str(error)}")
        
        # Suggestions de diagnostic
        logger.error("üîß Actions de diagnostic sugg√©r√©es:")
        logger.error("   - V√©rifier la connectivit√© r√©seau")
        logger.error("   - Valider les credentials Elasticsearch")
        logger.error("   - Contr√¥ler les variables d'environnement")
        logger.error("   - Tester la connection depuis un autre client")
    
    async def is_healthy(self) -> bool:
        """V√©rifie si le client est sain et fonctionnel."""
        if not self.client or not self._initialized:
            return False
        
        try:
            # Test rapide de ping
            start_time = time.time()
            health = await self.client.cluster.health()
            response_time = time.time() - start_time
            
            status = health.get("status", "red")
            is_healthy = status in ["green", "yellow"]
            
            # Mettre √† jour le cache de sant√©
            self._last_health_check = {
                "timestamp": time.time(),
                "healthy": is_healthy,
                "status": status,
                "response_time": response_time
            }
            
            return is_healthy
            
        except Exception as e:
            logger.error(f"‚ùå Health check failed: {e}")
            return False
    
    async def search(
        self,
        user_id: int,
        query: str,
        limit: int = 10,
        filters: Dict[str, Any] = None,
        include_highlights: bool = True
    ) -> List[Dict[str, Any]]:
        """Recherche des transactions avec logging des performances."""
        if not self.client or not self._initialized:
            raise RuntimeError("Client Elasticsearch non initialis√©")
        
        search_id = f"search_{int(time.time() * 1000)}"
        logger.info(f"üîç [{search_id}] Recherche pour user {user_id}: '{query}' (limit: {limit})")
        
        start_time = time.time()
        
        try:
            # Construction de la requ√™te de recherche
            search_body = {
                "query": {
                    "bool": {
                        "must": [
                            {"term": {"user_id": user_id}}
                        ],
                        "should": [
                            {
                                "multi_match": {
                                    "query": query,
                                    "fields": ["description^2", "merchant", "category"],
                                    "type": "best_fields",
                                    "fuzziness": "AUTO"
                                }
                            },
                            {
                                "wildcard": {
                                    "description": f"*{query.lower()}*"
                                }
                            }
                        ],
                        "minimum_should_match": 1
                    }
                },
                "size": limit,
                "sort": [
                    {"_score": {"order": "desc"}},
                    {"date": {"order": "desc"}}
                ]
            }
            
            # Ajouter les filtres si sp√©cifi√©s
            if filters:
                for field, value in filters.items():
                    if value is not None:
                        search_body["query"]["bool"]["filter"] = search_body["query"]["bool"].get("filter", [])
                        search_body["query"]["bool"]["filter"].append({"term": {field: value}})
            
            # Ajouter la mise en √©vidence si demand√©e
            if include_highlights:
                search_body["highlight"] = {
                    "fields": {
                        "description": {},
                        "merchant": {}
                    },
                    "pre_tags": ["<mark>"],
                    "post_tags": ["</mark>"]
                }
            
            # Ex√©cuter la recherche
            logger.info(f"üéØ [{search_id}] Ex√©cution recherche lexicale...")
            response = await self.client.search(index=self.index_name, body=search_body)
            
            query_time = time.time() - start_time
            
            # Analyser les r√©sultats
            hits = response.get("hits", {}).get("hits", [])
            total_hits = response.get("hits", {}).get("total", {})
            
            if isinstance(total_hits, dict):
                total_count = total_hits.get("value", 0)
            else:
                total_count = total_hits
            
            # Formater les r√©sultats
            results = []
            scores = []
            
            for hit in hits:
                result = {
                    "id": hit["_id"],
                    "score": hit["_score"],
                    "source": hit["_source"]
                }
                
                # Ajouter les highlights si disponibles
                if "highlight" in hit:
                    result["highlights"] = hit["highlight"]
                
                results.append(result)
                scores.append(hit["_score"])
            
            # Statistiques des scores
            if scores:
                max_score = max(scores)
                min_score = min(scores)
                avg_score = sum(scores) / len(scores)
            else:
                max_score = min_score = avg_score = 0
            
            # Logs de r√©sultats
            logger.info(f"‚úÖ [{search_id}] Recherche termin√©e en {query_time:.3f}s")
            logger.info(f"üìä [{search_id}] R√©sultats: {len(results)}/{total_count}")
            logger.info(f"üéØ [{search_id}] Scores: max={max_score:.3f}, min={min_score:.3f}, avg={avg_score:.3f}")
            
            # M√©triques
            metrics_logger.info(
                f"elasticsearch.search.success,"
                f"user_id={user_id},"
                f"query_time={query_time:.3f},"
                f"results={len(results)},"
                f"total={total_count},"
                f"max_score={max_score:.3f}"
            )
            
            return results
            
        except Exception as e:
            query_time = time.time() - start_time
            logger.error(f"‚ùå [{search_id}] Erreur recherche apr√®s {query_time:.3f}s: {e}")
            metrics_logger.error(f"elasticsearch.search.failed,user_id={user_id},time={query_time:.3f},error={type(e).__name__}")
            return []
    
    async def close(self):
        """Ferme la connexion avec logging."""
        if self.client:
            logger.info("üîí Fermeture connexion Elasticsearch...")
            await self.client.close()
            self._initialized = False
            logger.info("‚úÖ Connexion Elasticsearch ferm√©e")
        else:
            logger.debug("üîí Aucun client √† fermer")