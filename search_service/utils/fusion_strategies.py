"""
Strat√©gies de fusion pour la recherche hybride - VERSION CENTRALIS√âE.

Ce module contient toutes les impl√©mentations des strat√©gies
de fusion des r√©sultats lexicaux et s√©mantiques.

AM√âLIORATION:
- Configuration enti√®rement centralis√©e via config_service
- Plus de duplication de param√®tres
- Contr√¥le total via .env
"""
import math
from typing import Dict, Any, List, Optional
from enum import Enum

# ‚úÖ CONFIGURATION CENTRALIS√âE
from config_service.config import settings
from search_service.models.responses import SearchResultItem


class FusionStrategy(Enum):
    """Strat√©gies de fusion des r√©sultats."""
    WEIGHTED_AVERAGE = "weighted_average"
    RANK_FUSION = "rank_fusion"
    RECIPROCAL_RANK_FUSION = "reciprocal_rank_fusion"
    SCORE_NORMALIZATION = "score_normalization"
    ADAPTIVE_FUSION = "adaptive_fusion"
    BORDA_COUNT = "borda_count"
    COMBSUM = "combsum"
    COMBMNZ = "combmnz"


class ScoreNormalizer:
    """
    Normaliseur de scores pour diff√©rents moteurs.
    
    Classe simplifi√©e avec m√©thodes statiques robustes.
    """
    
    @staticmethod
    def normalize_lexical_score(score: float) -> float:
        """
        Normalise un score Elasticsearch (typiquement 1-20) vers 0-1.
        
        Args:
            score: Score brut d'Elasticsearch
            
        Returns:
            Score normalis√© entre 0 et 1
        """
        if score is None or score <= 0:
            return 0.0
        # Elasticsearch donne typiquement des scores entre 1 et 15
        return min(score / 15.0, 1.0)
    
    @staticmethod
    def normalize_semantic_score(score: float) -> float:
        """
        Normalise un score s√©mantique (g√©n√©ralement d√©j√† entre 0-1).
        
        Args:
            score: Score de similarit√© s√©mantique
            
        Returns:
            Score normalis√© entre 0 et 1
        """
        if score is None or score < 0:
            return 0.0
        return min(score, 1.0)
    
    @staticmethod
    def min_max_normalize(scores: List[float]) -> List[float]:
        """
        Normalisation Min-Max robuste.
        
        Args:
            scores: Liste des scores √† normaliser
            
        Returns:
            Scores normalis√©s entre 0 et 1
        """
        if not scores:
            return []
        
        if len(scores) == 1:
            return [1.0]
        
        valid_scores = [s for s in scores if s is not None and not math.isnan(s)]
        if not valid_scores:
            return [0.0] * len(scores)
        
        min_score = min(valid_scores)
        max_score = max(valid_scores)
        
        if max_score == min_score:
            return [0.5] * len(scores)
        
        normalized = []
        for score in scores:
            if score is None or math.isnan(score):
                normalized.append(0.0)
            else:
                normalized.append((score - min_score) / (max_score - min_score))
        
        return normalized
    
    @staticmethod
    def z_score_normalize(scores: List[float]) -> List[float]:
        """
        Normalisation Z-Score avec conversion sigmoid.
        
        Args:
            scores: Liste des scores √† normaliser
            
        Returns:
            Scores normalis√©s entre 0 et 1
        """
        if not scores:
            return []
        
        if len(scores) == 1:
            return [0.5]
        
        valid_scores = [s for s in scores if s is not None and not math.isnan(s)]
        if not valid_scores:
            return [0.0] * len(scores)
        
        mean_score = sum(valid_scores) / len(valid_scores)
        variance = sum((s - mean_score) ** 2 for s in valid_scores) / len(valid_scores)
        std_dev = math.sqrt(variance)
        
        if std_dev == 0:
            return [0.5] * len(scores)
        
        normalized = []
        for score in scores:
            if score is None or math.isnan(score):
                normalized.append(0.0)
            else:
                z_score = (score - mean_score) / std_dev
                # Convertir en probabilit√© avec sigmoid
                sigmoid_score = 1 / (1 + math.exp(-z_score))
                normalized.append(sigmoid_score)
        
        return normalized
    
    @staticmethod
    def sigmoid_normalize(scores: List[float]) -> List[float]:
        """
        Normalisation Sigmoid directe.
        
        Args:
            scores: Liste des scores √† normaliser
            
        Returns:
            Scores normalis√©s entre 0 et 1
        """
        if not scores:
            return []
        
        valid_scores = [s for s in scores if s is not None and not math.isnan(s)]
        if not valid_scores:
            return [0.0] * len(scores)
        
        mean_score = sum(valid_scores) / len(valid_scores)
        
        normalized = []
        for score in scores:
            if score is None or math.isnan(score):
                normalized.append(0.0)
            else:
                sigmoid_score = 1 / (1 + math.exp(-(score - mean_score)))
                normalized.append(sigmoid_score)
        
        return normalized


class FusionStrategyExecutor:
    """
    Ex√©cuteur des strat√©gies de fusion.
    
    Version simplifi√©e utilisant la configuration centralis√©e.
    """
    
    def __init__(self):
        """Initialise l'ex√©cuteur avec la configuration centralis√©e."""
        self.normalizer = ScoreNormalizer()
    
    def execute(
        self,
        strategy: FusionStrategy,
        lexical_results: List[SearchResultItem],
        semantic_results: List[SearchResultItem],
        weights: Optional[Dict[str, float]] = None,
        debug: bool = False
    ) -> List[SearchResultItem]:
        """
        Ex√©cute la strat√©gie de fusion sp√©cifi√©e.
        
        Args:
            strategy: Strat√©gie de fusion √† utiliser
            lexical_results: R√©sultats de la recherche lexicale
            semantic_results: R√©sultats de la recherche s√©mantique
            weights: Poids personnalis√©s (utilise config centralis√©e si None)
            debug: Mode debug pour informations suppl√©mentaires
            
        Returns:
            Liste des r√©sultats fusionn√©s et tri√©s
        """
        # ‚úÖ Utiliser les poids de la configuration centralis√©e
        fusion_weights = weights or {
            "lexical_weight": settings.DEFAULT_LEXICAL_WEIGHT,
            "semantic_weight": settings.DEFAULT_SEMANTIC_WEIGHT
        }
        
        try:
            # Indexer les r√©sultats par transaction_id pour faciliter la fusion
            lexical_by_id = {r.transaction_id: r for r in lexical_results if r.transaction_id}
            semantic_by_id = {r.transaction_id: r for r in semantic_results if r.transaction_id}
            
            # Ex√©cuter la strat√©gie appropri√©e
            if strategy == FusionStrategy.WEIGHTED_AVERAGE:
                return self._fuse_weighted_average(lexical_by_id, semantic_by_id, fusion_weights)
            
            elif strategy == FusionStrategy.RANK_FUSION:
                return self._fuse_rank_fusion(lexical_results, semantic_results, fusion_weights)
            
            elif strategy == FusionStrategy.RECIPROCAL_RANK_FUSION:
                return self._fuse_reciprocal_rank_fusion(lexical_results, semantic_results, fusion_weights)
            
            elif strategy == FusionStrategy.SCORE_NORMALIZATION:
                return self._fuse_score_normalization(lexical_by_id, semantic_by_id, fusion_weights)
            
            elif strategy == FusionStrategy.ADAPTIVE_FUSION:
                return self._fuse_adaptive(lexical_by_id, semantic_by_id, fusion_weights)
            
            elif strategy == FusionStrategy.COMBSUM:
                return self._fuse_combsum(lexical_by_id, semantic_by_id, fusion_weights)
            
            else:
                # Fallback vers weighted average pour toutes les autres strat√©gies
                return self._fuse_weighted_average(lexical_by_id, semantic_by_id, fusion_weights)
                
        except Exception as e:
            # En cas d'erreur, retourner une fusion simple
            return self._simple_fallback_fusion(lexical_results, semantic_results, fusion_weights)
    
    def _create_fused_item(
        self,
        base_item: SearchResultItem,
        lexical_item: Optional[SearchResultItem],
        semantic_item: Optional[SearchResultItem],
        final_score: float,
        fusion_method: str
    ) -> SearchResultItem:
        """
        Cr√©e un item fusionn√© √† partir des r√©sultats sources.
        
        Args:
            base_item: Item de base (avec le plus d'informations)
            lexical_item: Item lexical (peut √™tre None)
            semantic_item: Item s√©mantique (peut √™tre None)
            final_score: Score final calcul√©
            fusion_method: M√©thode de fusion utilis√©e
            
        Returns:
            Nouvel item avec le score fusionn√©
        """
        # Cr√©er une copie de l'item de base
        fused_item = SearchResultItem(
            transaction_id=base_item.transaction_id,
            description=base_item.description,
            amount=base_item.amount,
            date=base_item.date,
            merchant=base_item.merchant,
            category=base_item.category,
            score=final_score,
            highlights=base_item.highlights or [],
            metadata=base_item.metadata or {}
        )
        
        # Ajouter des m√©tadonn√©es de fusion
        fused_item.metadata = fused_item.metadata.copy()
        fused_item.metadata.update({
            "fusion_method": fusion_method,
            "has_lexical": lexical_item is not None,
            "has_semantic": semantic_item is not None,
            "lexical_score": lexical_item.score if lexical_item else None,
            "semantic_score": semantic_item.score if semantic_item else None,
            "config_source": "centralized"
        })
        
        return fused_item
    
    def _fuse_weighted_average(
        self,
        lexical_by_id: Dict[int, SearchResultItem],
        semantic_by_id: Dict[int, SearchResultItem],
        weights: Dict[str, float]
    ) -> List[SearchResultItem]:
        """Fusion par moyenne pond√©r√©e des scores."""
        fused_results = []
        all_transaction_ids = set(lexical_by_id.keys()) | set(semantic_by_id.keys())
        
        lexical_weight = weights.get("lexical_weight", settings.DEFAULT_LEXICAL_WEIGHT)
        semantic_weight = weights.get("semantic_weight", settings.DEFAULT_SEMANTIC_WEIGHT)
        
        for transaction_id in all_transaction_ids:
            lexical_item = lexical_by_id.get(transaction_id)
            semantic_item = semantic_by_id.get(transaction_id)
            
            # Utiliser l'item avec le plus d'informations comme base
            base_item = lexical_item or semantic_item
            
            # Normaliser les scores
            lexical_score = self.normalizer.normalize_lexical_score(
                lexical_item.score if lexical_item else 0.0
            )
            semantic_score = self.normalizer.normalize_semantic_score(
                semantic_item.score if semantic_item else 0.0
            )
            
            # Calculer le score combin√©
            combined_score = lexical_score * lexical_weight + semantic_score * semantic_weight
            
            # Cr√©er l'item fusionn√©
            fused_item = self._create_fused_item(
                base_item, lexical_item, semantic_item, combined_score, "weighted_average"
            )
            
            fused_results.append(fused_item)
        
        # Trier par score d√©croissant et limiter selon la config
        fused_results.sort(key=lambda x: x.score, reverse=True)
        return fused_results[:settings.MAX_SEARCH_LIMIT]
    
    def _fuse_rank_fusion(
        self,
        lexical_results: List[SearchResultItem],
        semantic_results: List[SearchResultItem],
        weights: Dict[str, float]
    ) -> List[SearchResultItem]:
        """Fusion bas√©e sur les rangs dans chaque liste."""
        lexical_weight = weights.get("lexical_weight", settings.DEFAULT_LEXICAL_WEIGHT)
        semantic_weight = weights.get("semantic_weight", settings.DEFAULT_SEMANTIC_WEIGHT)
        
        # Cr√©er des mappings rang -> transaction_id
        lexical_ranks = {item.transaction_id: i + 1 for i, item in enumerate(lexical_results)}
        semantic_ranks = {item.transaction_id: i + 1 for i, item in enumerate(semantic_results)}
        
        # Indexer par transaction_id
        lexical_by_id = {r.transaction_id: r for r in lexical_results}
        semantic_by_id = {r.transaction_id: r for r in semantic_results}
        
        all_transaction_ids = set(lexical_ranks.keys()) | set(semantic_ranks.keys())
        fused_results = []
        
        for transaction_id in all_transaction_ids:
            lexical_item = lexical_by_id.get(transaction_id)
            semantic_item = semantic_by_id.get(transaction_id)
            base_item = lexical_item or semantic_item
            
            # Calculer le score bas√© sur les rangs (plus bas = meilleur)
            lexical_rank = lexical_ranks.get(transaction_id, len(lexical_results) + 1)
            semantic_rank = semantic_ranks.get(transaction_id, len(semantic_results) + 1)
            
            # Convertir les rangs en scores (1/rang)
            lexical_rank_score = 1.0 / lexical_rank if lexical_rank > 0 else 0.0
            semantic_rank_score = 1.0 / semantic_rank if semantic_rank > 0 else 0.0
            
            combined_rank_score = (
                lexical_rank_score * lexical_weight + 
                semantic_rank_score * semantic_weight
            )
            
            fused_item = self._create_fused_item(
                base_item, lexical_item, semantic_item, combined_rank_score, "rank_fusion"
            )
            
            fused_results.append(fused_item)
        
        # Trier par score d√©croissant
        fused_results.sort(key=lambda x: x.score, reverse=True)
        return fused_results[:settings.MAX_SEARCH_LIMIT]
    
    def _fuse_reciprocal_rank_fusion(
        self,
        lexical_results: List[SearchResultItem],
        semantic_results: List[SearchResultItem],
        weights: Dict[str, float]
    ) -> List[SearchResultItem]:
        """Fusion RRF (Reciprocal Rank Fusion)."""
        # ‚úÖ Utiliser la configuration centralis√©e
        k = settings.RRF_K
        
        # Cr√©er des mappings rang -> transaction_id
        lexical_ranks = {item.transaction_id: i + 1 for i, item in enumerate(lexical_results)}
        semantic_ranks = {item.transaction_id: i + 1 for i, item in enumerate(semantic_results)}
        
        # Indexer par transaction_id
        lexical_by_id = {r.transaction_id: r for r in lexical_results}
        semantic_by_id = {r.transaction_id: r for r in semantic_results}
        
        all_transaction_ids = set(lexical_ranks.keys()) | set(semantic_ranks.keys())
        fused_results = []
        
        for transaction_id in all_transaction_ids:
            lexical_item = lexical_by_id.get(transaction_id)
            semantic_item = semantic_by_id.get(transaction_id)
            base_item = lexical_item or semantic_item
            
            # Calculer RRF score
            rrf_score = 0.0
            if lexical_item:
                lexical_rank = lexical_ranks[transaction_id]
                rrf_score += 1.0 / (k + lexical_rank)
            if semantic_item:
                semantic_rank = semantic_ranks[transaction_id]
                rrf_score += 1.0 / (k + semantic_rank)
            
            fused_item = self._create_fused_item(
                base_item, lexical_item, semantic_item, rrf_score, "reciprocal_rank_fusion"
            )
            
            fused_results.append(fused_item)
        
        # Trier par score d√©croissant
        fused_results.sort(key=lambda x: x.score, reverse=True)
        return fused_results[:settings.MAX_SEARCH_LIMIT]
    
    def _fuse_score_normalization(
        self,
        lexical_by_id: Dict[int, SearchResultItem],
        semantic_by_id: Dict[int, SearchResultItem],
        weights: Dict[str, float]
    ) -> List[SearchResultItem]:
        """Fusion avec normalisation avanc√©e des scores."""
        # Extraire tous les scores pour normalisation
        lexical_scores = [item.score for item in lexical_by_id.values() if item.score is not None]
        semantic_scores = [item.score for item in semantic_by_id.values() if item.score is not None]
        
        # ‚úÖ Utiliser la m√©thode de normalisation de la config centralis√©e
        normalization_method = settings.SCORE_NORMALIZATION_METHOD
        
        if normalization_method == "z_score":
            lexical_norm = self.normalizer.z_score_normalize(lexical_scores)
            semantic_norm = self.normalizer.z_score_normalize(semantic_scores)
        elif normalization_method == "sigmoid":
            lexical_norm = self.normalizer.sigmoid_normalize(lexical_scores)
            semantic_norm = self.normalizer.sigmoid_normalize(semantic_scores)
        else:  # min_max par d√©faut
            lexical_norm = self.normalizer.min_max_normalize(lexical_scores)
            semantic_norm = self.normalizer.min_max_normalize(semantic_scores)
        
        # Cr√©er des mappings score -> score normalis√©
        lexical_score_map = dict(zip(lexical_scores, lexical_norm)) if lexical_scores else {}
        semantic_score_map = dict(zip(semantic_scores, semantic_norm)) if semantic_scores else {}
        
        all_transaction_ids = set(lexical_by_id.keys()) | set(semantic_by_id.keys())
        fused_results = []
        
        lexical_weight = weights.get("lexical_weight", settings.DEFAULT_LEXICAL_WEIGHT)
        semantic_weight = weights.get("semantic_weight", settings.DEFAULT_SEMANTIC_WEIGHT)
        
        for transaction_id in all_transaction_ids:
            lexical_item = lexical_by_id.get(transaction_id)
            semantic_item = semantic_by_id.get(transaction_id)
            base_item = lexical_item or semantic_item
            
            # Obtenir les scores normalis√©s
            norm_lexical = lexical_score_map.get(lexical_item.score, 0.0) if lexical_item else 0.0
            norm_semantic = semantic_score_map.get(semantic_item.score, 0.0) if semantic_item else 0.0
            
            # Calculer le score final
            combined_score = norm_lexical * lexical_weight + norm_semantic * semantic_weight
            
            fused_item = self._create_fused_item(
                base_item, lexical_item, semantic_item, combined_score, "score_normalization"
            )
            
            fused_results.append(fused_item)
        
        # Trier par score d√©croissant
        fused_results.sort(key=lambda x: x.score, reverse=True)
        return fused_results[:settings.MAX_SEARCH_LIMIT]
    
    def _fuse_adaptive(
        self,
        lexical_by_id: Dict[int, SearchResultItem],
        semantic_by_id: Dict[int, SearchResultItem],
        weights: Dict[str, float]
    ) -> List[SearchResultItem]:
        """Fusion adaptative qui choisit la meilleure m√©thode par transaction."""
        fused_results = []
        all_transaction_ids = set(lexical_by_id.keys()) | set(semantic_by_id.keys())
        
        lexical_weight = weights.get("lexical_weight", settings.DEFAULT_LEXICAL_WEIGHT)
        semantic_weight = weights.get("semantic_weight", settings.DEFAULT_SEMANTIC_WEIGHT)
        
        # ‚úÖ Utiliser les seuils de la configuration centralis√©e
        adaptive_threshold = settings.ADAPTIVE_THRESHOLD
        quality_boost_factor = settings.QUALITY_BOOST_FACTOR
        
        for transaction_id in all_transaction_ids:
            lexical_item = lexical_by_id.get(transaction_id)
            semantic_item = semantic_by_id.get(transaction_id)
            base_item = lexical_item or semantic_item
            
            # D√©terminer la m√©thode optimale pour cette transaction
            if lexical_item and semantic_item:
                # Les deux moteurs ont trouv√© cette transaction
                norm_lexical = self.normalizer.normalize_lexical_score(lexical_item.score)
                norm_semantic = self.normalizer.normalize_semantic_score(semantic_item.score)
                
                score_diff = abs(norm_lexical - norm_semantic)
                
                if score_diff < adaptive_threshold:
                    # Scores similaires -> moyenne pond√©r√©e
                    combined_score = norm_lexical * lexical_weight + norm_semantic * semantic_weight
                else:
                    # Scores diff√©rents -> favoriser le meilleur
                    if norm_lexical > norm_semantic:
                        combined_score = norm_lexical * (1 + quality_boost_factor)
                    else:
                        combined_score = norm_semantic * (1 + quality_boost_factor)
            
            elif lexical_item:
                # Seulement lexical
                combined_score = self.normalizer.normalize_lexical_score(lexical_item.score)
            
            else:
                # Seulement s√©mantique
                combined_score = self.normalizer.normalize_semantic_score(semantic_item.score)
            
            # S'assurer que le score reste dans [0, 1]
            combined_score = min(combined_score, 1.0)
            
            fused_item = self._create_fused_item(
                base_item, lexical_item, semantic_item, combined_score, "adaptive_fusion"
            )
            
            fused_results.append(fused_item)
        
        # Trier par score d√©croissant
        fused_results.sort(key=lambda x: x.score, reverse=True)
        return fused_results[:settings.MAX_SEARCH_LIMIT]
    
    def _fuse_combsum(
        self,
        lexical_by_id: Dict[int, SearchResultItem],
        semantic_by_id: Dict[int, SearchResultItem],
        weights: Dict[str, float]
    ) -> List[SearchResultItem]:
        """Fusion CombSUM (somme des scores normalis√©s)."""
        # CombSUM = somme simple sans pond√©ration
        return self._fuse_weighted_average(lexical_by_id, semantic_by_id, {
            "lexical_weight": 0.5, "semantic_weight": 0.5
        })
    
    def _simple_fallback_fusion(
        self,
        lexical_results: List[SearchResultItem],
        semantic_results: List[SearchResultItem],
        weights: Dict[str, float]
    ) -> List[SearchResultItem]:
        """
        Fusion de fallback ultra-simple en cas d'erreur.
        
        Combine simplement les deux listes et trie par score.
        """
        all_results = []
        
        # Ajouter les r√©sultats lexicaux avec pond√©ration
        lexical_weight = weights.get("lexical_weight", settings.DEFAULT_LEXICAL_WEIGHT)
        for item in lexical_results[:50]:  # Limiter pour √©viter les surcharges
            weighted_item = SearchResultItem(
                transaction_id=item.transaction_id,
                description=item.description,
                amount=item.amount,
                date=item.date,
                merchant=item.merchant,
                category=item.category,
                score=item.score * lexical_weight if item.score else 0.0,
                highlights=item.highlights,
                metadata=item.metadata
            )
            all_results.append(weighted_item)
        
        # Ajouter les r√©sultats s√©mantiques avec pond√©ration
        semantic_weight = weights.get("semantic_weight", settings.DEFAULT_SEMANTIC_WEIGHT)
        semantic_ids = {item.transaction_id for item in all_results}
        
        for item in semantic_results[:50]:  # Limiter pour √©viter les surcharges
            if item.transaction_id not in semantic_ids:  # √âviter les doublons
                weighted_item = SearchResultItem(
                    transaction_id=item.transaction_id,
                    description=item.description,
                    amount=item.amount,
                    date=item.date,
                    merchant=item.merchant,
                    category=item.category,
                    score=item.score * semantic_weight if item.score else 0.0,
                    highlights=item.highlights,
                    metadata=item.metadata
                )
                all_results.append(weighted_item)
        
        # Trier par score et limiter selon la configuration centralis√©e
        all_results.sort(key=lambda x: x.score or 0.0, reverse=True)
        return all_results[:settings.MAX_SEARCH_LIMIT]


# ==========================================
# üîß FONCTIONS UTILITAIRES SIMPLIFI√âES
# ==========================================

def create_simple_executor() -> FusionStrategyExecutor:
    """Cr√©e un ex√©cuteur simple utilisant la configuration centralis√©e."""
    return FusionStrategyExecutor()


def create_default_executor() -> FusionStrategyExecutor:
    """Cr√©e un ex√©cuteur par d√©faut avec configuration centralis√©e."""
    return FusionStrategyExecutor()


# ==========================================
# üéØ CLASSE DE CONFIGURATION SIMPLIFI√âE
# ==========================================

class FusionConfig:
    """
    Configuration de fusion utilisant les param√®tres centralis√©s.
    
    Cette classe sert de facade pour acc√©der √† la configuration
    centralis√©e de mani√®re compatible avec l'ancien code.
    """
    
    @property
    def default_strategy(self) -> FusionStrategy:
        """Strat√©gie par d√©faut depuis la config centralis√©e."""
        strategy_mapping = {
            "weighted_average": FusionStrategy.WEIGHTED_AVERAGE,
            "rank_fusion": FusionStrategy.RANK_FUSION,
            "reciprocal_rank_fusion": FusionStrategy.RECIPROCAL_RANK_FUSION,
            "score_normalization": FusionStrategy.SCORE_NORMALIZATION,
            "adaptive_fusion": FusionStrategy.ADAPTIVE_FUSION,
            "combsum": FusionStrategy.COMBSUM,
            "combmnz": FusionStrategy.COMBMNZ
        }
        return strategy_mapping.get(settings.DEFAULT_SEARCH_TYPE, FusionStrategy.ADAPTIVE_FUSION)
    
    @property
    def default_lexical_weight(self) -> float:
        """Poids lexical par d√©faut."""
        return settings.DEFAULT_LEXICAL_WEIGHT
    
    @property
    def default_semantic_weight(self) -> float:
        """Poids s√©mantique par d√©faut."""
        return settings.DEFAULT_SEMANTIC_WEIGHT
    
    @property
    def score_normalization_method(self) -> str:
        """M√©thode de normalisation des scores."""
        return settings.SCORE_NORMALIZATION_METHOD
    
    @property
    def min_score_threshold(self) -> float:
        """Seuil minimum de score."""
        return settings.MIN_SCORE_THRESHOLD
    
    @property
    def rrf_k(self) -> int:
        """Param√®tre K pour RRF."""
        return settings.RRF_K
    
    @property
    def adaptive_threshold(self) -> float:
        """Seuil pour la fusion adaptative."""
        return settings.ADAPTIVE_THRESHOLD
    
    @property
    def quality_boost_factor(self) -> float:
        """Facteur de boost de qualit√©."""
        return settings.QUALITY_BOOST_FACTOR
    
    @property
    def enable_deduplication(self) -> bool:
        """D√©duplication activ√©e."""
        return settings.ENABLE_DEDUPLICATION
    
    @property
    def dedup_similarity_threshold(self) -> float:
        """Seuil de similarit√© pour d√©duplication."""
        return settings.DEDUP_SIMILARITY_THRESHOLD
    
    @property
    def enable_diversification(self) -> bool:
        """Diversification activ√©e."""
        return settings.ENABLE_DIVERSIFICATION
    
    @property
    def diversity_factor(self) -> float:
        """Facteur de diversit√©."""
        return settings.DIVERSITY_FACTOR
    
    @property
    def max_same_merchant(self) -> int:
        """Maximum de r√©sultats par marchand."""
        return settings.MAX_SAME_MERCHANT
    
    @property
    def max_results(self) -> int:
        """Nombre maximum de r√©sultats."""
        return settings.MAX_SEARCH_LIMIT