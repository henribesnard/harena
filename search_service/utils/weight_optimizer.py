"""
Optimiseur de poids pour la fusion hybride - VERSION CENTRALIS√âE.

Ce module d√©termine les poids optimaux pour combiner
les r√©sultats lexicaux et s√©mantiques selon le contexte.

AM√âLIORATION:
- Utilise la configuration centralis√©e pour les valeurs par d√©faut
- Plus de valeurs hardcod√©es
"""
from typing import Dict, Any, Optional
from search_service.core.lexical_engine import LexicalSearchResult
from search_service.core.semantic_engine import SemanticSearchResult
from search_service.core.query_processor import QueryAnalysis
from search_service.models.search_types import SearchQuality
from search_service.utils.fusion_strategies import FusionStrategy

# ‚úÖ CONFIGURATION CENTRALIS√âE
from config_service.config import settings


class WeightOptimizer:
    """Optimiseur de poids pour la fusion hybride."""
    
    def __init__(
        self,
        default_lexical_weight: Optional[float] = None,
        default_semantic_weight: Optional[float] = None
    ):
        """
        Initialise l'optimiseur avec la configuration centralis√©e.
        
        Args:
            default_lexical_weight: Poids lexical (utilise config si None)
            default_semantic_weight: Poids s√©mantique (utilise config si None)
        """
        # ‚úÖ Utiliser la configuration centralis√©e par d√©faut
        self.default_lexical_weight = default_lexical_weight or settings.DEFAULT_LEXICAL_WEIGHT
        self.default_semantic_weight = default_semantic_weight or settings.DEFAULT_SEMANTIC_WEIGHT
    
    def determine_optimal_weights(
        self,
        lexical_result: LexicalSearchResult,
        semantic_result: SemanticSearchResult,
        query_analysis: QueryAnalysis
    ) -> Dict[str, float]:
        """
        D√©termine les poids optimaux pour la fusion.
        
        Args:
            lexical_result: R√©sultats de recherche lexicale
            semantic_result: R√©sultats de recherche s√©mantique
            query_analysis: Analyse de la requ√™te
            
        Returns:
            Dictionnaire des poids optimaux
        """
        base_lexical = self.default_lexical_weight
        base_semantic = self.default_semantic_weight
        
        # Ajustements bas√©s sur la qualit√© des r√©sultats
        quality_diff = self._quality_to_score(lexical_result.quality) - \
                      self._quality_to_score(semantic_result.quality)
        
        # Ajuster les poids selon la diff√©rence de qualit√©
        weight_adjustment = quality_diff * 0.2
        
        lexical_weight = max(0.1, min(0.9, base_lexical + weight_adjustment))
        semantic_weight = 1.0 - lexical_weight
        
        # Ajustements sp√©cifiques selon le type de requ√™te
        lexical_weight, semantic_weight = self._apply_query_specific_adjustments(
            lexical_weight, semantic_weight, query_analysis
        )
        
        # Normaliser pour assurer que la somme = 1.0
        total = lexical_weight + semantic_weight
        lexical_weight /= total
        semantic_weight /= total
        
        return {
            "lexical_weight": lexical_weight,
            "semantic_weight": semantic_weight
        }
    
    def determine_optimal_strategy(
        self,
        lexical_result: LexicalSearchResult,
        semantic_result: SemanticSearchResult,
        query_analysis: QueryAnalysis
    ) -> FusionStrategy:
        """
        D√©termine la strat√©gie de fusion optimale.
        
        Args:
            lexical_result: R√©sultats de recherche lexicale
            semantic_result: R√©sultats de recherche s√©mantique
            query_analysis: Analyse de la requ√™te
            
        Returns:
            Strat√©gie de fusion recommand√©e
        """
        # Analyser la qualit√© des r√©sultats de chaque moteur
        lexical_quality = lexical_result.quality
        semantic_quality = semantic_result.quality
        
        # Si une recherche est de tr√®s mauvaise qualit√©
        if lexical_quality == SearchQuality.POOR and semantic_quality != SearchQuality.POOR:
            return FusionStrategy.WEIGHTED_AVERAGE  # Favoriser le s√©mantique
        
        if semantic_quality == SearchQuality.POOR and lexical_quality != SearchQuality.POOR:
            return FusionStrategy.WEIGHTED_AVERAGE  # Favoriser le lexical
        
        # Pour requ√™tes tr√®s sp√©cifiques (phrases exactes)
        if query_analysis.has_exact_phrases:
            return FusionStrategy.RANK_FUSION
        
        # Pour requ√™tes avec entit√©s financi√®res
        if query_analysis.has_financial_entities:
            return FusionStrategy.RECIPROCAL_RANK_FUSION
        
        # Pour requ√™tes courtes et simples
        if len(query_analysis.key_terms) <= 2:
            return FusionStrategy.SCORE_NORMALIZATION
        
        # Strat√©gie adaptative par d√©faut
        return FusionStrategy.ADAPTIVE_FUSION
    
    def _quality_to_score(self, quality: SearchQuality) -> float:
        """Convertit une qualit√© en score num√©rique."""
        # ‚úÖ Utiliser les seuils de qualit√© de la configuration centralis√©e
        quality_scores = {
            SearchQuality.EXCELLENT: settings.QUALITY_EXCELLENT_THRESHOLD,
            SearchQuality.GOOD: settings.QUALITY_GOOD_THRESHOLD,
            SearchQuality.MEDIUM: settings.QUALITY_MEDIUM_THRESHOLD,
            SearchQuality.POOR: settings.QUALITY_POOR_THRESHOLD
        }
        return quality_scores.get(quality, settings.QUALITY_MEDIUM_THRESHOLD)
    
    def _apply_query_specific_adjustments(
        self,
        lexical_weight: float,
        semantic_weight: float,
        query_analysis: QueryAnalysis
    ) -> tuple[float, float]:
        """
        Applique des ajustements sp√©cifiques au type de requ√™te.
        
        Args:
            lexical_weight: Poids lexical initial
            semantic_weight: Poids s√©mantique initial
            query_analysis: Analyse de la requ√™te
            
        Returns:
            Tuple des poids ajust√©s (lexical, semantic)
        """
        # Ajustements pour phrases exactes
        if query_analysis.has_exact_phrases:
            # Favoriser le lexical pour les phrases exactes
            lexical_weight += 0.1
            semantic_weight -= 0.1
        
        # Ajustements pour entit√©s financi√®res
        if query_analysis.has_financial_entities:
            # √âquilibrer pour les entit√©s financi√®res
            lexical_weight = 0.5
            semantic_weight = 0.5
        
        # Ajustements pour requ√™tes complexes
        if len(query_analysis.key_terms) > 5:
            # Favoriser le s√©mantique pour les requ√™tes complexes
            semantic_weight += 0.1
            lexical_weight -= 0.1
        
        # Ajustements pour requ√™tes avec montants
        if query_analysis.has_amounts:
            # Favoriser l√©g√®rement le lexical pour les montants
            lexical_weight += 0.05
            semantic_weight -= 0.05
        
        # Ajustements pour requ√™tes avec dates
        if query_analysis.has_dates:
            # Favoriser l√©g√®rement le lexical pour les dates
            lexical_weight += 0.05
            semantic_weight -= 0.05
        
        # S'assurer que les poids restent dans des limites raisonnables
        lexical_weight = max(0.1, min(0.9, lexical_weight))
        semantic_weight = max(0.1, min(0.9, semantic_weight))
        
        return lexical_weight, semantic_weight


class AdaptiveWeightManager:
    """Gestionnaire de poids adaptatifs avec historique."""
    
    def __init__(self):
        self.weight_history = []
        self.performance_history = []
        # ‚úÖ Param√®tres configurables pourraient √™tre ajout√©s √† settings plus tard
        self.learning_rate = 0.1
        self.min_history_for_adaptation = 2
        self.good_performance_threshold = 0.7
        self.poor_performance_threshold = 0.5
    
    def update_weights_from_feedback(
        self,
        current_weights: Dict[str, float],
        feedback_score: float,
        query_features: Dict[str, Any]
    ) -> Dict[str, float]:
        """
        Met √† jour les poids bas√©s sur le feedback de performance.
        
        Args:
            current_weights: Poids actuels utilis√©s
            feedback_score: Score de feedback (0-1, plus haut = meilleur)
            query_features: Caract√©ristiques de la requ√™te
            
        Returns:
            Poids mis √† jour
        """
        # Enregistrer l'historique
        self.weight_history.append(current_weights.copy())
        self.performance_history.append(feedback_score)
        
        # Si on n'a pas assez d'historique, retourner les poids actuels
        if len(self.weight_history) < self.min_history_for_adaptation:
            return current_weights
        
        # Calculer la tendance de performance
        recent_performance = self.performance_history[-5:]  # 5 derni√®res performances
        avg_performance = sum(recent_performance) / len(recent_performance)
        
        # Si la performance est bonne, garder les poids actuels
        if avg_performance >= self.good_performance_threshold:
            return current_weights
        
        # Sinon, ajuster graduellement
        adjusted_weights = current_weights.copy()
        
        # Logique simple : si performance faible, √©quilibrer plus
        if avg_performance < self.poor_performance_threshold:
            lexical_weight = adjusted_weights["lexical_weight"]
            semantic_weight = adjusted_weights["semantic_weight"]
            
            # ‚úÖ Rapprocher vers les valeurs par d√©faut de la configuration centralis√©e
            target_lexical = settings.DEFAULT_LEXICAL_WEIGHT
            target_semantic = settings.DEFAULT_SEMANTIC_WEIGHT
            
            adjusted_weights["lexical_weight"] = (
                lexical_weight + (target_lexical - lexical_weight) * self.learning_rate
            )
            adjusted_weights["semantic_weight"] = (
                semantic_weight + (target_semantic - semantic_weight) * self.learning_rate
            )
        
        return adjusted_weights
    
    def get_weight_recommendations(
        self,
        query_features: Dict[str, Any]
    ) -> Dict[str, float]:
        """
        Recommande des poids bas√©s sur l'historique et les caract√©ristiques de requ√™te.
        
        Args:
            query_features: Caract√©ristiques de la requ√™te
            
        Returns:
            Poids recommand√©s
        """
        # ‚úÖ Utiliser la configuration centralis√©e comme base
        base_weights = {
            "lexical_weight": settings.DEFAULT_LEXICAL_WEIGHT,
            "semantic_weight": settings.DEFAULT_SEMANTIC_WEIGHT
        }
        
        # Ajustements bas√©s sur les caract√©ristiques
        if query_features.get("has_exact_phrases", False):
            base_weights["lexical_weight"] += 0.1
            base_weights["semantic_weight"] -= 0.1
        
        if query_features.get("query_length", 0) > 5:
            base_weights["semantic_weight"] += 0.1
            base_weights["lexical_weight"] -= 0.1
        
        if query_features.get("has_financial_entities", False):
            # √âquilibrer pour les entit√©s financi√®res
            base_weights["lexical_weight"] = 0.5
            base_weights["semantic_weight"] = 0.5
        
        if query_features.get("has_amounts", False):
            # L√©g√®rement favoriser le lexical pour les montants
            base_weights["lexical_weight"] += 0.05
            base_weights["semantic_weight"] -= 0.05
        
        # Normaliser
        total = sum(base_weights.values())
        for key in base_weights:
            base_weights[key] /= total
        
        return base_weights
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """Retourne un r√©sum√© des performances."""
        if not self.performance_history:
            return {
                "total_feedback_count": 0,
                "average_performance": 0.0,
                "recent_performance": 0.0,
                "performance_trend": "unknown"
            }
        
        total_count = len(self.performance_history)
        avg_performance = sum(self.performance_history) / total_count
        
        # Performance r√©cente (5 derniers)
        recent_scores = self.performance_history[-5:]
        recent_avg = sum(recent_scores) / len(recent_scores)
        
        # Tendance
        if total_count >= 3:
            first_half = self.performance_history[:total_count//2]
            second_half = self.performance_history[total_count//2:]
            first_avg = sum(first_half) / len(first_half)
            second_avg = sum(second_half) / len(second_half)
            
            if second_avg > first_avg + 0.1:
                trend = "improving"
            elif second_avg < first_avg - 0.1:
                trend = "declining"
            else:
                trend = "stable"
        else:
            trend = "insufficient_data"
        
        return {
            "total_feedback_count": total_count,
            "average_performance": avg_performance,
            "recent_performance": recent_avg,
            "performance_trend": trend,
            "config_source": "centralized"
        }
    
    def reset_history(self) -> None:
        """Remet √† z√©ro l'historique."""
        self.weight_history.clear()
        self.performance_history.clear()


# ==========================================
# üîß FONCTIONS UTILITAIRES
# ==========================================

def create_weight_optimizer() -> WeightOptimizer:
    """Cr√©e un optimiseur de poids avec la configuration centralis√©e."""
    return WeightOptimizer()


def create_adaptive_weight_manager() -> AdaptiveWeightManager:
    """Cr√©e un gestionnaire de poids adaptatifs."""
    return AdaptiveWeightManager()


def get_default_weights() -> Dict[str, float]:
    """Retourne les poids par d√©faut de la configuration centralis√©e."""
    return {
        "lexical_weight": settings.DEFAULT_LEXICAL_WEIGHT,
        "semantic_weight": settings.DEFAULT_SEMANTIC_WEIGHT
    }


def validate_weights(weights: Dict[str, float]) -> Dict[str, float]:
    """
    Valide et normalise les poids.
    
    Args:
        weights: Dictionnaire des poids √† valider
        
    Returns:
        Poids valid√©s et normalis√©s
    """
    lexical_weight = weights.get("lexical_weight", settings.DEFAULT_LEXICAL_WEIGHT)
    semantic_weight = weights.get("semantic_weight", settings.DEFAULT_SEMANTIC_WEIGHT)
    
    # S'assurer que les poids sont positifs
    lexical_weight = max(0.0, lexical_weight)
    semantic_weight = max(0.0, semantic_weight)
    
    # Normaliser pour que la somme = 1.0
    total = lexical_weight + semantic_weight
    if total > 0:
        lexical_weight /= total
        semantic_weight /= total
    else:
        # Fallback vers les valeurs par d√©faut
        lexical_weight = settings.DEFAULT_LEXICAL_WEIGHT
        semantic_weight = settings.DEFAULT_SEMANTIC_WEIGHT
    
    return {
        "lexical_weight": lexical_weight,
        "semantic_weight": semantic_weight
    }


def get_weight_optimization_config() -> Dict[str, Any]:
    """Retourne la configuration d'optimisation des poids."""
    return {
        "default_weights": get_default_weights(),
        "quality_thresholds": {
            "excellent": settings.QUALITY_EXCELLENT_THRESHOLD,
            "good": settings.QUALITY_GOOD_THRESHOLD,
            "medium": settings.QUALITY_MEDIUM_THRESHOLD,
            "poor": settings.QUALITY_POOR_THRESHOLD
        },
        "adjustment_factors": {
            "quality_adjustment_factor": 0.2,
            "exact_phrase_boost": 0.1,
            "complex_query_semantic_boost": 0.1,
            "amount_lexical_boost": 0.05,
            "date_lexical_boost": 0.05
        },
        "constraints": {
            "min_weight": 0.1,
            "max_weight": 0.9
        },
        "config_source": "centralized"
    }