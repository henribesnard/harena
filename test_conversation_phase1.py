
"""
ğŸ’¡ FICHIER SÃ‰PARÃ‰: test_conversation_phase1.py
Script pour valider la Phase 1 L0 Pattern Matching
"""

import asyncio
import aiohttp
import json
import time

async def test_conversation_phase1_complete():
    """
    ğŸ§ª Test complet Phase 1 - L0 Pattern Matching
    
    VÃ©rifie:
    1. Service Phase 1 dÃ©marrÃ© correctement
    2. Pattern Matcher L0 fonctionnel
    3. Performance <10ms sur patterns simples
    4. Taux succÃ¨s >85% sur requÃªtes financiÃ¨res
    """
    
    print("ğŸ§ª Test Validation Phase 1 - L0 Pattern Matching")
    print("=" * 70)
    
    base_url = "http://localhost:8001"
    results = {"tests": [], "performance": [], "errors": [], "l0_metrics": {}}
    
    async with aiohttp.ClientSession() as session:
        
        # ===== TEST 1: Health Check Phase 1 =====
        print("\n1ï¸âƒ£ Test Health Check Phase 1...")
        try:
            async with session.get(f"{base_url}/health", timeout=5) as response:
                if response.status == 200:
                    health_data = await response.json()
                    print(f"   âœ… Health: {health_data['status']}")
                    print(f"   ğŸ”§ Phase: {health_data.get('phase', 'unknown')}")
                    print(f"   âš¡ Pattern Matcher: {health_data.get('pattern_matcher', {}).get('status', 'unknown')}")
                    
                    # VÃ©rification targets
                    targets = health_data.get('targets_status', {})
                    print(f"   ğŸ¯ Latence target: {targets.get('latency_met', False)}")
                    print(f"   ğŸ¯ Success target: {targets.get('success_met', False)}")
                    
                    results["tests"].append({"test": "health_phase1", "status": "pass"})
                else:
                    print(f"   âŒ Health failed: {response.status}")
                    results["tests"].append({"test": "health_phase1", "status": "fail", "code": response.status})
                    return results
        except Exception as e:
            print(f"   âŒ Health error: {e}")
            results["errors"].append({"test": "health_phase1", "error": str(e)})
            return results
        
        # ===== TEST 2: Patterns L0 Basiques =====
        print("\n2ï¸âƒ£ Test Patterns L0 Basiques...")
        
        l0_test_queries = [
            # Patterns haute confiance
            ("solde", "BALANCE_CHECK", True),
            ("virement", "TRANSFER", True),
            ("bloquer carte", "CARD_MANAGEMENT", True),
            ("dÃ©penses", "EXPENSE_ANALYSIS", True),
            ("bonjour", "GREETING", True),
            
            # Patterns avec entitÃ©s
            ("virer 100â‚¬", "TRANSFER", True),
            ("dÃ©penses restaurant", "EXPENSE_ANALYSIS", True),
            ("quel est mon solde", "BALANCE_CHECK", True),
            
            # Cas qui ne devraient pas matcher L0
            ("mÃ©tÃ©o aujourd'hui", None, False),
            ("recette de cuisine", None, False)
        ]
        
        l0_success = 0
        l0_latencies = []
        
        for i, (message, expected_intent, should_match) in enumerate(l0_test_queries, 1):
            try:
                start_time = time.time()
                
                payload = {"message": message, "user_id": 100 + i}
                async with session.post(
                    f"{base_url}/api/v1/chat",
                    json=payload,
                    timeout=1  # Timeout court pour L0
                ) as response:
                    
                    request_latency = (time.time() - start_time) * 1000
                    
                    if response.status == 200:
                        chat_data = await response.json()
                        
                        if should_match and chat_data["success"]:
                            # Devrait matcher et a matchÃ©
                            processing_latency = chat_data["processing_metadata"]["latency_ms"]
                            level_used = chat_data["processing_metadata"]["level_used"]
                            pattern_used = chat_data["processing_metadata"].get("pattern_matched", "unknown")
                            
                            print(f"   {i:2d}. âœ… '{message}' â†’ {chat_data['intent']} ({pattern_used})")
                            print(f"        â±ï¸ L0: {processing_latency:.1f}ms, Total: {request_latency:.1f}ms")
                            
                            l0_success += 1
                            l0_latencies.append(processing_latency)
                            
                            # VÃ©rification intention attendue
                            if expected_intent and chat_data['intent'] != expected_intent:
                                print(f"        âš ï¸ Intention diffÃ©rente: attendu {expected_intent}")
                            
                            # Alerte performance
                            if processing_latency > 10:
                                print(f"        âš ï¸ LATENCE L0 Ã‰LEVÃ‰E: {processing_latency:.1f}ms > 10ms")
                            
                        elif not should_match and not chat_data["success"]:
                            # Ne devrait pas matcher et n'a pas matchÃ©
                            print(f"   {i:2d}. âœ… '{message}' â†’ Correctement non matchÃ©")
                            l0_success += 1
                            
                        else:
                            # RÃ©sultat inattendu
                            print(f"   {i:2d}. âŒ '{message}' â†’ RÃ©sultat inattendu")
                            print(f"        Attendu match: {should_match}, RÃ©sultat: {chat_data['success']}")
                            
                    else:
                        print(f"   {i:2d}. âŒ '{message}' â†’ HTTP {response.status}")
                        
            except Exception as e:
                print(f"   {i:2d}. âŒ '{message}' â†’ Exception: {e}")
        
        # ===== ANALYSE PERFORMANCE L0 =====
        print(f"\nğŸ“Š Analyse Performance L0:")
        print(f"   - Tests rÃ©ussis: {l0_success}/{len(l0_test_queries)}")
        print(f"   - Taux succÃ¨s: {(l0_success/len(l0_test_queries)*100):.1f}%")
        
        if l0_latencies:
            avg_l0_latency = sum(l0_latencies) / len(l0_latencies)
            max_l0_latency = max(l0_latencies)
            min_l0_latency = min(l0_latencies)
            
            print(f"   - Latence L0 moyenne: {avg_l0_latency:.1f}ms")
            print(f"   - Latence L0 min/max: {min_l0_latency:.1f}/{max_l0_latency:.1f}ms")
            
            # Ã‰valuation targets Phase 1
            latency_target_met = avg_l0_latency < 10.0
            success_target_met = (l0_success/len(l0_test_queries)) >= 0.85
            
            print(f"   ğŸ¯ Target latence (<10ms): {'âœ…' if latency_target_met else 'âŒ'}")
            print(f"   ğŸ¯ Target succÃ¨s (>85%): {'âœ…' if success_target_met else 'âŒ'}")
            
            results["l0_metrics"] = {
                "success_rate": l0_success/len(l0_test_queries),
                "avg_latency_ms": avg_l0_latency,
                "latency_target_met": latency_target_met,
                "success_target_met": success_target_met
            }
        
        # ===== TEST 3: MÃ©triques DÃ©taillÃ©es =====
        print(f"\n3ï¸âƒ£ Test MÃ©triques L0...")
        try:
            async with session.get(f"{base_url}/api/v1/metrics", timeout=5) as response:
                if response.status == 200:
                    metrics_data = await response.json()
                    l0_perf = metrics_data.get("l0_performance", {})
                    targets = metrics_data.get("targets_validation", {})
                    
                    print(f"   âœ… MÃ©triques L0 disponibles")
                    print(f"   ğŸ“Š RequÃªtes totales: {l0_perf.get('total_requests', 0)}")
                    print(f"   ğŸ“Š Taux succÃ¨s L0: {l0_perf.get('success_rate', 0):.1%}")
                    print(f"   ğŸ“Š Latence L0: {l0_perf.get('avg_latency_ms', 0):.1f}ms")
                    print(f"   ğŸ¯ Targets: {sum(targets.get(k+'_met', False) for k in ['latency', 'success_rate', 'usage'])}/3")
                    
                    results["tests"].append({"test": "metrics_l0", "status": "pass"})
                else:
                    print(f"   âš ï¸ MÃ©triques unavailable: {response.status}")
                    results["tests"].append({"test": "metrics_l0", "status": "degraded"})
        except Exception as e:
            print(f"   âš ï¸ MÃ©triques error: {e}")
            results["errors"].append({"test": "metrics_l0", "error": str(e)})
        
        # ===== TEST 4: Debug Patterns =====
        print(f"\n4ï¸âƒ£ Test Debug Patterns...")
        try:
            debug_payload = {"message": "solde compte courant", "user_id": 999}
            async with session.post(
                f"{base_url}/api/v1/debug/test-patterns",
                json=debug_payload,
                timeout=5
            ) as response:
                if response.status == 200:
                    debug_data = await response.json()
                    
                    if debug_data.get("best_match"):
                        match_info = debug_data["best_match"]
                        print(f"   âœ… Debug patterns fonctionnel")
                        print(f"   ğŸ¯ Pattern: {match_info['pattern_name']}")
                        print(f"   ğŸ“Š Confiance: {match_info['confidence']:.2f}")
                        print(f"   â±ï¸ Latence: {debug_data['performance']['latency_ms']:.1f}ms")
                        
                        results["tests"].append({"test": "debug_patterns", "status": "pass"})
                    else:
                        print(f"   âš ï¸ Debug patterns no match")
                        results["tests"].append({"test": "debug_patterns", "status": "partial"})
                else:
                    print(f"   âŒ Debug patterns failed: {response.status}")
                    results["tests"].append({"test": "debug_patterns", "status": "fail"})
        except Exception as e:
            print(f"   âŒ Debug patterns error: {e}")
            results["errors"].append({"test": "debug_patterns", "error": str(e)})
    
    # ===== RÃ‰SUMÃ‰ FINAL PHASE 1 =====
    print(f"\nğŸ¯ RÃ‰SUMÃ‰ FINAL PHASE 1")
    print("=" * 70)
    
    total_tests = len(results["tests"])
    passed_tests = len([t for t in results["tests"] if t["status"] == "pass"])
    
    print(f"Tests rÃ©ussis: {passed_tests}/{total_tests}")
    print(f"Erreurs: {len(results['errors'])}")
    
    # Status global Phase 1
    l0_metrics = results.get("l0_metrics", {})
    phase1_success = (
        passed_tests >= total_tests * 0.75 and  # 75% tests passed
        l0_metrics.get("latency_target_met", False) and  # <10ms
        l0_metrics.get("success_target_met", False)  # >85% success
    )
    
    if phase1_success:
        print("ğŸ‰ PHASE 1 VALIDÃ‰E - L0 Pattern Matching opÃ©rationnel!")
        print("ğŸš€ PrÃªt pour Phase 2 (L1 TinyBERT)")
        results["overall_status"] = "phase1_complete"
    elif passed_tests >= total_tests * 0.5:
        print("âš ï¸ PHASE 1 PARTIELLE - Optimisations nÃ©cessaires")
        print("ğŸ”§ Recommandations: AmÃ©liorer patterns, optimiser latence")
        results["overall_status"] = "phase1_partial"
    else:
        print("âŒ PHASE 1 Ã‰CHOUÃ‰E - ProblÃ¨mes critiques")
        results["overall_status"] = "phase1_failed"
    
    # Performance summary
    if l0_metrics:
        print(f"\nğŸ“ˆ Performance L0:")
        print(f"   - SuccÃ¨s: {l0_metrics['success_rate']:.1%}")
        print(f"   - Latence: {l0_metrics['avg_latency_ms']:.1f}ms")
        print(f"   - Targets: {'âœ…' if l0_metrics.get('latency_target_met') and l0_metrics.get('success_target_met') else 'âŒ'}")
    
    return results

# ===== FONCTION MAIN =====
async def main():
    """Point d'entrÃ©e principal du test Phase 1"""
    try:
        results = await test_conversation_phase1_complete()
        
        # Sauvegarde rÃ©sultats
        with open("test_results_phase1.json", "w") as f:
            json.dump(results, f, indent=2, default=str)
        
        print(f"\nğŸ’¾ RÃ©sultats sauvegardÃ©s: test_results_phase1.json")
        
        # Code de sortie
        if results.get("overall_status") == "phase1_complete":
            exit(0)
        elif results.get("overall_status") == "phase1_partial":
            exit(1)
        else:
            exit(2)
            
    except KeyboardInterrupt:
        print("\nğŸ›‘ Test interrompu")
        exit(130)
    except Exception as e:
        print(f"\nğŸ’¥ Erreur test: {e}")
        exit(3)

if __name__ == "__main__":
    print("ğŸš€ Lancement test Phase 1...")
    asyncio.run(main())