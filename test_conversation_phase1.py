"""
ğŸ’¡ FICHIER SÃ‰PARÃ‰: test_conversation_phase1.py
Script pour valider la Phase 1 L0 Pattern Matching

âœ… CORRECTIONS APPLIQUÃ‰ES:
- URLs corrigÃ©es: http://localhost:8000 (local_app.py port)
- Endpoints corrigÃ©s: /api/v1/conversation/* (prefix conversation)
- Tests adaptÃ©s aux vrais modÃ¨les Pydantic
- Validation cohÃ©rente avec routes.py et main.py
"""

import asyncio
import aiohttp
import json
import time
import sys
from typing import Dict, Any, List

async def test_conversation_phase1_complete():
    """
    ğŸ§ª Test complet Phase 1 - L0 Pattern Matching
    
    VÃ©rifie:
    1. Service Phase 1 dÃ©marrÃ© correctement
    2. Pattern Matcher L0 fonctionnel 
    3. Performance <10ms sur patterns simples
    4. Taux succÃ¨s >85% sur requÃªtes financiÃ¨res
    5. CohÃ©rence avec architecture Phase 1
    """
    
    print("ğŸ§ª Test Validation Phase 1 - L0 Pattern Matching")
    print("=" * 70)
    
    # âœ… CORRIGÃ‰: Port 8000 pour local_app.py, prefix /api/v1/conversation
    base_url = "http://localhost:8000"
    conversation_url = f"{base_url}/api/v1/conversation"
    
    results = {"tests": [], "performance": [], "errors": [], "l0_metrics": {}}
    
    async with aiohttp.ClientSession() as session:
        
        # ===== TEST 0: Health Check Global =====
        print("\n0ï¸âƒ£ Test Health Check Global...")
        try:
            async with session.get(f"{base_url}/health", timeout=5) as response:
                if response.status == 200:
                    health_data = await response.json()
                    print(f"   âœ… Plateforme: {health_data['status']}")
                    
                    # VÃ©rifier conversation_service dans health global
                    conv_service = health_data.get('conversation_service', {})
                    print(f"   ğŸ¤– Conversation Service: {conv_service.get('status', 'unknown')}")
                    print(f"   ğŸ”§ Phase: {conv_service.get('phase', 'unknown')}")
                    print(f"   ğŸ“¦ Version: {conv_service.get('version', 'unknown')}")
                    
                    if conv_service.get('status') not in ['ok', 'degraded']:
                        print(f"   âŒ Conversation Service non disponible")
                        results["errors"].append({"test": "global_health", "error": "conversation_service not available"})
                        return results
                    
                    results["tests"].append({"test": "global_health", "status": "pass"})
                else:
                    print(f"   âŒ Health global failed: {response.status}")
                    results["tests"].append({"test": "global_health", "status": "fail", "code": response.status})
                    return results
        except Exception as e:
            print(f"   âŒ Health global error: {e}")
            results["errors"].append({"test": "global_health", "error": str(e)})
            return results
        
        # ===== TEST 1: Health Check Conversation Service =====
        print("\n1ï¸âƒ£ Test Health Check Conversation Service...")
        try:
            async with session.get(f"{conversation_url}/health", timeout=5) as response:
                if response.status == 200:
                    health_data = await response.json()
                    print(f"   âœ… Health: {health_data['status']}")
                    print(f"   ğŸ”§ Phase: {health_data.get('phase', 'unknown')}")
                    print(f"   âš¡ Latence Health: {health_data.get('latency_ms', 0):.1f}ms")
                    
                    # VÃ©rifier Pattern Matcher
                    pattern_matcher = health_data.get('pattern_matcher', {})
                    print(f"   ğŸ¯ Pattern Matcher: {pattern_matcher.get('status', 'unknown')}")
                    print(f"   ğŸ“Š Patterns: {pattern_matcher.get('patterns_loaded', 0)}")
                    
                    # VÃ©rifier L0 Performance dans health
                    l0_perf = health_data.get('l0_performance', {})
                    if l0_perf:
                        print(f"   ğŸ“ˆ L0 Success Rate: {l0_perf.get('success_rate', 0):.1%}")
                        print(f"   â±ï¸ L0 Latence: {l0_perf.get('avg_latency_ms', 0):.1f}ms")
                    
                    # VÃ©rifier targets
                    targets = health_data.get('targets_status', {})
                    if targets:
                        print(f"   ğŸ¯ Latence target: {'âœ…' if targets.get('latency_met', False) else 'âŒ'}")
                        print(f"   ğŸ¯ Success target: {'âœ…' if targets.get('success_met', False) else 'âŒ'}")
                    
                    results["tests"].append({"test": "conversation_health", "status": "pass"})
                else:
                    print(f"   âŒ Health conversation failed: {response.status}")
                    results["tests"].append({"test": "conversation_health", "status": "fail", "code": response.status})
                    return results
        except Exception as e:
            print(f"   âŒ Health conversation error: {e}")
            results["errors"].append({"test": "conversation_health", "error": str(e)})
            return results
        
        # ===== TEST 2: Patterns L0 Basiques =====
        print("\n2ï¸âƒ£ Test Patterns L0 Basiques...")
        
        l0_test_queries = [
            # âœ… Patterns haute confiance Phase 1
            ("solde", "BALANCE_CHECK", True),
            ("virement", "TRANSFER", True), 
            ("bloquer carte", "CARD_MANAGEMENT", True),
            ("dÃ©penses", "EXPENSE_ANALYSIS", True),
            ("bonjour", "GREETING", True),
            ("aide", "HELP", True),
            
            # âœ… Patterns avec entitÃ©s
            ("virer 100â‚¬", "TRANSFER", True),
            ("dÃ©penses restaurant", "EXPENSE_ANALYSIS", True),
            ("quel est mon solde", "BALANCE_CHECK", True),
            ("combien j'ai", "BALANCE_CHECK", True),
            
            # âœ… Patterns systÃ¨me
            ("salut", "GREETING", True),
            ("au revoir", "GOODBYE", True),
            
            # âœ… Cas qui ne devraient pas matcher L0
            ("mÃ©tÃ©o aujourd'hui", "UNKNOWN", False),
            ("recette de cuisine", "UNKNOWN", False),
            ("comment aller Ã  Paris", "UNKNOWN", False)
        ]
        
        l0_success = 0
        l0_latencies = []
        l0_detailed_results = []
        
        for i, (message, expected_intent, should_match) in enumerate(l0_test_queries, 1):
            try:
                start_time = time.time()
                
                # âœ… Format ChatRequest cohÃ©rent avec conversation_models.py
                payload = {
                    "message": message, 
                    "user_id": 100 + i,
                    "conversation_id": f"test_phase1_{i}",
                    "enable_cache": True,
                    "debug_mode": False
                }
                
                async with session.post(
                    f"{conversation_url}/chat",  # âœ… CORRIGÃ‰: /api/v1/conversation/chat
                    json=payload,
                    timeout=2  # Timeout court pour L0
                ) as response:
                    
                    request_latency = (time.time() - start_time) * 1000
                    
                    if response.status == 200:
                        chat_data = await response.json()
                        
                        # âœ… Validation format ChatResponse
                        processing_metadata = chat_data.get("processing_metadata", {})
                        processing_latency = processing_metadata.get("processing_time_ms", 0)
                        level_used = processing_metadata.get("level_used", "unknown")
                        pattern_used = processing_metadata.get("pattern_matched", "unknown")
                        
                        result_detail = {
                            "query": message,
                            "expected": expected_intent,
                            "actual": chat_data.get('intent', 'UNKNOWN'),
                            "success": chat_data.get("success", False),
                            "confidence": chat_data.get("confidence", 0),
                            "latency_ms": processing_latency,
                            "level_used": level_used,
                            "pattern_used": pattern_used
                        }
                        
                        if should_match and chat_data["success"] and level_used == "L0_PATTERN":
                            # âœ… Devrait matcher L0 et a matchÃ© L0
                            print(f"   {i:2d}. âœ… '{message}' â†’ {chat_data['intent']} ({pattern_used})")
                            print(f"        â±ï¸ L0: {processing_latency:.1f}ms, Total: {request_latency:.1f}ms, Conf: {chat_data['confidence']:.2f}")
                            
                            l0_success += 1
                            l0_latencies.append(processing_latency)
                            
                            # VÃ©rification intention attendue
                            if expected_intent and chat_data['intent'] != expected_intent:
                                print(f"        âš ï¸ Intention diffÃ©rente: attendu {expected_intent}")
                                result_detail["warning"] = f"Expected {expected_intent}, got {chat_data['intent']}"
                            
                            # Alerte performance
                            if processing_latency > 10:
                                print(f"        âš ï¸ LATENCE L0 Ã‰LEVÃ‰E: {processing_latency:.1f}ms > 10ms")
                                result_detail["performance_warning"] = True
                            
                        elif not should_match and (not chat_data["success"] or chat_data['intent'] == 'UNKNOWN'):
                            # âœ… Ne devrait pas matcher et n'a pas matchÃ© (ou UNKNOWN)
                            print(f"   {i:2d}. âœ… '{message}' â†’ Correctement non matchÃ© ({chat_data.get('intent', 'UNKNOWN')})")
                            l0_success += 1
                            
                        else:
                            # âŒ RÃ©sultat inattendu
                            print(f"   {i:2d}. âŒ '{message}' â†’ RÃ©sultat inattendu")
                            print(f"        Attendu match: {should_match}, RÃ©sultat: {chat_data['success']}")
                            print(f"        Intent: {chat_data.get('intent', 'UNKNOWN')}, Level: {level_used}")
                            result_detail["unexpected"] = True
                        
                        l0_detailed_results.append(result_detail)
                            
                    else:
                        print(f"   {i:2d}. âŒ '{message}' â†’ HTTP {response.status}")
                        error_text = await response.text()
                        print(f"        Error: {error_text[:100]}")
                        
                        l0_detailed_results.append({
                            "query": message,
                            "error": f"HTTP {response.status}",
                            "error_detail": error_text[:200]
                        })
                        
            except Exception as e:
                print(f"   {i:2d}. âŒ '{message}' â†’ Exception: {e}")
                l0_detailed_results.append({
                    "query": message,
                    "exception": str(e)
                })
        
        # ===== ANALYSE PERFORMANCE L0 =====
        print(f"\nğŸ“Š Analyse Performance L0:")
        print(f"   - Tests rÃ©ussis: {l0_success}/{len(l0_test_queries)}")
        success_rate = l0_success/len(l0_test_queries)
        print(f"   - Taux succÃ¨s: {success_rate*100:.1f}%")
        
        if l0_latencies:
            avg_l0_latency = sum(l0_latencies) / len(l0_latencies)
            max_l0_latency = max(l0_latencies)
            min_l0_latency = min(l0_latencies)
            
            print(f"   - Latence L0 moyenne: {avg_l0_latency:.1f}ms")
            print(f"   - Latence L0 min/max: {min_l0_latency:.1f}/{max_l0_latency:.1f}ms")
            
            # âœ… Ã‰valuation targets Phase 1
            latency_target_met = avg_l0_latency < 10.0
            success_target_met = success_rate >= 0.85
            
            print(f"   ğŸ¯ Target latence (<10ms): {'âœ…' if latency_target_met else 'âŒ'}")
            print(f"   ğŸ¯ Target succÃ¨s (>85%): {'âœ…' if success_target_met else 'âŒ'}")
            
            results["l0_metrics"] = {
                "success_rate": success_rate,
                "avg_latency_ms": avg_l0_latency,
                "min_latency_ms": min_l0_latency,
                "max_latency_ms": max_l0_latency,
                "latency_target_met": latency_target_met,
                "success_target_met": success_target_met,
                "detailed_results": l0_detailed_results
            }
        
        # ===== TEST 3: MÃ©triques DÃ©taillÃ©es =====
        print(f"\n3ï¸âƒ£ Test MÃ©triques L0...")
        try:
            async with session.get(f"{conversation_url}/metrics", timeout=5) as response:
                if response.status == 200:
                    metrics_data = await response.json()
                    
                    # âœ… Structure cohÃ©rente avec routes.py corrigÃ©
                    l0_perf = metrics_data.get("l0_performance", {})
                    targets_validation = metrics_data.get("targets_validation", {})
                    system_info = metrics_data.get("system_info", {})
                    
                    print(f"   âœ… MÃ©triques L0 disponibles")
                    print(f"   ğŸ“Š RequÃªtes totales: {l0_perf.get('total_requests', 0)}")
                    print(f"   ğŸ“Š Taux succÃ¨s L0: {l0_perf.get('success_rate', 0):.1%}")
                    print(f"   ğŸ“Š Latence L0: {l0_perf.get('avg_latency_ms', 0):.1f}ms")
                    print(f"   ğŸ“Š Usage L0: {l0_perf.get('usage_percent', 0):.1f}%")
                    print(f"   ğŸ“Š Cache hit rate: {l0_perf.get('cache_hit_rate', 0):.1%}")
                    
                    # Validation targets depuis mÃ©triques
                    targets_met = sum(1 for k in ['latency_target_met', 'success_rate_met', 'usage_target_met'] 
                                    if targets_validation.get(k, False))
                    print(f"   ğŸ¯ Targets met: {targets_met}/3")
                    
                    # Ready for L1?
                    ready_for_l1 = system_info.get("ready_for_l1", False)
                    print(f"   ğŸš€ Ready for Phase 2: {'âœ…' if ready_for_l1 else 'âŒ'}")
                    
                    results["tests"].append({"test": "metrics_l0", "status": "pass"})
                    results["metrics_details"] = {
                        "l0_performance": l0_perf,
                        "targets_validation": targets_validation,
                        "ready_for_l1": ready_for_l1
                    }
                else:
                    print(f"   âš ï¸ MÃ©triques unavailable: {response.status}")
                    results["tests"].append({"test": "metrics_l0", "status": "degraded"})
        except Exception as e:
            print(f"   âš ï¸ MÃ©triques error: {e}")
            results["errors"].append({"test": "metrics_l0", "error": str(e)})
        
        # ===== TEST 4: Status Service =====
        print(f"\n4ï¸âƒ£ Test Status Service...")
        try:
            async with session.get(f"{conversation_url}/status", timeout=5) as response:
                if response.status == 200:
                    status_data = await response.json()
                    
                    print(f"   âœ… Status disponible")
                    print(f"   ğŸ”§ Phase: {status_data.get('phase', 'unknown')}")
                    print(f"   ğŸ“¦ Version: {status_data.get('version', 'unknown')}")
                    
                    # Architecture Phase 1
                    architecture = status_data.get('architecture', {})
                    if architecture:
                        print(f"   ğŸ—ï¸ L0 enabled: {architecture.get('l0_enabled', False)}")
                        print(f"   ğŸ—ï¸ L1 enabled: {architecture.get('l1_enabled', False)}")
                        print(f"   ğŸ—ï¸ L2 enabled: {architecture.get('l2_enabled', False)}")
                    
                    # Pattern Matcher info
                    pm_info = status_data.get('pattern_matcher', {})
                    if pm_info:
                        print(f"   ğŸ¯ Patterns loaded: {pm_info.get('loaded', 0)}")
                    
                    results["tests"].append({"test": "status_service", "status": "pass"})
                else:
                    print(f"   âš ï¸ Status unavailable: {response.status}")
                    results["tests"].append({"test": "status_service", "status": "degraded"})
        except Exception as e:
            print(f"   âš ï¸ Status error: {e}")
            results["errors"].append({"test": "status_service", "error": str(e)})
        
        # ===== TEST 5: Debug Patterns =====
        print(f"\n5ï¸âƒ£ Test Debug Patterns...")
        try:
            debug_payload = {
                "message": "solde compte courant", 
                "user_id": 999,
                "debug_mode": True
            }
            async with session.post(
                f"{conversation_url}/debug/test-patterns",
                json=debug_payload,
                timeout=5
            ) as response:
                if response.status == 200:
                    debug_data = await response.json()
                    
                    # âœ… Validation structure debug response
                    if debug_data.get("status") == "success" and debug_data.get("pattern_match"):
                        match_info = debug_data["pattern_match"]
                        print(f"   âœ… Debug patterns fonctionnel")
                        print(f"   ğŸ¯ Pattern: {match_info.get('pattern_name', 'unknown')}")
                        print(f"   ğŸ“Š Confiance: {match_info.get('confidence', 0):.2f}")
                        print(f"   â±ï¸ Latence: {debug_data.get('processing_time_ms', 0):.1f}ms")
                        
                        results["tests"].append({"test": "debug_patterns", "status": "pass"})
                    else:
                        print(f"   âš ï¸ Debug patterns no match")
                        results["tests"].append({"test": "debug_patterns", "status": "partial"})
                else:
                    print(f"   âŒ Debug patterns failed: {response.status}")
                    results["tests"].append({"test": "debug_patterns", "status": "fail"})
        except Exception as e:
            print(f"   âŒ Debug patterns error: {e}")
            results["errors"].append({"test": "debug_patterns", "error": str(e)})
        
        # ===== TEST 6: Validation Phase 1 =====
        print(f"\n6ï¸âƒ£ Test Validation Phase 1...")
        try:
            async with session.get(f"{conversation_url}/validate-phase1", timeout=10) as response:
                if response.status == 200:
                    validation_data = await response.json()
                    
                    phase1_validation = validation_data.get("phase1_validation", {})
                    overall_status = phase1_validation.get("overall_status", "unknown")
                    phase1_success = phase1_validation.get("phase1_success", False)
                    next_action = phase1_validation.get("next_action", "unknown")
                    
                    print(f"   âœ… Validation Phase 1 disponible")
                    print(f"   ğŸ¯ Status: {overall_status}")
                    print(f"   ğŸ‰ Phase 1 Success: {'âœ…' if phase1_success else 'âŒ'}")
                    print(f"   ğŸš€ Next Action: {next_action}")
                    
                    # DÃ©tails targets
                    targets_validation = validation_data.get("targets_validation", {})
                    if targets_validation:
                        all_targets_met = targets_validation.get("all_targets_met", False)
                        print(f"   ğŸ¯ All Targets Met: {'âœ…' if all_targets_met else 'âŒ'}")
                    
                    # Performance actuelle
                    current_perf = validation_data.get("current_performance", {})
                    if current_perf:
                        print(f"   ğŸ“Š Current Latency: {current_perf.get('latency_ms', 0):.1f}ms")
                        print(f"   ğŸ“Š Current Success: {current_perf.get('success_rate', 0):.1%}")
                    
                    results["tests"].append({"test": "validate_phase1", "status": "pass"})
                    results["phase1_validation"] = validation_data
                else:
                    print(f"   âŒ Validation Phase 1 failed: {response.status}")
                    results["tests"].append({"test": "validate_phase1", "status": "fail"})
        except Exception as e:
            print(f"   âŒ Validation Phase 1 error: {e}")
            results["errors"].append({"test": "validate_phase1", "error": str(e)})
    
    # ===== RÃ‰SUMÃ‰ FINAL PHASE 1 =====
    print(f"\nğŸ¯ RÃ‰SUMÃ‰ FINAL PHASE 1")
    print("=" * 70)
    
    total_tests = len(results["tests"])
    passed_tests = len([t for t in results["tests"] if t["status"] == "pass"])
    
    print(f"Tests rÃ©ussis: {passed_tests}/{total_tests}")
    print(f"Erreurs: {len(results['errors'])}")
    
    # âœ… Status global Phase 1 basÃ© sur mÃ©triques rÃ©elles
    l0_metrics = results.get("l0_metrics", {})
    validation_data = results.get("phase1_validation", {})
    
    # CritÃ¨res Phase 1
    tests_passed = passed_tests >= total_tests * 0.75  # 75% tests passed
    latency_ok = l0_metrics.get("latency_target_met", False)
    success_ok = l0_metrics.get("success_target_met", False)
    
    # Validation officielle si disponible
    official_validation = validation_data.get("phase1_validation", {}).get("phase1_success", None)
    
    if official_validation is True:
        print("ğŸ‰ PHASE 1 OFFICIELLEMENT VALIDÃ‰E!")
        print("ğŸš€ PrÃªt pour Phase 2 (L1 TinyBERT)")
        results["overall_status"] = "phase1_complete_official"
    elif tests_passed and latency_ok and success_ok:
        print("ğŸ‰ PHASE 1 VALIDÃ‰E - L0 Pattern Matching opÃ©rationnel!")
        print("ğŸš€ PrÃªt pour Phase 2 (L1 TinyBERT)")
        results["overall_status"] = "phase1_complete"
    elif passed_tests >= total_tests * 0.5:
        print("âš ï¸ PHASE 1 PARTIELLE - Optimisations nÃ©cessaires")
        print("ğŸ”§ Recommandations: AmÃ©liorer patterns, optimiser latence")
        results["overall_status"] = "phase1_partial"
    else:
        print("âŒ PHASE 1 Ã‰CHOUÃ‰E - ProblÃ¨mes critiques")
        results["overall_status"] = "phase1_failed"
    
    # âœ… Performance summary dÃ©taillÃ©
    if l0_metrics:
        print(f"\nğŸ“ˆ Performance L0:")
        print(f"   - SuccÃ¨s: {l0_metrics['success_rate']:.1%}")
        print(f"   - Latence moyenne: {l0_metrics['avg_latency_ms']:.1f}ms")
        print(f"   - Latence min/max: {l0_metrics['min_latency_ms']:.1f}/{l0_metrics['max_latency_ms']:.1f}ms")
        print(f"   - Targets: {'âœ…' if l0_metrics.get('latency_target_met') and l0_metrics.get('success_target_met') else 'âŒ'}")
    
    # Recommandations
    if results["overall_status"] in ["phase1_partial", "phase1_failed"]:
        print(f"\nğŸ”§ Recommandations:")
        if not latency_ok:
            print(f"   - Optimiser patterns pour rÃ©duire latence <10ms")
        if not success_ok:
            print(f"   - Ajouter patterns pour couvrir plus de cas d'usage")
        if len(results["errors"]) > 0:
            print(f"   - Corriger erreurs techniques identifiÃ©es")
    
    return results

# ===== FONCTION MAIN =====
async def main():
    """Point d'entrÃ©e principal du test Phase 1"""
    try:
        print("ğŸš€ Lancement test Phase 1...")
        print("ğŸ“‹ Configuration:")
        print("   - URL: http://localhost:8000")
        print("   - Service: conversation_service")
        print("   - Phase: L0_PATTERN_MATCHING")
        print("   - Endpoints: /api/v1/conversation/*")
        print()
        
        results = await test_conversation_phase1_complete()
        
        # âœ… Sauvegarde rÃ©sultats dÃ©taillÃ©s
        timestamp = int(time.time())
        results_file = f"test_results_phase1_{timestamp}.json"
        
        with open(results_file, "w", encoding="utf-8") as f:
            json.dump(results, f, indent=2, default=str, ensure_ascii=False)
        
        print(f"\nğŸ’¾ RÃ©sultats sauvegardÃ©s: {results_file}")
        
        # âœ… Code de sortie basÃ© sur status
        overall_status = results.get("overall_status", "unknown")
        
        if overall_status in ["phase1_complete", "phase1_complete_official"]:
            print("âœ… EXIT CODE 0: Phase 1 validÃ©e")
            return 0
        elif overall_status == "phase1_partial":
            print("âš ï¸ EXIT CODE 1: Phase 1 partielle")
            return 1
        else:
            print("âŒ EXIT CODE 2: Phase 1 Ã©chouÃ©e")
            return 2
            
    except KeyboardInterrupt:
        print("\nğŸ›‘ Test interrompu par utilisateur")
        return 130
    except Exception as e:
        print(f"\nğŸ’¥ Erreur critique test: {e}")
        import traceback
        traceback.print_exc()
        return 3

if __name__ == "__main__":
    # âœ… Gestion propre des codes de sortie
    exit_code = asyncio.run(main())
    sys.exit(exit_code)