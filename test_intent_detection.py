#!/usr/bin/env python3
"""
üß™ Test Complet Conversation Service

Suite de tests exhaustive pour v√©rifier que tous les endpoints du conversation_service
sont op√©rationnels et que le service fonctionne correctement dans son √©tat actuel.

Usage:
    python test_conversation_service_complete.py
    ou
    pytest test_conversation_service_complete.py -v
"""

import asyncio
import json
import time
import uuid
import requests
import pytest
from typing import Dict, Any, List, Optional
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging
from conversation_service.models.financial_models import FinancialEntity

# Configuration logging pour les tests
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


@dataclass
class TestConfig:
    """Configuration des tests"""
    base_url: str = "http://localhost:8001"  # Port par d√©faut conversation_service
    timeout: int = 10
    max_retries: int = 3
    test_user_id: str = "test_user_123"
    admin_token: str = "admin_test_token"


@dataclass
class EndpointTestResult:
    """R√©sultat d'un test d'endpoint"""
    endpoint: str
    method: str
    status_code: int
    response_time_ms: float
    success: bool
    error_message: Optional[str] = None
    response_data: Optional[Dict] = None


class ConversationServiceTester:
    """Testeur complet du service de conversation"""
    
    def __init__(self, config: TestConfig):
        self.config = config
        self.session = requests.Session()
        self.session.timeout = config.timeout
        self.test_results: List[EndpointTestResult] = []
        
    def _make_request(
        self, 
        method: str, 
        endpoint: str, 
        data: Optional[Dict] = None,
        headers: Optional[Dict] = None,
        params: Optional[Dict] = None
    ) -> EndpointTestResult:
        """
        Effectue une requ√™te HTTP et retourne le r√©sultat
        """
        url = f"{self.config.base_url}{endpoint}"
        start_time = time.time()
        
        try:
            if method.upper() == "GET":
                response = self.session.get(url, params=params, headers=headers)
            elif method.upper() == "POST":
                response = self.session.post(url, json=data, headers=headers, params=params)
            elif method.upper() == "PUT":
                response = self.session.put(url, json=data, headers=headers)
            elif method.upper() == "DELETE":
                response = self.session.delete(url, headers=headers)
            else:
                raise ValueError(f"M√©thode HTTP non support√©e: {method}")
            
            response_time_ms = (time.time() - start_time) * 1000
            
            # Tentative de parsing JSON
            try:
                response_data = response.json()
            except:
                response_data = {"raw_content": response.text[:500]}
            
            return EndpointTestResult(
                endpoint=endpoint,
                method=method,
                status_code=response.status_code,
                response_time_ms=response_time_ms,
                success=200 <= response.status_code < 300,
                response_data=response_data
            )
            
        except Exception as e:
            response_time_ms = (time.time() - start_time) * 1000
            return EndpointTestResult(
                endpoint=endpoint,
                method=method,
                status_code=0,
                response_time_ms=response_time_ms,
                success=False,
                error_message=str(e)
            )
    
    # =====================================
    # TESTS ENDPOINTS SYST√àME
    # =====================================
    
    def test_root_endpoint(self) -> EndpointTestResult:
        """Test endpoint racine /"""
        logger.info("üè† Test endpoint racine")
        result = self._make_request("GET", "/")
        
        if result.success and result.response_data:
            expected_keys = ["service", "version", "endpoints", "features"]
            missing_keys = [k for k in expected_keys if k not in result.response_data]
            
            if missing_keys:
                result.success = False
                result.error_message = f"Cl√©s manquantes: {missing_keys}"
            else:
                logger.info(f"‚úÖ Service: {result.response_data.get('service')}")
                logger.info(f"‚úÖ Version: {result.response_data.get('version')}")
        
        return result
    
    def test_api_status_endpoint(self) -> EndpointTestResult:
        """Test endpoint /api/v1/status"""
        logger.info("üìä Test endpoint /api/v1/status")
        result = self._make_request("GET", "/api/v1/status")
        
        if result.success and result.response_data:
            required_fields = ["status", "timestamp"]
            missing_fields = [f for f in required_fields if f not in result.response_data]
            
            if missing_fields:
                result.success = False
                result.error_message = f"Champs manquants: {missing_fields}"
            elif result.response_data.get("status") != "ok":
                result.success = False
                result.error_message = f"Status incorrect: {result.response_data.get('status')}"
        
        return result
    
    def test_api_version_endpoint(self) -> EndpointTestResult:
        """Test endpoint /api/v1/version"""
        logger.info("üî¢ Test endpoint /api/v1/version")
        result = self._make_request("GET", "/api/v1/version")
        
        if result.success and result.response_data:
            required_fields = ["version", "service"]
            missing_fields = [f for f in required_fields if f not in result.response_data]
            
            if missing_fields:
                result.success = False
                result.error_message = f"Champs manquants: {missing_fields}"
        
        return result
    
    def test_legacy_status_endpoint(self) -> EndpointTestResult:
        """Test endpoint legacy /status"""
        logger.info("üìä Test endpoint legacy /status")
        result = self._make_request("GET", "/status")
        
        if result.success and result.response_data:
            # V√©rifier que l'endpoint legacy inclut un avertissement de d√©pr√©ciation
            if not result.response_data.get("deprecated"):
                result.error_message = "Endpoint legacy devrait inclure deprecated=true"
        
        return result
    
    def test_legacy_version_endpoint(self) -> EndpointTestResult:
        """Test endpoint legacy /version"""
        logger.info("üî¢ Test endpoint legacy /version")
        return self._make_request("GET", "/version")
    
    # =====================================
    # TESTS ENDPOINTS CORE BUSINESS
    # =====================================
    
    def test_health_endpoint(self) -> EndpointTestResult:
        """Test endpoint /api/v1/health"""
        logger.info("üè• Test endpoint health")
        result = self._make_request("GET", "/api/v1/health")
        
        if result.success and result.response_data:
            required_fields = ["status", "components", "timestamp"]
            missing_fields = [f for f in required_fields if f not in result.response_data]
            
            if missing_fields:
                result.success = False
                result.error_message = f"Champs manquants: {missing_fields}"
            else:
                # V√©rifier statut service
                status = result.response_data.get("status")
                if status not in ["healthy", "degraded", "unhealthy"]:
                    result.success = False
                    result.error_message = f"Status sant√© invalide: {status}"
                else:
                    logger.info(f"‚úÖ Statut sant√©: {status}")
        
        return result
    
    def test_metrics_endpoint(self) -> EndpointTestResult:
        """Test endpoint /api/v1/metrics"""
        logger.info("üìà Test endpoint metrics")
        result = self._make_request("GET", "/api/v1/metrics")
        
        if result.success and result.response_data:
            # V√©rifier pr√©sence des m√©triques de base
            expected_sections = ["service_metrics", "performance_metrics"]
            
            for section in expected_sections:
                if section not in result.response_data:
                    result.error_message = f"Section m√©triques manquante: {section}"
                    break
        
        return result
    
    def test_detect_intent_endpoint(self) -> EndpointTestResult:
        """Test endpoint principal /api/v1/detect-intent"""
        logger.info("üéØ Test endpoint detect-intent")
        
        test_payload = {
            "query": "bonjour comment allez-vous",
            "user_id": self.config.test_user_id,
            "use_deepseek_fallback": False,
            "context": {"session_id": "test_session"}
        }
        
        result = self._make_request("POST", "/api/v1/detect-intent", data=test_payload)
        
        if result.success and result.response_data:
            required_fields = [
                "intent_type",
                "intent_category",
                "confidence",
                "entities",
                "processing_time_ms",
            ]
            missing_fields = [f for f in required_fields if f not in result.response_data]

            if missing_fields:
                result.success = False
                result.error_message = f"Champs r√©ponse manquants: {missing_fields}"
            else:
                intent_type = result.response_data.get("intent_type")
                intent_category = result.response_data.get("intent_category")
                confidence = result.response_data.get("confidence")
                latency = result.response_data.get("processing_time_ms")
                entities = result.response_data.get("entities", [])

                logger.info(
                    f"‚úÖ Intent d√©tect√©: {intent_type} ({intent_category})"
                )
                logger.info(f"‚úÖ Confiance: {confidence:.3f}")
                logger.info(f"‚úÖ Latence: {latency:.1f}ms")

                if not isinstance(entities, list):
                    result.success = False
                    result.error_message = "Entities should be a list"
                else:
                    for entity in entities:
                        FinancialEntity(**entity)

                # V√©rifications basiques
                if not isinstance(confidence, (int, float)) or not (0 <= confidence <= 1):
                    result.success = False
                    result.error_message = f"Confiance invalide: {confidence}"
                elif latency > 5000:  # 5 secondes max
                    result.success = False
                    result.error_message = f"Latence trop √©lev√©e: {latency}ms"
        
        return result
    
    def test_batch_detect_intent_endpoint(self) -> EndpointTestResult:
        """Test endpoint /api/v1/batch-detect"""
        logger.info("üì¶ Test endpoint batch-detect")
        
        test_payload = {
            "requests": [
                {
                    "query": "bonjour",
                    "user_id": self.config.test_user_id
                },
                {
                    "query": "quel est mon solde",
                    "user_id": self.config.test_user_id
                },
                {
                    "query": "au revoir",
                    "user_id": self.config.test_user_id
                }
            ]
        }
        
        result = self._make_request("POST", "/api/v1/batch-detect", data=test_payload)
        
        if result.success and result.response_data:
            required_fields = ["results", "batch_metadata"]
            missing_fields = [f for f in required_fields if f not in result.response_data]
            
            if missing_fields:
                result.success = False
                result.error_message = f"Champs batch manquants: {missing_fields}"
            else:
                results = result.response_data.get("results", [])
                if len(results) != 3:
                    result.success = False
                    result.error_message = f"Nombre r√©sultats incorrect: {len(results)} != 3"
                else:
                    logger.info(f"‚úÖ Batch trait√©: {len(results)} requ√™tes")
        
        return result
    
    def test_supported_intents_endpoint(self) -> EndpointTestResult:
        """Test endpoint /api/v1/supported-intents"""
        logger.info("üìã Test endpoint supported-intents")
        result = self._make_request("GET", "/api/v1/supported-intents")
        
        if result.success and result.response_data:
            required_fields = ["supported_intents", "intent_details"]
            missing_fields = [f for f in required_fields if f not in result.response_data]
            
            if missing_fields:
                result.success = False
                result.error_message = f"Champs intentions manquants: {missing_fields}"
            else:
                intents = result.response_data.get("supported_intents", [])
                logger.info(f"‚úÖ Intentions support√©es: {len(intents)}")
        
        return result
    
    # =====================================
    # TESTS ENDPOINTS ADMIN
    # =====================================
    
    def test_cache_clear_endpoint(self) -> EndpointTestResult:
        """Test endpoint admin /api/v1/cache/clear"""
        logger.info("üóëÔ∏è Test endpoint cache/clear")
        
        headers = {"Authorization": f"Bearer {self.config.admin_token}"}
        result = self._make_request("POST", "/api/v1/cache/clear", headers=headers)
        
        # Note: Peut √©chouer si pas d'auth admin, c'est normal
        if result.status_code == 401:
            logger.info("‚ÑπÔ∏è Endpoint admin n√©cessite authentification (normal)")
            result.success = True  # On consid√®re √ßa comme un succ√®s
        
        return result
    
    def test_debug_test_endpoint(self) -> EndpointTestResult:
        """Test endpoint debug /api/v1/debug/test"""
        logger.info("üîß Test endpoint debug/test")
        
        headers = {"Authorization": f"Bearer {self.config.admin_token}"}
        result = self._make_request("GET", "/api/v1/debug/test", headers=headers)
        
        # Endpoint debug peut √™tre d√©sactiv√© en production
        if result.status_code == 404:
            logger.info("‚ÑπÔ∏è Endpoint debug d√©sactiv√© (normal en production)")
            result.success = True
        
        return result
    
    def test_debug_config_endpoint(self) -> EndpointTestResult:
        """Test endpoint debug /api/v1/debug/config"""
        logger.info("‚öôÔ∏è Test endpoint debug/config")
        result = self._make_request("GET", "/api/v1/debug/config")
        
        # Endpoint debug peut √™tre d√©sactiv√©
        if result.status_code == 404:
            logger.info("‚ÑπÔ∏è Endpoint debug/config d√©sactiv√© (normal)")
            result.success = True
        
        return result
    
    # =====================================
    # TESTS DE CHARGE ET PERFORMANCE
    # =====================================
    
    def test_concurrent_requests(self, num_requests: int = 10) -> List[EndpointTestResult]:
        """Test de charge avec requ√™tes concurrentes"""
        logger.info(f"‚ö° Test charge: {num_requests} requ√™tes concurrentes")
        
        test_queries = [
            "bonjour",
            "quel est mon solde",
            "mes derni√®res transactions",
            "virement 100 euros",
            "bloquer ma carte",
            "au revoir",
            "aide moi",
            "historique janvier",
            "budget mensuel",
            "d√©penses restaurant"
        ]
        
        def make_concurrent_request(i):
            query = test_queries[i % len(test_queries)]
            payload = {
                "query": query,
                "user_id": f"concurrent_user_{i}",
                "use_deepseek_fallback": False
            }
            return self._make_request("POST", "/api/v1/detect-intent", data=payload)
        
        results = []
        with ThreadPoolExecutor(max_workers=5) as executor:
            futures = [executor.submit(make_concurrent_request, i) for i in range(num_requests)]
            
            for future in as_completed(futures):
                result = future.result()
                results.append(result)
        
        # Analyse des r√©sultats
        successful = [r for r in results if r.success]
        failed = [r for r in results if not r.success]
        avg_latency = sum(r.response_time_ms for r in successful) / len(successful) if successful else 0
        
        logger.info(f"‚úÖ Requ√™tes r√©ussies: {len(successful)}/{num_requests}")
        logger.info(f"‚ùå Requ√™tes √©chou√©es: {len(failed)}")
        logger.info(f"‚è±Ô∏è Latence moyenne: {avg_latency:.1f}ms")
        
        return results
    
    def test_different_query_types(self) -> List[EndpointTestResult]:
        """Test avec diff√©rents types de requ√™tes"""
        logger.info("üé≠ Test vari√©t√© de requ√™tes")
        
        test_cases = [
            # Salutations
            {"query": "bonjour", "expected_intent_category": "conversational"},
            {"query": "salut comment √ßa va", "expected_intent_category": "conversational"},
            {"query": "bonsoir", "expected_intent_category": "conversational"},

            # Finance - Solde
            {"query": "quel est mon solde", "expected_intent_category": "financial"},
            {"query": "combien j'ai sur mon compte", "expected_intent_category": "financial"},
            {"query": "mon argent disponible", "expected_intent_category": "financial"},

            # Finance - Transactions
            {"query": "mes derniers achats", "expected_intent_category": "financial"},
            {"query": "historique janvier 2024", "expected_intent_category": "financial"},
            {"query": "d√©penses restaurant ce mois", "expected_intent_category": "financial"},

            # Finance - Virements
            {"query": "virer 50 euros √† Paul", "expected_intent_category": "financial"},
            {"query": "envoyer de l'argent", "expected_intent_category": "financial"},

            # Finance - Cartes
            {"query": "bloquer ma carte", "expected_intent_category": "financial"},
            {"query": "faire opposition", "expected_intent_category": "financial"},

            # Aide
            {"query": "aide moi", "expected_intent_category": "conversational"},
            {"query": "je ne comprends pas", "expected_intent_category": "conversational"},

            # Au revoir
            {"query": "au revoir", "expected_intent_category": "conversational"},
            {"query": "√† bient√¥t", "expected_intent_category": "conversational"},

            # Requ√™tes ambig√ºes/inconnues
            {"query": "aksjdhaksjdh", "expected_intent_category": "unknown"},
            {"query": "xyz 123 test", "expected_intent_category": "unknown"}
        ]
        
        results = []
        for i, test_case in enumerate(test_cases):
            payload = {
                "query": test_case["query"],
                "user_id": f"test_variety_{i}",
                "use_deepseek_fallback": False
            }
            
            result = self._make_request("POST", "/api/v1/detect-intent", data=payload)
            result.test_case = test_case
            results.append(result)
            
            if result.success and result.response_data:
                intent_type = result.response_data.get("intent_type")
                confidence = result.response_data.get("confidence", 0)
                logger.info(
                    f"  '{test_case['query'][:30]}' -> {intent_type} ({confidence:.3f})"
                )
        
        return results
    
    # =====================================
    # TESTS EDGE CASES
    # =====================================
    
    def test_edge_cases(self) -> List[EndpointTestResult]:
        """Test cas limites et edge cases"""
        logger.info("üî¨ Test cas limites")
        
        edge_cases = [
            # Requ√™te vide
            {"query": "", "description": "requ√™te vide"},
            
            # Requ√™te tr√®s courte
            {"query": "a", "description": "requ√™te tr√®s courte"},
            
            # Requ√™te tr√®s longue
            {"query": "a" * 1000, "description": "requ√™te tr√®s longue"},
            
            # Caract√®res sp√©ciaux
            {"query": "√©√†√ß√±√ºÁâπÊÆäÂ≠óÁ¨¶", "description": "caract√®res sp√©ciaux"},
            
            # Uniquement espaces
            {"query": "   ", "description": "uniquement espaces"},
            
            # Chiffres uniquement
            {"query": "123456789", "description": "chiffres uniquement"},
            
            # Ponctuation uniquement
            {"query": "!@#$%^&*()", "description": "ponctuation uniquement"}
        ]
        
        results = []
        for i, case in enumerate(edge_cases):
            payload = {
                "query": case["query"],
                "user_id": f"edge_case_{i}",
                "use_deepseek_fallback": False
            }
            
            result = self._make_request("POST", "/api/v1/detect-intent", data=payload)
            result.edge_case = case
            results.append(result)
            
            logger.info(f"  {case['description']}: {'‚úÖ' if result.success else '‚ùå'}")
        
        return results
    
    def test_invalid_payloads(self) -> List[EndpointTestResult]:
        """Test avec payloads invalides"""
        logger.info("üö´ Test payloads invalides")
        
        invalid_payloads = [
            # Payload vide
            {},
            
            # Query manquante
            {"user_id": "test"},
            
            # Types incorrects
            {"query": 123, "user_id": "test"},
            {"query": "test", "user_id": 123},
            {"query": "test", "confidence_threshold": "invalid"},
            
            # Valeurs nulles
            {"query": None, "user_id": "test"},
            {"query": "test", "user_id": None},
        ]
        
        results = []
        for i, payload in enumerate(invalid_payloads):
            result = self._make_request("POST", "/api/v1/detect-intent", data=payload)
            result.invalid_payload = payload
            results.append(result)
            
            # Pour les payloads invalides, on s'attend √† une erreur 422
            if result.status_code == 422:
                result.success = True  # C'est le comportement attendu
                logger.info(f"  Payload {i}: ‚úÖ (erreur 422 attendue)")
            else:
                logger.info(f"  Payload {i}: ‚ùå (attendu 422, re√ßu {result.status_code})")
        
        return results
    
    # =====================================
    # ORCHESTRATEUR DE TESTS
    # =====================================
    
    def run_all_tests(self) -> Dict[str, Any]:
        """Ex√©cute tous les tests et retourne un rapport complet"""
        logger.info("üöÄ D√©marrage de la suite de tests compl√®te")
        start_time = time.time()
        
        all_results = []
        test_sections = {}
        
        # Tests endpoints syst√®me
        logger.info("\nüìÅ Section: Endpoints syst√®me")
        system_tests = [
            self.test_root_endpoint(),
            self.test_api_status_endpoint(),
            self.test_api_version_endpoint(),
            self.test_legacy_status_endpoint(),
            self.test_legacy_version_endpoint()
        ]
        test_sections["system"] = system_tests
        all_results.extend(system_tests)
        
        # Tests endpoints business
        logger.info("\nüìÅ Section: Endpoints m√©tier")
        business_tests = [
            self.test_health_endpoint(),
            self.test_metrics_endpoint(),
            self.test_detect_intent_endpoint(),
            self.test_batch_detect_intent_endpoint(),
            self.test_supported_intents_endpoint()
        ]
        test_sections["business"] = business_tests
        all_results.extend(business_tests)
        
        # Tests endpoints admin
        logger.info("\nüìÅ Section: Endpoints admin")
        admin_tests = [
            self.test_cache_clear_endpoint(),
            self.test_debug_test_endpoint(),
            self.test_debug_config_endpoint()
        ]
        test_sections["admin"] = admin_tests
        all_results.extend(admin_tests)
        
        # Tests de performance
        logger.info("\nüìÅ Section: Tests de performance")
        concurrent_results = self.test_concurrent_requests(10)
        test_sections["performance"] = concurrent_results
        all_results.extend(concurrent_results)
        
        # Tests vari√©t√© de requ√™tes
        logger.info("\nüìÅ Section: Vari√©t√© de requ√™tes")
        variety_results = self.test_different_query_types()
        test_sections["variety"] = variety_results
        all_results.extend(variety_results)
        
        # Tests edge cases
        logger.info("\nüìÅ Section: Cas limites")
        edge_results = self.test_edge_cases()
        test_sections["edge_cases"] = edge_results
        all_results.extend(edge_results)
        
        # Tests payloads invalides
        logger.info("\nüìÅ Section: Payloads invalides")
        invalid_results = self.test_invalid_payloads()
        test_sections["invalid_payloads"] = invalid_results
        all_results.extend(invalid_results)
        
        # Calcul des statistiques finales
        total_time = time.time() - start_time
        successful_tests = [r for r in all_results if r.success]
        failed_tests = [r for r in all_results if not r.success]
        
        avg_response_time = sum(r.response_time_ms for r in successful_tests) / len(successful_tests) if successful_tests else 0
        
        report = {
            "summary": {
                "total_tests": len(all_results),
                "successful": len(successful_tests),
                "failed": len(failed_tests),
                "success_rate": len(successful_tests) / len(all_results) if all_results else 0,
                "total_duration_seconds": total_time,
                "average_response_time_ms": avg_response_time
            },
            "sections": {
                section: {
                    "total": len(results),
                    "successful": len([r for r in results if r.success]),
                    "failed": len([r for r in results if not r.success]),
                    "results": results
                }
                for section, results in test_sections.items()
            },
            "failed_tests": [
                {
                    "endpoint": r.endpoint,
                    "method": r.method,
                    "error": r.error_message,
                    "status_code": r.status_code
                }
                for r in failed_tests
            ],
            "service_status": "OPERATIONAL" if len(successful_tests) >= len(all_results) * 0.8 else "DEGRADED"
        }
        
        return report
    
    def print_report(self, report: Dict[str, Any]):
        """Affiche le rapport de tests de fa√ßon lisible"""
        print("\n" + "="*80)
        print("üß™ RAPPORT DE TESTS CONVERSATION SERVICE")
        print("="*80)
        
        summary = report["summary"]
        print(f"\nüìä R√âSUM√â GLOBAL:")
        print(f"   Total tests: {summary['total_tests']}")
        print(f"   ‚úÖ R√©ussis: {summary['successful']}")
        print(f"   ‚ùå √âchou√©s: {summary['failed']}")
        print(f"   üìà Taux de r√©ussite: {summary['success_rate']:.1%}")
        print(f"   ‚è±Ô∏è Dur√©e totale: {summary['total_duration_seconds']:.2f}s")
        print(f"   üöÄ Temps de r√©ponse moyen: {summary['average_response_time_ms']:.1f}ms")
        print(f"   üéØ Statut service: {report['service_status']}")
        
        print(f"\nüìÅ D√âTAIL PAR SECTION:")
        for section_name, section_data in report["sections"].items():
            success_rate = section_data["successful"] / section_data["total"] if section_data["total"] > 0 else 0
            print(f"   {section_name.title()}: {section_data['successful']}/{section_data['total']} ({success_rate:.1%})")
        
        if report["failed_tests"]:
            print(f"\n‚ùå TESTS √âCHOU√âS:")
            for failed in report["failed_tests"]:
                print(f"   {failed['method']} {failed['endpoint']} (HTTP {failed['status_code']})")
                if failed["error"]:
                    print(f"      Erreur: {failed['error']}")
        
        print(f"\n{'üéâ SERVICE OP√âRATIONNEL' if report['service_status'] == 'OPERATIONAL' else '‚ö†Ô∏è SERVICE D√âGRAD√â'}")
        print("="*80)


def main():
    """Fonction principale pour ex√©cuter les tests"""
    config = TestConfig()
    tester = ConversationServiceTester(config)
    
    print("üß™ D√©marrage des tests complets du Conversation Service")
    print(f"üéØ URL de test: {config.base_url}")
    print("="*60)
    
    try:
        # Ex√©cution de tous les tests
        report = tester.run_all_tests()
        
        # Affichage du rapport
        tester.print_report(report)
        
        # Sauvegarde du rapport en JSON
        with open("conversation_service_test_report.json", "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"\nüíæ Rapport sauvegard√©: conversation_service_test_report.json")
        
        # Code de sortie selon le statut
        return 0 if report["service_status"] == "OPERATIONAL" else 1
        
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è Tests interrompus par l'utilisateur")
        return 2
    except Exception as e:
        print(f"\nüí• Erreur lors des tests: {e}")
        logger.error(f"Erreur critique: {e}", exc_info=True)
        return 3


# =====================================
# TESTS PYTEST (optionnel)
# =====================================

@pytest.mark.skip("requires live conversation service")
class TestConversationServicePytest:
    """
    Classe de tests compatible pytest pour int√©gration CI/CD
    """
    
    @classmethod
    def setup_class(cls):
        """Setup global des tests pytest"""
        cls.config = TestConfig()
        cls.tester = ConversationServiceTester(cls.config)
    
    def test_service_availability(self):
        """Test disponibilit√© g√©n√©rale du service"""
        result = self.tester.test_root_endpoint()
        assert result.success, f"Service indisponible: {result.error_message}"
        assert result.response_time_ms < 1000, f"Temps de r√©ponse trop √©lev√©: {result.response_time_ms}ms"
    
    def test_health_check(self):
        """Test endpoint de sant√©"""
        result = self.tester.test_health_endpoint()
        assert result.success, f"Health check √©chou√©: {result.error_message}"
        
        if result.response_data:
            status = result.response_data.get("status")
            assert status in ["healthy", "degraded"], f"Statut sant√© invalide: {status}"
    
    def test_intent_detection_basic(self):
        """Test d√©tection d'intention basique"""
        result = self.tester.test_detect_intent_endpoint()
        assert result.success, f"D√©tection intention √©chou√©e: {result.error_message}"
        
        if result.response_data:
            confidence = result.response_data.get("confidence")
            assert isinstance(confidence, (int, float)), "Confiance doit √™tre num√©rique"
            assert 0 <= confidence <= 1, f"Confiance hors limites: {confidence}"
    
    def test_batch_processing(self):
        """Test traitement par batch"""
        result = self.tester.test_batch_detect_intent_endpoint()
        assert result.success, f"Batch processing √©chou√©: {result.error_message}"
    
    def test_api_consistency(self):
        """Test coh√©rence des endpoints API"""
        # Test que tous les endpoints /api/v1 sont coh√©rents
        endpoints_to_test = [
            "/api/v1/status",
            "/api/v1/version", 
            "/api/v1/health",
            "/api/v1/metrics",
            "/api/v1/supported-intents"
        ]
        
        for endpoint in endpoints_to_test:
            result = self.tester._make_request("GET", endpoint)
            assert result.success or result.status_code in [401, 403], f"Endpoint {endpoint} inaccessible: {result.status_code}"
    
    def test_performance_baseline(self):
        """Test performance baseline"""
        concurrent_results = self.tester.test_concurrent_requests(5)
        successful = [r for r in concurrent_results if r.success]
        
        assert len(successful) >= len(concurrent_results) * 0.8, "Trop d'√©checs en concurrence"
        
        if successful:
            avg_latency = sum(r.response_time_ms for r in successful) / len(successful)
            assert avg_latency < 2000, f"Latence moyenne trop √©lev√©e: {avg_latency}ms"
    
    def test_error_handling(self):
        """Test gestion d'erreurs"""
        invalid_results = self.tester.test_invalid_payloads()
        
        # La plupart des payloads invalides doivent retourner 422
        valid_error_responses = [r for r in invalid_results if r.status_code == 422]
        assert len(valid_error_responses) >= len(invalid_results) * 0.7, "Gestion d'erreurs insuffisante"


# =====================================
# UTILITAIRES ET HELPERS
# =====================================

def check_service_dependencies():
    """
    V√©rifie les d√©pendances du service avant les tests
    """
    print("üîç V√©rification des d√©pendances...")
    
    dependencies = {
        "requests": "pour les appels HTTP",
        "json": "pour le parsing JSON",
        "concurrent.futures": "pour les tests concurrents"
    }
    
    missing_deps = []
    for dep, description in dependencies.items():
        try:
            __import__(dep)
            print(f"  ‚úÖ {dep}: OK")
        except ImportError:
            print(f"  ‚ùå {dep}: MANQUANT ({description})")
            missing_deps.append(dep)
    
    if missing_deps:
        print(f"\n‚ö†Ô∏è D√©pendances manquantes: {', '.join(missing_deps)}")
        print("Installez avec: pip install requests pytest")
        return False
    
    return True


def generate_test_data():
    """
    G√©n√®re des donn√©es de test pour les diff√©rents sc√©narios
    """
    return {
        "financial_queries": [
            "quel est mon solde",
            "mes derni√®res transactions",
            "virement 100 euros √† Paul",
            "d√©penses restaurant janvier",
            "bloquer ma carte",
            "historique des virements",
            "budget mensuel",
            "mes revenus",
            "analyse des d√©penses",
            "recherche par cat√©gorie alimentation"
        ],
        "conversational_queries": [
            "bonjour",
            "salut comment √ßa va",
            "bonsoir",
            "aide moi",
            "je ne comprends pas",
            "peux tu m'aider",
            "au revoir",
            "√† bient√¥t",
            "merci beaucoup",
            "comment √ßa marche"
        ],
        "edge_case_queries": [
            "",
            "a",
            "?",
            "123",
            "√©√†√ß",
            "tr√®s tr√®s tr√®s longue requ√™te " * 50,
            "!@#$%",
            "   ",
            "\n\t",
            "query with\nnewlines"
        ]
    }


def benchmark_service_performance():
    """
    Benchmark rapide de performance du service
    """
    print("‚ö° Benchmark de performance...")
    
    config = TestConfig()
    tester = ConversationServiceTester(config)
    
    # Test de latence simple
    single_request_times = []
    for i in range(10):
        result = tester._make_request("GET", "/api/v1/health")
        if result.success:
            single_request_times.append(result.response_time_ms)
    
    if single_request_times:
        avg_single = sum(single_request_times) / len(single_request_times)
        print(f"  üìä Latence moyenne (health): {avg_single:.1f}ms")
        print(f"  üìä Latence min/max: {min(single_request_times):.1f}ms / {max(single_request_times):.1f}ms")
    
    # Test de charge
    concurrent_results = tester.test_concurrent_requests(20)
    successful_concurrent = [r for r in concurrent_results if r.success]
    
    if successful_concurrent:
        avg_concurrent = sum(r.response_time_ms for r in successful_concurrent) / len(successful_concurrent)
        success_rate = len(successful_concurrent) / len(concurrent_results)
        print(f"  ‚ö° Latence sous charge: {avg_concurrent:.1f}ms")
        print(f"  ‚ö° Taux de succ√®s concurrent: {success_rate:.1%}")
    
    return {
        "single_request_avg_ms": avg_single if single_request_times else 0,
        "concurrent_avg_ms": avg_concurrent if successful_concurrent else 0,
        "concurrent_success_rate": success_rate if successful_concurrent else 0
    }


def create_monitoring_dashboard_data(report: Dict[str, Any]) -> Dict[str, Any]:
    """
    Cr√©e des donn√©es pour un dashboard de monitoring
    """
    return {
        "timestamp": time.time(),
        "service_status": report["service_status"],
        "overall_health": {
            "success_rate": report["summary"]["success_rate"],
            "avg_response_time_ms": report["summary"]["average_response_time_ms"],
            "total_tests": report["summary"]["total_tests"]
        },
        "endpoint_health": {
            section: {
                "availability": data["successful"] / data["total"] if data["total"] > 0 else 0,
                "test_count": data["total"]
            }
            for section, data in report["sections"].items()
        },
        "alerts": [
            f"Test √©chou√©: {test['method']} {test['endpoint']}"
            for test in report["failed_tests"][:5]  # Limite √† 5 alertes
        ],
        "performance_metrics": {
            "response_time_threshold_ms": 1000,
            "success_rate_threshold": 0.95,
            "current_success_rate": report["summary"]["success_rate"],
            "current_avg_response_time": report["summary"]["average_response_time_ms"]
        }
    }


if __name__ == "__main__":
    # V√©rification des d√©pendances
    if not check_service_dependencies():
        print("‚ùå Impossible de continuer sans les d√©pendances requises")
        exit(1)
    
    # Benchmark rapide optionnel
    if "--benchmark" in __import__("sys").argv:
        benchmark_results = benchmark_service_performance()
        print("\nüìä R√©sultats benchmark:")
        for key, value in benchmark_results.items():
            print(f"   {key}: {value}")
        print()
    
    # Ex√©cution des tests principaux
    exit_code = main()
    exit(exit_code)